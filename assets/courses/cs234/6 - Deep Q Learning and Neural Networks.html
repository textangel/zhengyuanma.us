
    <!DOCTYPE html>
    <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <!-- common.css -->
      <style>* {-webkit-tap-highlight-color: rgba(0,0,0,0);}html {-webkit-text-size-adjust: none;}body {font-family: -apple-system, Helvetica, Arial, sans-serif;margin: 0;padding: 20px;color: #333;word-wrap: break-word;}h1, h2, h3, h4, h5, h6 {line-height: 1.1;}img {max-width: 100% !important;height: auto;}blockquote {margin: 0;padding: 0 15px;color: #777;border-left: 4px solid #ddd;}hr {background-color: #ddd;border: 0;height: 1px;margin: 15px 0;}code {font-family: Menlo, Consolas, 'Ubuntu Mono', Monaco, 'source-code-pro', monospace;line-height: 1.4;margin: 0;padding: 0.2em 0;font-size: 90%;background-color: rgba(0,0,0,0.04);border-radius: 3px;}pre > code {margin: 0;padding: 0;font-size: 100%;word-break: normal;background: transparent;border: 0;}ol {list-style-type: decimal;}ol ol, ul ol {list-style-type: lower-latin;}ol ol ol, ul ol ol, ul ul ol, ol ul ol {list-style-type: lower-roman;}table {border-spacing: 0;border-collapse: collapse;margin-top: 0;margin-bottom: 16px;}table th {font-weight: bold;}table th, table td {padding: 6px 13px;border: 1px solid #ddd;}table tr {border-top: 1px solid #ccc;}table tr:nth-child(even) {background-color: #f8f8f8;}input[type="checkbox"] {cursor: default;margin-right: 0.5em;font-size: 13px;}.task-list-item {list-style-type: none;}.task-list-item+.task-list-item {margin-top: 3px;}.task-list-item input {float: left;margin: 0.3em 1em 0.25em -1.6em;vertical-align: middle;}#tag-field {margin: 8px 2px 10px;}#tag-field .tag {display: inline-block;background: #cadff3;border-radius: 4px;padding: 1px 8px;color: black;font-size: 12px;margin-right: 10px;line-height: 1.4;}</style>
      <!-- ace-static.css -->
      <style>.ace_static_highlight {white-space: pre-wrap;}.ace_static_highlight .ace_gutter {width: 2em;text-align: right;padding: 0 3px 0 0;margin-right: 3px;}.ace_static_highlight.ace_show_gutter > .ace_line {padding-left: 2.6em;}.ace_static_highlight .ace_line {position: relative;}.ace_static_highlight .ace_gutter-cell {-moz-user-select: -moz-none;-khtml-user-select: none;-webkit-user-select: none;user-select: none;top: 0;bottom: 0;left: 0;position: absolute;}.ace_static_highlight .ace_gutter-cell:before {content: counter(ace_line, decimal);counter-increment: ace_line;}.ace_static_highlight {counter-reset: ace_line;}</style>
      <style>.ace-chrome .ace_gutter {background: #ebebeb;color: #333;overflow : hidden;}.ace-chrome .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-chrome {background-color: #FFFFFF;color: black;}.ace-chrome .ace_cursor {color: black;}.ace-chrome .ace_invisible {color: rgb(191, 191, 191);}.ace-chrome .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-chrome .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-chrome .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-chrome .ace_invalid {background-color: rgb(153, 0, 0);color: white;}.ace-chrome .ace_fold {}.ace-chrome .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-chrome .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-chrome .ace_support.ace_type,.ace-chrome .ace_support.ace_class.ace-chrome .ace_support.ace_other {color: rgb(109, 121, 222);}.ace-chrome .ace_variable.ace_parameter {font-style:italic;color:#FD971F;}.ace-chrome .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-chrome .ace_comment {color: #236e24;}.ace-chrome .ace_comment.ace_doc {color: #236e24;}.ace-chrome .ace_comment.ace_doc.ace_tag {color: #236e24;}.ace-chrome .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-chrome .ace_variable {color: rgb(49, 132, 149);}.ace-chrome .ace_xml-pe {color: rgb(104, 104, 91);}.ace-chrome .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-chrome .ace_heading {color: rgb(12, 7, 255);}.ace-chrome .ace_list {color:rgb(185, 6, 144);}.ace-chrome .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-chrome .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-chrome .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-chrome .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-chrome .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-chrome .ace_gutter-active-line {background-color : #dcdcdc;}.ace-chrome .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-chrome .ace_storage,.ace-chrome .ace_keyword,.ace-chrome .ace_meta.ace_tag {color: rgb(147, 15, 128);}.ace-chrome .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-chrome .ace_string {color: #1A1AA6;}.ace-chrome .ace_entity.ace_other.ace_attribute-name {color: #994409;}.ace-chrome .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}</style>
      <!-- export.css -->
      <style>
        body{margin:0 auto;max-width:800px;line-height:1.4}
        #nav{margin:5px 0 10px;font-size:15px}
        #titlearea{border-bottom:1px solid #ccc;font-size:17px;padding:10px 0;}
        #contentarea{font-size:15px;margin:16px 0}
        .cell{outline:0;min-height:20px;margin:5px 0;padding:5px 0;}
        .code-cell{font-family:Menlo,Consolas,'Ubuntu Mono',Monaco,'source-code-pro',monospace;font-size:12px;}
        .latex-cell{white-space:pre-wrap;}
      </style>
      <!-- User CSS -->
      <style> .text-cell {font-size: 15px;}.code-cell {font-size: 12px;}.markdown-cell {font-size: 15px;}.latex-cell {font-size: 15px;}</style>
    </head>
    <body>
      <div id="titlearea">
        <h2>CS234 - Lecture 6 - Deep Q Learning and NN</h2>
      </div>
      <div id="contentarea"><div class="cell text-cell"><p>Nonlinear function approximations for RL:</p><ul><li>Kernel Based Approaches (Convergence Properties but don’t scale well)</li><li>Neural Networks</li></ul><p>History of NN approaches to RL:</p><ul><li>1994 TD Backgammon.</li><li>1995-1990 Was proved that Function approximation + off policy control + bootstrapping (“the deadly triad”) can fail to converge and even simple cases can fail. People felt discouraged from using NN approaches.</li><li>2014 Atari</li></ul></div><div class="cell text-cell"><p><b style="font-size: 19px;">Deep Q Learning and Improvements</b><br></p><div><img src="resources/33E15FDDDFE6750133831D8872A18C05.png" alt="IMG_0353.PNG" width="2208" height="1242"><br></div><div><img src="resources/5190FC2120E936D9EF15AD9374F22FC0.png" alt="IMG_0354.PNG" width="2208" height="1242">In 2014 Atari Paper, DQN Used Q-learning with CNN as Value Function Approximator. They used two tricks to make it work:<b>&nbsp;Experience Replay</b>&nbsp;and&nbsp;<b>Fixed Q Targets</b>.</div><div><br></div><div><b>Experience Replay</b></div><div>Keep a store of prior experience and sample from that store with replacement as the training dataset.</div><div>Benefits - allows us to use a lot more data - experience is not just thrown away after being used.</div><div>Note that each sampled action (s,a,r,s’) will update the weights a different amount even if the same action (s,a,r,s’) was sampled before - because the Q function was different from before, so the update is now different.</div><div>Expereince replay was super super important for success (Appendix 1)!<br></div><div><img src="resources/A099571ED17B400B5B231B91F393A50F.png" alt="IMG_0356.PNG" width="2208" height="1242"><br></div><div><br></div><div><b>Fixed Q Targets</b></div><div>Basically, instead of updating the target weights evert step, when calculating the bootstrap value use the same weights for every couple of steps (freeze its value) and then reset the frozen value to the new learned value every n steps. This is no longer precise gradient descent but improves stability in the training with little loss.</div><div>&nbsp;</div><div><img src="resources/63EA01AB4D93B23DC611BB203D87780C.png" alt="IMG_0361.PNG" width="2208" height="1242"><b><br></b></div><div><br></div><div><b>Some Improvements over Original DeepQL Paper.</b></div><div><img src="resources/805B01368095142AC4FA43F091CDC19B.png" alt="IMG_0365.PNG" width="2208" height="1242"><br></div><div><b>Double DQN</b></div><div>Improvement over Original Deep QL Paper.</div><div>Basically Double Q-learning applied to DQN.</div><div>Easy to Implement, impressive Results (Apendix 1)<br></div><div><img src="resources/9AF705BB4C5233FD751755531A5A93EF.png" alt="IMG_0368.PNG" width="2208" height="1242"><br></div><div><br></div><div><b>Prioritized Replay:</b>&nbsp;In TD Learning, the order of sampling from the replay experience affects the speed of learning, becuase it determines whether or not some updates will propagate to other states.</div><div>The authors proposed sampling updates with probability proportional to their DQN error. So tuples (s,a,r,s’) for which the DQN update is more accurate is sampled more often.</div><div>This seems to have a very high impact also! (Appendix 1) (See Appendix 2 for example)</div><div><br></div><div><img src="resources/7E35C5C7B92DBA576CFAEF97B9DB97E7.png" alt="IMG_0370.PNG" width="2208" height="1242"><br></div><div><img src="resources/88744C88680EB517DB0463722105F261.png" alt="IMG_0371.PNG" width="2208" height="1242"><b><br></b></div><div>If we knew the optimal order in which to sample, the above paper proved that the time to convergence can be reduced by an exponential factor! Of course it is computationally infeasible to know the exact optimal order, but we can estimate it with heuristics.</div><div><img src="resources/E96BEBC5DD7BA145067F3A821AAC5B91.png" alt="IMG_0372.PNG" width="2208" height="1242"></div><div>(note: alpha is a temperature setting which, if 0 gives requal prob for all actions and if large gives the max action every time).</div><div><br></div><div><b>Dueling QN:</b></div><div>Also very good effects! (Appendix 1)</div><div>Intuition: The features that you might need to write down the values of the state might be different from the features you need to specify the relative benefit of differnet actions (advantage functions) in that state.</div><div><br></div><div>Idea: Instead of estimating the Q value function, estimate the advantage function.&nbsp;</div><div><img src="resources/F340CDBEC3734A6FF08B8325DB6CD079.png" alt="IMG_0377.PNG" width="2208" height="1242"><b><br></b></div><div><img src="resources/312B1A5EE7BEA30695820AE5D92A818B.png" alt="IMG_0378.PNG" width="2208" height="1242"><br></div><div><br></div><div>However, the advantage function is unidedentifiable (there is not a unique V and A you can decompose Q into - you can add a constant to one and subtract the same from the other and get the same Q. So we add an additional constraint to force out a particular V, A.</div><div><img src="resources/6AF1C48EDDC8F636037014E6A68260E5.png" alt="IMG_0379.PNG" width="2208" height="1242"><br></div><div><br></div></div><div class="cell text-cell"><p><b>Practical Tips for training DQNs:</b><br></p><div><img src="resources/188FC6FC5501B7DBCD0CD073DCA43117.png" alt="IMG_0381.PNG" width="2208" height="1242"><b><br></b></div><div><img src="resources/EBBC606249373B3E847B93FC9B354A3B.png" alt="IMG_0382.PNG" width="2208" height="1242"><b><br></b></div><div><b><br></b></div><div><b>Appendix 1: DQN for Atari, Summary and Results</b></div><div><b><br></b></div><div><img src="resources/E2D1FF7746FD28242A436437C57E395D.png" alt="IMG_0363.PNG" width="2208" height="1242"><b><br></b></div><div><img src="resources/4215D1F9423E745CB7848C1F76BBD480.png" alt="IMG_0364.PNG" width="2208" height="1242"><br></div><div><img src="resources/9779CDACDF2B9DA1AF892F8CBF6E2037.png" alt="IMG_0369.PNG" width="2208" height="1242"><br></div><div><img src="resources/F3232A687403CD1D4403E502AD1232DE.png" alt="IMG_0373.PNG" width="2208" height="1242"><img src="resources/A25B85C1A5958A27E118CA979108F5BA.png" alt="IMG_0380.PNG" width="2208" height="1242"><br></div><div><br></div><div><b>Appendix 2:&nbsp;Mars Rover example for different values of estimate for differnet orders sampling.</b><br></div><div><img src="resources/412C1AD81D175D3A89444163354C8D45.png" alt="Screenshot 2020-03-30 at 10.37.10 PM.png" width="643" height="483"><br></div><div><br></div><div>Tip: Build and debug your Q-learning implementation first before you test on Atari, Atari could take several hours.<br></div>
</div><div class="cell text-cell"></div></div>
    </body>
    </html>
  