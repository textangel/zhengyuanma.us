
    <!DOCTYPE html>
    <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <!-- common.css -->
      <style>* {-webkit-tap-highlight-color: rgba(0,0,0,0);}html {-webkit-text-size-adjust: none;}body {font-family: -apple-system, Helvetica, Arial, sans-serif;margin: 0;padding: 20px;color: #333;word-wrap: break-word;}h1, h2, h3, h4, h5, h6 {line-height: 1.1;}img {max-width: 100% !important;height: auto;}blockquote {margin: 0;padding: 0 15px;color: #777;border-left: 4px solid #ddd;}hr {background-color: #ddd;border: 0;height: 1px;margin: 15px 0;}code {font-family: Menlo, Consolas, 'Ubuntu Mono', Monaco, 'source-code-pro', monospace;line-height: 1.4;margin: 0;padding: 0.2em 0;font-size: 90%;background-color: rgba(0,0,0,0.04);border-radius: 3px;}pre > code {margin: 0;padding: 0;font-size: 100%;word-break: normal;background: transparent;border: 0;}ol {list-style-type: decimal;}ol ol, ul ol {list-style-type: lower-latin;}ol ol ol, ul ol ol, ul ul ol, ol ul ol {list-style-type: lower-roman;}table {border-spacing: 0;border-collapse: collapse;margin-top: 0;margin-bottom: 16px;}table th {font-weight: bold;}table th, table td {padding: 6px 13px;border: 1px solid #ddd;}table tr {border-top: 1px solid #ccc;}table tr:nth-child(even) {background-color: #f8f8f8;}input[type="checkbox"] {cursor: default;margin-right: 0.5em;font-size: 13px;}.task-list-item {list-style-type: none;}.task-list-item+.task-list-item {margin-top: 3px;}.task-list-item input {float: left;margin: 0.3em 1em 0.25em -1.6em;vertical-align: middle;}#tag-field {margin: 8px 2px 10px;}#tag-field .tag {display: inline-block;background: #cadff3;border-radius: 4px;padding: 1px 8px;color: black;font-size: 12px;margin-right: 10px;line-height: 1.4;}</style>
      <!-- ace-static.css -->
      <style>.ace_static_highlight {white-space: pre-wrap;}.ace_static_highlight .ace_gutter {width: 2em;text-align: right;padding: 0 3px 0 0;margin-right: 3px;}.ace_static_highlight.ace_show_gutter > .ace_line {padding-left: 2.6em;}.ace_static_highlight .ace_line {position: relative;}.ace_static_highlight .ace_gutter-cell {-moz-user-select: -moz-none;-khtml-user-select: none;-webkit-user-select: none;user-select: none;top: 0;bottom: 0;left: 0;position: absolute;}.ace_static_highlight .ace_gutter-cell:before {content: counter(ace_line, decimal);counter-increment: ace_line;}.ace_static_highlight {counter-reset: ace_line;}</style>
      <style>.ace-chrome .ace_gutter {background: #ebebeb;color: #333;overflow : hidden;}.ace-chrome .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-chrome {background-color: #FFFFFF;color: black;}.ace-chrome .ace_cursor {color: black;}.ace-chrome .ace_invisible {color: rgb(191, 191, 191);}.ace-chrome .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-chrome .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-chrome .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-chrome .ace_invalid {background-color: rgb(153, 0, 0);color: white;}.ace-chrome .ace_fold {}.ace-chrome .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-chrome .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-chrome .ace_support.ace_type,.ace-chrome .ace_support.ace_class.ace-chrome .ace_support.ace_other {color: rgb(109, 121, 222);}.ace-chrome .ace_variable.ace_parameter {font-style:italic;color:#FD971F;}.ace-chrome .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-chrome .ace_comment {color: #236e24;}.ace-chrome .ace_comment.ace_doc {color: #236e24;}.ace-chrome .ace_comment.ace_doc.ace_tag {color: #236e24;}.ace-chrome .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-chrome .ace_variable {color: rgb(49, 132, 149);}.ace-chrome .ace_xml-pe {color: rgb(104, 104, 91);}.ace-chrome .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-chrome .ace_heading {color: rgb(12, 7, 255);}.ace-chrome .ace_list {color:rgb(185, 6, 144);}.ace-chrome .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-chrome .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-chrome .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-chrome .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-chrome .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-chrome .ace_gutter-active-line {background-color : #dcdcdc;}.ace-chrome .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-chrome .ace_storage,.ace-chrome .ace_keyword,.ace-chrome .ace_meta.ace_tag {color: rgb(147, 15, 128);}.ace-chrome .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-chrome .ace_string {color: #1A1AA6;}.ace-chrome .ace_entity.ace_other.ace_attribute-name {color: #994409;}.ace-chrome .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}</style>
      <!-- export.css -->
      <style>
        body{margin:0 auto;max-width:800px;line-height:1.4}
        #nav{margin:5px 0 10px;font-size:15px}
        #titlearea{border-bottom:1px solid #ccc;font-size:17px;padding:10px 0;}
        #contentarea{font-size:15px;margin:16px 0}
        .cell{outline:0;min-height:20px;margin:5px 0;padding:5px 0;}
        .code-cell{font-family:Menlo,Consolas,'Ubuntu Mono',Monaco,'source-code-pro',monospace;font-size:12px;}
        .latex-cell{white-space:pre-wrap;}
      </style>
      <!-- User CSS -->
      <style> .text-cell {font-size: 15px;}.code-cell {font-size: 12px;}.markdown-cell {font-size: 15px;}.latex-cell {font-size: 15px;}</style>
    </head>
    <body>
      <div id="titlearea">
        <h2>CS224n- Lecture 18 - Constituency Parsing, TreeRNNs</h2>
      </div>
      <div id="contentarea"><div class="cell text-cell">TreeRNNs are hard to scale and don't necessarily work better than LSTMs and Transformers.&nbsp;<div>Part of the tricks of being a good deep learning researcher is managing to get thigns done and not wasting a ton of time. So train on a smal model firs tand make sure you can overfit on them. Plot your training and dev errors over model iterations.</div><div><br></div><div>The spectrum of language in CS.</div><div><br></div><div><b>Semantic Interpretation of Language</b></div><div>Beyond word vectors:&nbsp;</div><div>How can we work out the meaning of larger phrases?</div><div><br></div><div><i>The snowboarder</i> is leaping over a mogul.</div><div><i>A person on a snowboard</i> jumps into the air.</div><div><br></div><div>In the above phrase, <i>the snowboarder</i>&nbsp;and <i>a person on a snowboard</i>&nbsp;refer to the same thing, but they’re made up of a different number of words (2 vs 5). How can we have a representation that works for variable word lengths? =&gt; Recursive TreeRNNs.</div><div><br></div><div>We need a way to compose linguistic knowledge from smaller parts. Chomsky argues that the singular innovation that allowed human beings to model langauge is that we could model recursion (putting together bigger things from smaller parts).</div><div><br></div><div>We want a method of computing a embedded representation of phrases of any length in a compositional manner.</div><div><br></div><div>The meaning (vector) of a sentence is determined by</div><div>&nbsp; (1) the meanings of its words and&nbsp;</div><div>&nbsp; (2) the rules that combine them</div><div><br></div><div><img src="resources/7844253BEA09ACCA98DC4F5F415194F2.png" alt="Screenshot 2020-02-16 at 7.29.15 PM.png" width="620" height="461"><br></div><div><br></div><div>Given some parse of a sentence, learn a vector space representation through compositionality. Run a NN through pairs of words to determine whether to combine them and what the new combined representation should be.</div><div><br></div><div><img src="resources/34676575FB512387CE4F028DA14B2097.png" alt="Screenshot 2020-02-16 at 7.34.19 PM.png" width="577" height="460"><br></div><div><br></div><div><b style="font-size: 18px;">Version 1:&nbsp;</b><br></div><div>Their first attempt to do this just used a simple one layer NN (Concatenating the embeddings of the two words), and use a greedy parser that uses the NN generated confidences in max-order. (Note adter you do a combination of two words you have to run the new phrase with its adjacent words to get the nexst level confidence)</div><div>See Process of Generation in Appendix 1.</div><div><br></div><div><img src="resources/26B52DE2B06A0277A28EDF52D1C3C7A0.png" alt="Screenshot 2020-02-16 at 7.33.31 PM.png" width="583" height="452"><br></div><div><br></div><div>They scored the tree by summing the parsing decision scores at each node. They optimized it against a max-margin objective.&nbsp;</div><div><img src="resources/3A4E094C721A9E4E7D577F38039B4DBC.png" alt="Screenshot 2020-02-16 at 7.39.54 PM.png" width="595" height="222"><br></div><div><b>Issues</b>:</div><div>Because it’s a recursive RNN, the weight vector in the composition is the same for all compositions, in particular, for all syntactic categories. This might be a problem.</div><div style="font-size: 15px;"><img src="resources/CDEE2B47616C3E962B3A340A0CC71006.png" alt="Screenshot 2020-02-16 at 7.45.48 PM.png" width="596" height="122"><br></div><div style="font-size: 17px;"><b>Version 2:</b></div><div>A treeRNN with different compsotition (weight) matric for differnet syntactic environments. These syntactic enviornments are given by a (probabilistic) constituency parser.</div><div><img src="resources/147CA9B75F1AA4500AD45BBE88CA7562.png" alt="Screenshot 2020-02-16 at 7.49.51 PM.png" width="604" height="456"><br></div><div><br></div><div>This model can learn where the important semantics of sentences are. When words are merged, the resultant embedding is more similar to the more important word that was merged.&nbsp;</div><div><br></div><div style="font-size: 17px;"><b>Version 3:</b></div><div><b>Compositionality Through Recursive Matrix-Vector Spaces</b></div><div>Proposal: A new composition function where words that act as operators (“very”) are matrices (modifiers) rather than vector.</div><div>To be general, each word has both a vector and a matrix representation.&nbsp;</div><div><br></div><div>To get the new vector meaning of a phrase: To combine two words, we take their matrices and operate on the other’s vector, and push thorugh a NN to get the new&nbsp;</div><div><br></div><div><img src="resources/2364664AF6CE0E1A78217588990160CF.png" alt="Screenshot 2020-02-16 at 8.02.10 PM.png" width="578" height="454"><br></div><div>To get the new matrix meaning of a phrase (which might have been a bit too simple and not too good): Concatenate their two matrices and multiply by a third weight matrix.</div><div><img src="resources/15E24E7A354853E698488A5BA338E9B4.png" alt="Screenshot 2020-02-16 at 8.03.49 PM.png" width="603" height="455"><br></div><div>This was very hard to compute because the matrices required n^2 number of parameters. They kept the dimensionality very small (=25).</div><div><br></div><div style="font-size: 18px;"><b>Version 4: Recursive Neural Tensor Network</b><span style="font-size: 13px;">.</span></div><div>(RNTN)</div><div>Inspired work in putting vector embeddings in knowledge graphs! Also led to the creation of the Stanford Sentiment Treebank! This model was applied to the problem of sentiment analysis.&nbsp;<img src="resources/6048A374B986C5F01720474ED7142196.png" alt="Screenshot 2020-02-16 at 8.08.33 PM.png" width="594" height="439" style="font-size: 3px;"></div><div>Idea: Still have the representaiton be vectors, but despite that have a meaning for a phrase composed in a way that allowed the two vectors to act upon each other. =&gt; By allowing both additive and mediated multiplicative interactions of vectors.</div><div><br></div><div>The combination of the vectors is performed by multiplying them against a 3D tensor whose depth is the dimension of the word vector (this is like performing d separate attentions) at the same time.&nbsp;</div><div><br></div><div><img src="resources/9548BDC617F7016E38329A0BD3FDCF08.png" alt="Screenshot 2020-02-16 at 8.21.31 PM.png" width="619" height="463"><br></div><div>RNTN was pretty good at classifying sentiment!</div><div><br></div><div><img src="resources/F00D047B9AFEEAFAF62F86DDC5FC6952.png" alt="Screenshot 2020-02-16 at 8.22.56 PM.png" width="603" height="434"><br></div><div>GPUs work really well if there is a uniform computation you gotta do, whether RNNs (apply the same recurrence every time) or CNNs.</div><div><br></div><div></div><br></div><div style="font-size: 16px;"><b>Stanford Center for Human Centered Artificial Intelligence (HAI)</b></div><div>Develoing AI technolgoies inspired by human intelligence</div><div>Guiding and forecasting the human and societal impact of AI</div><div>Designing AI applications that augment human capabilities</div><div><br></div><div style="font-size: 16px;"><b>Appendix 1:</b></div><div>Generating the recursive parse:</div><div><div><img src="resources/28949B6C1D31C2E98E3579F4800DE09D.png" alt="Screenshot 2020-02-16 at 7.35.42 PM.png" width="564" height="445"></div><div><br></div><div><img src="resources/F257500ED604232BA9B1E9ECE65D50AD.png" alt="Screenshot 2020-02-16 at 7.36.29 PM.png" width="565" height="418"><br></div><div><br></div><div><img src="resources/7DC07CA3A1DB17EA89FFF0D61223BCCA.png" alt="Screenshot 2020-02-16 at 7.36.42 PM.png" width="552" height="437"><br></div></div><div><br></div><div style="font-size: 17px;"><b>Appendix 2:</b><img src="resources/FF60190BB3AEC1E552CC3FC15B551B61.png" alt="Screenshot 2020-02-16 at 7.56.19 PM.png" width="600" height="446" style="font-size: 13px;"></div><div style="font-size: 17px;"><b><br></b></div></div></div>
 
    </body>
    </html>
  