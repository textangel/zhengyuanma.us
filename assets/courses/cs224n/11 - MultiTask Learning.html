
    <!DOCTYPE html>
    <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <!-- common.css -->
      <style>* {-webkit-tap-highlight-color: rgba(0,0,0,0);}html {-webkit-text-size-adjust: none;}body {font-family: -apple-system, Helvetica, Arial, sans-serif;margin: 0;padding: 20px;color: #333;word-wrap: break-word;}h1, h2, h3, h4, h5, h6 {line-height: 1.1;}img {max-width: 100% !important;height: auto;}blockquote {margin: 0;padding: 0 15px;color: #777;border-left: 4px solid #ddd;}hr {background-color: #ddd;border: 0;height: 1px;margin: 15px 0;}code {font-family: Menlo, Consolas, 'Ubuntu Mono', Monaco, 'source-code-pro', monospace;line-height: 1.4;margin: 0;padding: 0.2em 0;font-size: 90%;background-color: rgba(0,0,0,0.04);border-radius: 3px;}pre > code {margin: 0;padding: 0;font-size: 100%;word-break: normal;background: transparent;border: 0;}ol {list-style-type: decimal;}ol ol, ul ol {list-style-type: lower-latin;}ol ol ol, ul ol ol, ul ul ol, ol ul ol {list-style-type: lower-roman;}table {border-spacing: 0;border-collapse: collapse;margin-top: 0;margin-bottom: 16px;}table th {font-weight: bold;}table th, table td {padding: 6px 13px;border: 1px solid #ddd;}table tr {border-top: 1px solid #ccc;}table tr:nth-child(even) {background-color: #f8f8f8;}input[type="checkbox"] {cursor: default;margin-right: 0.5em;font-size: 13px;}.task-list-item {list-style-type: none;}.task-list-item+.task-list-item {margin-top: 3px;}.task-list-item input {float: left;margin: 0.3em 1em 0.25em -1.6em;vertical-align: middle;}#tag-field {margin: 8px 2px 10px;}#tag-field .tag {display: inline-block;background: #cadff3;border-radius: 4px;padding: 1px 8px;color: black;font-size: 12px;margin-right: 10px;line-height: 1.4;}</style>
      <!-- ace-static.css -->
      <style>.ace_static_highlight {white-space: pre-wrap;}.ace_static_highlight .ace_gutter {width: 2em;text-align: right;padding: 0 3px 0 0;margin-right: 3px;}.ace_static_highlight.ace_show_gutter > .ace_line {padding-left: 2.6em;}.ace_static_highlight .ace_line {position: relative;}.ace_static_highlight .ace_gutter-cell {-moz-user-select: -moz-none;-khtml-user-select: none;-webkit-user-select: none;user-select: none;top: 0;bottom: 0;left: 0;position: absolute;}.ace_static_highlight .ace_gutter-cell:before {content: counter(ace_line, decimal);counter-increment: ace_line;}.ace_static_highlight {counter-reset: ace_line;}</style>
      <style>.ace-chrome .ace_gutter {background: #ebebeb;color: #333;overflow : hidden;}.ace-chrome .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-chrome {background-color: #FFFFFF;color: black;}.ace-chrome .ace_cursor {color: black;}.ace-chrome .ace_invisible {color: rgb(191, 191, 191);}.ace-chrome .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-chrome .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-chrome .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-chrome .ace_invalid {background-color: rgb(153, 0, 0);color: white;}.ace-chrome .ace_fold {}.ace-chrome .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-chrome .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-chrome .ace_support.ace_type,.ace-chrome .ace_support.ace_class.ace-chrome .ace_support.ace_other {color: rgb(109, 121, 222);}.ace-chrome .ace_variable.ace_parameter {font-style:italic;color:#FD971F;}.ace-chrome .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-chrome .ace_comment {color: #236e24;}.ace-chrome .ace_comment.ace_doc {color: #236e24;}.ace-chrome .ace_comment.ace_doc.ace_tag {color: #236e24;}.ace-chrome .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-chrome .ace_variable {color: rgb(49, 132, 149);}.ace-chrome .ace_xml-pe {color: rgb(104, 104, 91);}.ace-chrome .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-chrome .ace_heading {color: rgb(12, 7, 255);}.ace-chrome .ace_list {color:rgb(185, 6, 144);}.ace-chrome .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-chrome .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-chrome .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-chrome .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-chrome .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-chrome .ace_gutter-active-line {background-color : #dcdcdc;}.ace-chrome .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-chrome .ace_storage,.ace-chrome .ace_keyword,.ace-chrome .ace_meta.ace_tag {color: rgb(147, 15, 128);}.ace-chrome .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-chrome .ace_string {color: #1A1AA6;}.ace-chrome .ace_entity.ace_other.ace_attribute-name {color: #994409;}.ace-chrome .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}</style>
      <!-- export.css -->
      <style>
        body{margin:0 auto;max-width:800px;line-height:1.4}
        #nav{margin:5px 0 10px;font-size:15px}
        #titlearea{border-bottom:1px solid #ccc;font-size:17px;padding:10px 0;}
        #contentarea{font-size:15px;margin:16px 0}
        .cell{outline:0;min-height:20px;margin:5px 0;padding:5px 0;}
        .code-cell{font-family:Menlo,Consolas,'Ubuntu Mono',Monaco,'source-code-pro',monospace;font-size:12px;}
        .latex-cell{white-space:pre-wrap;}
      </style>
      <!-- User CSS -->
      <style> .text-cell {font-size: 15px;}.code-cell {font-size: 12px;}.markdown-cell {font-size: 15px;}.latex-cell {font-size: 15px;}</style>
    </head>
    <body>
      <div id="titlearea">
        <h2>CS224n- Lecture 17 - MultiTask Learning, Guest Lecture Richard Socher</h2>
      </div>
      <div id="contentarea"><div class="cell text-cell"><div><b>Richard Socher -&nbsp;</b></div><b>The Natural Langauge Decathlon- MultiTask Learning for Question Answering.</b><div>One Model to Rule All The Different Tasks.&nbsp;<div>One model, using a lot of preprocessing, for 10 different tasks.</div><div><br></div><div>Great performane improvements given {dataset, task, model, metric}</div><div>With image and langauge models,&nbsp;</div><div>we can hill-climb to local optima as long as |dataset| &gt; 1000 x |num-classes to work with|</div><div><br></div><div>We’ve seen the benefits of pre-training in NLP (word2vec,…, BERT). Why not just pretrain the entire model, and see if a single task solve it all?</div><div>Why is this difficult:</div><div>&nbsp; - NLP requires many times of reasoning: logical, linguistic, emotional, etc</div><div>&nbsp; - NLP requires short and long term memory</div><div>&nbsp; - NLP had been divided into intermediate and separate tasks (articifically) to chase benchmarks</div><div><br></div><div>Can a single unsupervised task solve it all? No. Language clearly requires supervision in nature.</div><div><br></div><div><b>Multi-task learning is a blocker for general NLP systems</b></div><div>Unified models can decide how to transfer knowledge (domain adaptation, weight sharing, transfer and zero-shot learning).</div><div>Unified, multi-task models can</div><div>&nbsp; - More easily adapt to new tasks, make deploying to production X times simpler</div><div>&nbsp; - Lower the bar for more people to solve new tasks, potentially move towards continual learning.</div><div><br></div><div><b>Approach:</b></div><div>Classify NLP tasks into these categoies:</div><div>&nbsp; - Sequence tagging (NER, aspect specific sentiment)</div><div>&nbsp; - Text classification: (Dialogue state tracking, sentiment classification)</div><div>&nbsp; - Seq2Seq: (Machine translation, summarization, QA)</div><div><br></div><div><b>How to model the general MLP tasks?</b></div><div>Idea: There are about 3 Equivalent Supertasks of NLP: <b>Langauge Modeling</b> (however, only used to rescore or pretrain these days, used less and less), Question Answering (we used this one), Dialogue Systems (Currently no real good dialogue datasets out there).</div><div><br></div><div>They use Question Answering as the main formalism for modeling all the below NLP tasks. (See Appendix 1)</div><div>&nbsp; - Question Answering, Machine Translation, Summarisation, Natural Langauge Inference, Sentiment Classification, Semantic Role Labelling, Relation Extraction, Dialogue, Semantic Parsing, Commonsense Reasoning.&nbsp;</div><div>&nbsp; (Meta supervised learning: From {x,y} to {x,t,y}, t is the task. The question q is a natural description of the task t. x is the context necessary to answer q.)<br></div><div><br></div><div><b>Model for DecaNLP</b></div><div>&nbsp; - No task specific modules or parameters; we assume the taskID is not available.</div><div>&nbsp; - Must be able to adjust internally to perform disparate tasks</div><div>&nbsp; - Start with a context</div><div>&nbsp; - Ask a question</div><div>&nbsp; - Generate the answer one word ad a time by</div><div>&nbsp; &nbsp; - Pointing to context</div><div>&nbsp; &nbsp; - Pointing to question</div><div>&nbsp; &nbsp; - Choosing a word from an external vocabulary</div><div>&nbsp; - Pointer switch mechanism chooses how much to weight each of these three mechanisms.</div><div><br></div><div>Full model (decaNLP.com) takes the best of all SOTA techniques and puts them to gether in a model that can generalize well enough.</div><div><img src="resources/8CCA2EA6D44F2D761EC04F36CC9C3C43.png" alt="Screenshot 2020-02-16 at 4.51.00 PM.png" width="693" height="391"><br></div><div><br></div><div><b>1. Initial Embedding Layer:</b></div><div><img src="resources/93F8797A3DA4AA27A674CDC194528319.png" alt="Screenshot 2020-02-16 at 4.54.38 PM.png" width="671" height="352"></div><div><br></div><div>Fixed Glove + Character n-gram embeddings -&gt; Linear -&gt; Shared BiLSTM with skip connection.</div><div>(Fixed because some of the tasks have very small datasets. If you allow the word embeddings to move too much its very hard to generalize.)</div><div><br></div><div><b>2. Co-attention layer:</b></div><div>Outer products between all the hidden states of those two sequences. With skipconnections to circumvent some of them also. Now we have context representations, representations of the context.</div><div><img src="resources/8EF3274E5BFB9D3AE4BADF45BA8B3A6F.png" alt="Screenshot 2020-02-16 at 4.55.54 PM.png" width="677" height="362"><br></div><div><b>3. Transformer Layers</b></div><div><img src="resources/E96006E9442ED5F0790F5FC9E6929875.png" alt="Screenshot 2020-02-16 at 4.57.45 PM.png" width="674" height="353"><b><br></b></div><div>Transformers had biLSTMs before and after the transformer layers, because they weren’t robust enough at time of model to do all tasks well in one model.&nbsp;</div><div><br></div><div><img src="resources/9FFA2353A1BE1FCC3160F7967B919C92.png" alt="Screenshot 2020-02-16 at 4.58.53 PM.png" width="677" height="388"><br></div><div>Output of layers (final embedding) is piped into a standard autoregressive decoder to get the next word, with three pointer mechanisms which basically learn to point to question words, context words, or words in the vocabulary based on the hidden states of the final encoding. The output distribution is a weighted convex sum of these three distributions of output words.</div><div><br></div><div><b>Evaluated on a ton of datasets</b></div><div><img src="resources/DB590EC7818BF58FDDBF9389231C0FE3.png" alt="Screenshot 2020-02-16 at 5.01.37 PM.png" width="664" height="369"><br></div><div><br></div><div><b>Results</b></div><div><img src="resources/646F8DBAE7556DEE956789DDE68212DA.png" alt="Screenshot 2020-02-16 at 5.06.24 PM.png" width="703" height="409"><br></div><div><br></div><div><b>Takeaways:</b></div><div>QA and Semantile Role Labelling have a strong connection.</div><div>Pointing to the question is essential</div><div>Most of the architecture got really really good at pointing to things to answer questions.<br></div><div><br></div><div>This model does exceptionally well on Zero-shot relation extraction, accuracy went up almost 2x when you learned it with everythign else.</div><div><br></div><div>There is still a gap between the combined single task models and the single multi-task model.</div><div><br></div><div><b>Complexities</b></div><div>How to train:&nbsp;</div><div>1. Fully joint: Train a minibatch from each task at a time, train from that task, and loop over each of the tasks (so the next minibatch is from the next task, etc)</div><div>2. An Optimization: Anti-Curiculum Learning- Start with the hardest tasks, train on those first, then train on simple tasks. Otherwise your hard task training might forget your simple tasks embeddings in the beginning.</div><div><br></div><div><img src="resources/6C5BA5AEEEE6A055E9B78F93CF0AF025.png" alt="Screenshot 2020-02-16 at 6.46.02 PM.png" width="689" height="340"><br></div><div><br></div><div>Also, pretraining on decaNLP improves final performacne on other tasks (e.g. other IWSLT language pairs, NER).</div><div>Zero-shot domain adapcation for pretrained MQAN (NLI) - 80% accuracy on Amazon and Yelp reviews. Zero-shot classification, etc.</div><div><br></div><div><b>Appendix 1</b>:</div><div>Socher’s Slides</div><div><br></div><div><img src="resources/53D8CB4B26888666805A7DC64E6C4245.png" alt="Screenshot 2020-02-16 at 4.12.22 PM.png" width="678" height="379"></div><div><br></div><div><img src="resources/5B9EE56A3C700CB9963A0928A84B6DE7.png" alt="Screenshot 2020-02-16 at 4.17.10 PM.png" width="685" height="377"><br></div><div><img src="resources/7FD963ADED4A79521FDC8641666DDE79.png" alt="Screenshot 2020-02-16 at 4.28.08 PM.png" width="1209" height="586"><br></div><div><br></div>

    </body>
    </html>
  