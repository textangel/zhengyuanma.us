
    <!DOCTYPE html>
    <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <!-- common.css -->
      <style>* {-webkit-tap-highlight-color: rgba(0,0,0,0);}html {-webkit-text-size-adjust: none;}body {font-family: -apple-system, Helvetica, Arial, sans-serif;margin: 0;padding: 20px;color: #333;word-wrap: break-word;}h1, h2, h3, h4, h5, h6 {line-height: 1.1;}img {max-width: 100% !important;height: auto;}blockquote {margin: 0;padding: 0 15px;color: #777;border-left: 4px solid #ddd;}hr {background-color: #ddd;border: 0;height: 1px;margin: 15px 0;}code {font-family: Menlo, Consolas, 'Ubuntu Mono', Monaco, 'source-code-pro', monospace;line-height: 1.4;margin: 0;padding: 0.2em 0;font-size: 90%;background-color: rgba(0,0,0,0.04);border-radius: 3px;}pre > code {margin: 0;padding: 0;font-size: 100%;word-break: normal;background: transparent;border: 0;}ol {list-style-type: decimal;}ol ol, ul ol {list-style-type: lower-latin;}ol ol ol, ul ol ol, ul ul ol, ol ul ol {list-style-type: lower-roman;}table {border-spacing: 0;border-collapse: collapse;margin-top: 0;margin-bottom: 16px;}table th {font-weight: bold;}table th, table td {padding: 6px 13px;border: 1px solid #ddd;}table tr {border-top: 1px solid #ccc;}table tr:nth-child(even) {background-color: #f8f8f8;}input[type="checkbox"] {cursor: default;margin-right: 0.5em;font-size: 13px;}.task-list-item {list-style-type: none;}.task-list-item+.task-list-item {margin-top: 3px;}.task-list-item input {float: left;margin: 0.3em 1em 0.25em -1.6em;vertical-align: middle;}#tag-field {margin: 8px 2px 10px;}#tag-field .tag {display: inline-block;background: #cadff3;border-radius: 4px;padding: 1px 8px;color: black;font-size: 12px;margin-right: 10px;line-height: 1.4;}</style>
      <!-- ace-static.css -->
      <style>.ace_static_highlight {white-space: pre-wrap;}.ace_static_highlight .ace_gutter {width: 2em;text-align: right;padding: 0 3px 0 0;margin-right: 3px;}.ace_static_highlight.ace_show_gutter > .ace_line {padding-left: 2.6em;}.ace_static_highlight .ace_line {position: relative;}.ace_static_highlight .ace_gutter-cell {-moz-user-select: -moz-none;-khtml-user-select: none;-webkit-user-select: none;user-select: none;top: 0;bottom: 0;left: 0;position: absolute;}.ace_static_highlight .ace_gutter-cell:before {content: counter(ace_line, decimal);counter-increment: ace_line;}.ace_static_highlight {counter-reset: ace_line;}</style>
      <style>.ace-chrome .ace_gutter {background: #ebebeb;color: #333;overflow : hidden;}.ace-chrome .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-chrome {background-color: #FFFFFF;color: black;}.ace-chrome .ace_cursor {color: black;}.ace-chrome .ace_invisible {color: rgb(191, 191, 191);}.ace-chrome .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-chrome .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-chrome .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-chrome .ace_invalid {background-color: rgb(153, 0, 0);color: white;}.ace-chrome .ace_fold {}.ace-chrome .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-chrome .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-chrome .ace_support.ace_type,.ace-chrome .ace_support.ace_class.ace-chrome .ace_support.ace_other {color: rgb(109, 121, 222);}.ace-chrome .ace_variable.ace_parameter {font-style:italic;color:#FD971F;}.ace-chrome .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-chrome .ace_comment {color: #236e24;}.ace-chrome .ace_comment.ace_doc {color: #236e24;}.ace-chrome .ace_comment.ace_doc.ace_tag {color: #236e24;}.ace-chrome .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-chrome .ace_variable {color: rgb(49, 132, 149);}.ace-chrome .ace_xml-pe {color: rgb(104, 104, 91);}.ace-chrome .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-chrome .ace_heading {color: rgb(12, 7, 255);}.ace-chrome .ace_list {color:rgb(185, 6, 144);}.ace-chrome .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-chrome .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-chrome .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-chrome .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-chrome .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-chrome .ace_gutter-active-line {background-color : #dcdcdc;}.ace-chrome .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-chrome .ace_storage,.ace-chrome .ace_keyword,.ace-chrome .ace_meta.ace_tag {color: rgb(147, 15, 128);}.ace-chrome .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-chrome .ace_string {color: #1A1AA6;}.ace-chrome .ace_entity.ace_other.ace_attribute-name {color: #994409;}.ace-chrome .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}</style>
      <!-- export.css -->
      <style>
        body{margin:0 auto;max-width:800px;line-height:1.4}
        #nav{margin:5px 0 10px;font-size:15px}
        #titlearea{border-bottom:1px solid #ccc;font-size:17px;padding:10px 0;}
        #contentarea{font-size:15px;margin:16px 0}
        .cell{outline:0;min-height:20px;margin:5px 0;padding:5px 0;}
        .code-cell{font-family:Menlo,Consolas,'Ubuntu Mono',Monaco,'source-code-pro',monospace;font-size:12px;}
        .latex-cell{white-space:pre-wrap;}
      </style>
      <!-- User CSS -->
      <style> .text-cell {font-size: 15px;}.code-cell {font-size: 12px;}.markdown-cell {font-size: 15px;}.latex-cell {font-size: 15px;}</style>
    </head>
    <body>
      
      <div id="titlearea">
        <h2>CS224n Lecture 12- Subword Models</h2>
      </div>
      <div id="contentarea"><div class="cell text-cell"><h2><b>1. Sub-Word Models - BPE</b></h2><div><b>Byte-Pair Encoding&nbsp;</b></div><div><b><br></b></div><div><i>Neural Machine Translation of Rare Words and Subword units</i>- Sennrich, Haddow, Birch ACL 2016a</div><div><ol><li>Start with a unigram vocabulary of all unicode characters in data.<br></li><li>Merge most frequent ngram pairs -&gt; generate a new ngram which is added to our vocabulary.&nbsp;Iterate until you reach a target vocabulary size.&nbsp;</li></ol>BPE is a word segmentation algorithm, origianlly a compression algorithm done though botton up clustering. It is similar to Huffman encoding. (Appendix 3)<br><div><br></div><div>Then, for each word in the text, do deterministic longest piece segmentation of words and that is our new vocabulary and our new sequence (Note that segmentaiton is only done within words identified by some tokenizer for NLP, usually Moses Tokenizer.)&nbsp;</div><div><br></div><div>If we keep running BPE, we get a vocabulary of common clumps of English letters (most commonly important morphlogical tokens, short words, etc).</div></div><div><div>The segmented words form the new primitives that you input to whatever model you are using.</div></div><div></div><p><br></p><p><img src="resources/D6DEA93431D2C3F6F2E80F8E5FE05984.png" alt="Screen Shot 2020-09-10 at 5.24.20 PM.png" width="602" height="82"><br></p>
</div><div class="cell text-cell"><div><br></div><div><b style="font-size: 15px;">Aside: Google Wordpiece/Sentencepiece model (Used in BERT)</b><br></div><div><b></b></div><div>Google NMT uses a variant of BPE, which was V1 a wordpiece model and V2 a sentencepiece model.<br></div><div>Rather than add char n-gram based on count, they use a greedy approximation to maximizing language model log likelihood to choose the pieces. The next n-gram to add is the next n-gram that maximally reduces perplexity. (Note: A good explanation of this is <a href="https://blog.floydhub.com/tokenization-nlp/">here</a>. The below is given in a bit more detail in Appendix 1).</div>
<div></div><p></p><p><b>Summary of WordPiece/SentencePiece</b><br>
<strong>BPE:</strong> Just uses the frequency of occurrences to identify the best match at every iteration until it reaches the predefined vocabulary size.<br>
<strong>WordPiece:</strong> (Used in v1 of BERT) Similar to BPE and uses frequency occurrences to identify potential merges but makes the final decision based on the likelihood of the merged token<br>
<strong>SentencePiece:</strong> (Used in v2 of BERT) Basically the same as wordpiece, but starts from the whole chunk of raw text, all text converted into unicode, with spaces encoded as “_”. This allows it to be language agnostic.<br>
<strong>Unigram: </strong>(old)&nbsp;A fully probabilistic model which does not use frequency occurrences. Instead, it trains a LM using a probabilistic model, removing the token which improves the overall likelihood the least and then starting over until it reaches the final token limit. The LM probabillistic model aspect was taken and incorporated into BPE as wordpiece.</p>
<div><br></div><div><div style="font-size: 18px;"><b>Practical Takeaways</b></div><div>- Use BPE (when possible)</div><div>- FastText word embeddings used to be the best in 2017 but they have been replaced by comtextual word vectors (ala BERT). Everyone shuold be using them, not traditional word vectors.</div></div><div><br></div><div><br></div><h2><b style="font-size: 17px;">2. Sub-Word Models - Character-Level Models</b></h2><div><div><font color="#323232" style="background-color: rgb(254, 250, 0);">Feel free to skip everything from here down from an applied point of view - in applications most likely you will be using BPE variants and BERT.&nbsp;</font><span style="caret-color: rgb(50, 50, 50); color: rgb(50, 50, 50); background-color: rgb(254, 250, 0);">The rest of these notes are a literature overview about character models;&nbsp;</span><span style="background-color: rgb(254, 250, 0); color: rgb(50, 50, 50);">Some of this material is also quite dated.</span></div></div><div></div><div><br></div><h3></h3><h3><b style="font-size: 16px;">2.0 Purely Character Level NMT Models&nbsp;</b></h3><div style="font-size: 13px; font-weight: normal;">In Appendix 2. There’s been a bunch of work on this but it’s not the most promising. This is more for intellectual interest. Most people in practice use subword models.&nbsp;</div><h3><b><span style="font-size: 16px;">2.1 Using Character-Level to Build Word-Level</span>&nbsp;<span style="font-size: 16px;">Embeddings</span></b></h3><div><b>2.1.1. </b><i>Learning Character Level Representations for POS Tagging</i> (Dos Santos and Zadrozny 2014, early)</div><div>Convolution over characters to generate word embeddings. The task they used the word embeeddings for was that they used a fixed window of word embeddings for PoS tagging.</div><div><br></div><div><b>2.1.2.&nbsp;</b><i>Character-Based LSTM to Build Word Representations</i></div><div>Ling Luis, Marujo, et al Finding Function in Form: Compositional Character Models 2015, early.</div><div>Build word embeddings from characters but from final states of bi-LSTMs (slow)</div><div><br></div><div><b>2.1.3.&nbsp;</b><b><i>Character-Aware Neural Langauge Models</i></b></div><div>Kim, Jernite, Sontag, Rush 2015, early.</div><div>They do convolutions over character embeddings, and do max-pooling-over-time over the outputs of the character embeddings, which what it effectively does is it chooses what n-grams best represent the meaning of the word.</div><div>They then feed that through a highway network, and the output of that at a word level goes into an LSTM network which they use for a langauge model.&nbsp;</div><div>Their char-level languague model works just as well as a word-level LSTM model, but with way less parameters (becuase vocab size is smaller so embedding space can be smaller). The space complixity is never so big of a deal. What is a big deal is that their learned embeddings are somewhat better than original word-level word embeddings and they are able to deal with OOV words quite well.<br></div><div></div></div><div class="cell text-cell"><p><img src="resources/FEC0D8D7150E1A14B87D48F0EE854060.png" alt="Screenshot 2020-01-25 at 1.54.56 PM.png" width="300"><img src="resources/CAB49960E6709C177829F25D4A530852.png" alt="Screenshot 2020-01-25 at 1.56.54 PM.png" width="300">&nbsp;<img src="resources/DD34896A33E68C377DFF9A2EED3A85A2.png" alt="Screenshot 2020-01-25 at 1.57.22 PM.png" width="300">&nbsp;<img src="resources/612FA0CB50F013574AB61DE7FB8858DA.png" alt="Screenshot 2020-01-25 at 1.58.00 PM.png" width="300"></p>
</div><div class="cell text-cell"><p>Interesting insights in to the effects of a character based model. The below table shows three models, and the list is the words that are most similar to the given word.</p>
<ul>
<li>The LSTM word model puts together somewhat similar words based on their appearences in the text.</li>
<li>The char model before the highway network, if we just did the CNN and the maxpool, only remembers thigns about characters, and outputs the words that are spelled similarly to the target word.</li>
<li>But after the highway network, the highway layers are successfully transforming those character level sequences into something that does have semantic meaning. So then the most similar words are words that are semantically similar to the original word, preserving more character-based features. So words that aren’t in the vocab of the model can be captured as well (second-slide)</li>
</ul>
<p><img src="resources/A80AE48815ED68BD8831D2F27E1708F9.png" alt="Screenshot 2020-01-25 at 1.59.56 PM.png" width="300"> <img src="resources/3D0D480EC00EF9BDB6D4585292366A0F.png" alt="Screenshot 2020-01-25 at 2.10.24 PM.png" width="300"></p>
<p>The last column can only be done well by the character level model.</p>
<p>Take-aways:</p>
<ul>
<li>CNNs + highway network over characters can extract rich semantic and structural information</li>
<li>Key thoguht: You can compose ‘building blocks’ to obtain nuanced and powerful models!</li></ul>
<h3><strong>2.2 Hybrid architectures</strong></h3>
<p>A best of both worlds architecture (“Char LSTM”):</p>
<ul>
<li>Translate mostly at the word level</li>
<li>Only go to the character level when needed</li></ul><p><b>2.2.1 Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models </b>(Luong and Manning ACL2016) - Pretty successful (+2 BLEU points)</p>
<p>At training time they decide on a vocabulary of 16000 words, and everything not in that vocabulary gets mapped to &lt;unk&gt;.</p>
<p>Run a standard seq2seq lstm (they actually used a 4 layer one for the encoder and 4 layers for the decoder) with attention.</p>
<p>But for all words that aren’t in the vocabulary, train a character level LSTM with the letters of the unknown word as input and the last hidden state is given to be the hidden state in the word-level representation. (I.e. Work out a char level representaiton of unkonw words using a character level LSTM)</p>
<p>Also, when we decode, we have a softmax with vocabulary of 16000, and if it generates &lt;unk&gt;, we take the representation of &lt;unk&gt; and feed it as the initial input to a decoder character level LSTM and then we have the character level LSTM generate a character seqeunce until it generates the STOP sequence.</p>
<p>(So there are 3 models: the encoder &lt;unk&gt; LSTM, the seq2seq encoder-decoder at the word level, and the decoder &lt;unk&gt; LSTM)</p>
<p>We add the loss of the word level model and both character level models during training.</p>
<p>The decoder uses word-level beam search and also a character level beam search for &lt;unk&gt;.</p>
<p><b>2.2.2&nbsp;</b><b>Hybrid NMT-&nbsp;“Char LSTM”&nbsp;</b>(Luong and Manning 2016) in 2016 for 20.7 which was 2.5 BLEU points better than the SOTA at the time.</p>
<p>One disadvantage, is that the hybrid model doesnt have any representation. The purely character level model is able to use the character level sequence as the conditioning context very effectily, whereas in the hybrid model, although they feel the hidden state of the word level model as the starting hidden state of the character model , it doesn’t have any representation further back than that then what’s in teh word model, so it doesnt do as good of a job of capturing context that allows it to do translation with things like names. (Appendix 4)</p>
<h3><strong>2.3 Chars for Word Embeddings</strong></h3>
<p>See Appendix 3. Most of this has been Superseded by BERT Now.</p>
</div><div class="cell text-cell"><div style="font-size: 0.10000000149011612px;"><span style="font-size: 13px;"></span></div><div><br></div><div style="font-size: 19px;"><b><u>Appendix</u></b></div><div style="font-size: 16px;"><b><br></b></div><div style="font-size: 16px;"><b>Appendix 1</b></div><div style="font-size: 16px;"><div style="font-size: 13px;"><b>Unigram approach</b><br></div><div style="font-size: 13px;"><p>To generate a unigram subword token set you need to first define the desired final size of your token set and also a starting seed subword token set. You can choose the seed subword token set in a similar way to BPE and choose the most frequently occurring substrings. Once you have this in place then you need to:</p><p>1. Work out the probability for each subword token<br>2. Work out a loss value which would result if each subwork token were to be dropped. The loss is worked out via an algorithm described in the paper (an expectation maximization algorithm).<br>3. Drop the tokens which have the largest loss value. You can choose a value here, e.g. drop the bottom 10% or 20% of subword tokens based on their loss calculations. Note you need to keep single characters to be able to deal with out-of-vocabulary words.

<h3>Appendix 3</h3><h3>Chars for Word Embeddings</h3><p>Most of this has been Superseded by BERT Now.</p><p>2.3.1 A Joint Model for Word Embedding and Word Morphology (Cao and Rei 2016)</p><ul><li>Same objective as word2vec, but using characters</li><li>bi-directional LSTM to compute embedding</li><li>Model attempts to capure morphology</li><li>Model can infer roots of words.</li></ul><p><strong>2.3.2 FastText Embeddings -&nbsp;</strong><b>Enriching Word Vectors with Subword Information&nbsp;</b>(Bojanowski, Grave, Joulin, Mikolov. FAIR 2016)</p><p>Aim: A next-generation efficient word2vec-like word representation library, but better for rare words and langauges with lots of morphology.</p><p>An extension of the w2v skip-gram model with character n-grams.</p><p>Represent word as char n-grams augmented with boundary sumbols and as whole word, so</p><p>where = &lt;wh, the, her, ere, re&gt;, &lt;where&gt;.</p><ul><li>Note that &lt;her&gt; or &lt;her is different from ‘her’. Prefix, suffixed and whole words are special</li></ul><p></p></div><div class="cell markdown-cell"><p>So ‘where’ is represented by the above 6 tokens. Recall that in word2vec, you have a center word representation and a context word representation and you learn these representations based on how often center words appear by context words. In this version, they train the system for center vectors corresopnding to all 6 of these representations, and then when they obtain final representations, sum the results.</p>
<p>So their word in context score is: <span class="MathJax" id="MathJax-Element-3-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-1" style="width: 10.391em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.347em; height: 0px; font-size: 111%;"><span style="position: absolute; clip: rect(1.559em, 1000em, 3.124em, -0.491em); top: -2.534em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: STIXGeneral-Italic;">s</span><span class="mo" id="MathJax-Span-4" style="font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-5" style="font-family: STIXGeneral-Italic;">w</span><span class="mo" id="MathJax-Span-6" style="font-family: STIXGeneral-Regular;">,</span><span class="mi" id="MathJax-Span-7" style="font-family: STIXGeneral-Italic; padding-left: 0.188em;">c</span><span class="mo" id="MathJax-Span-8" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-9" style="font-family: STIXGeneral-Regular; padding-left: 0.313em;">=</span><span class="munderover" id="MathJax-Span-10" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 3.341em; height: 0px;"><span style="position: absolute; clip: rect(1.548em, 1000em, 2.907em, -0.449em); top: -2.477em; left: 0em;"><span class="mo" id="MathJax-Span-11" style="font-family: STIXGeneral-Regular; vertical-align: -0.002em;">∑</span><span style="display: inline-block; width: 0px; height: 2.477em;"></span></span><span style="position: absolute; top: -2.012em; left: 0.957em;"><span class="texatom" id="MathJax-Span-12"><span class="mrow" id="MathJax-Span-13"><span class="mi" id="MathJax-Span-14" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.016em;"></span></span><span class="mo" id="MathJax-Span-15" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">∈</span><span class="mi" id="MathJax-Span-16" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">G</span><span class="mo" id="MathJax-Span-17" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">(</span><span class="mi" id="MathJax-Span-18" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">w</span><span class="mo" id="MathJax-Span-19" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">)</span></span></span><span style="display: inline-block; width: 0px; height: 2.309em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-20" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px;"><span style="position: absolute; clip: rect(1.937em, 1000em, 2.784em, -0.509em); top: -2.534em; left: 0em;"><span class="mi" id="MathJax-Span-21" style="font-family: STIXGeneral-Italic;">z</span><span style="display: inline-block; width: 0px; height: 2.534em;"></span></span><span style="position: absolute; clip: rect(1.678em, 1000em, 2.477em, -0.465em); top: -2.653em; left: 0.45em;"><span class="mi" id="MathJax-Span-22" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 2.309em;"></span></span><span style="position: absolute; clip: rect(1.828em, 1000em, 2.623em, -0.501em); top: -2.161em; left: 0.45em;"><span class="mi" id="MathJax-Span-23" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">g</span><span style="display: inline-block; width: 0px; height: 2.309em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-24"><span style="display: inline-block; position: relative; width: 0.863em; height: 0px;"><span style="position: absolute; clip: rect(1.924em, 1000em, 2.721em, -0.487em); top: -2.534em; left: 0em;"><span class="mi" id="MathJax-Span-25" style="font-family: STIXGeneral-Italic;">v</span><span style="display: inline-block; width: 0px; height: 2.534em;"></span></span><span style="position: absolute; top: -2.159em; left: 0.45em;"><span class="mi" id="MathJax-Span-26" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">c</span><span style="display: inline-block; width: 0px; height: 2.309em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.534em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.488em; vertical-align: -0.53em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-3">s(w,c) = \sum_{r \in G(w)} z^T_gv_c</script></p>
<p>Note: (In actuality instead of sharing representaiton for all n-grams, they use a hashing trick to have a fixed number of vectors) That word embedding actually workds pretty successfully, only later overtaken by BERT.</p>
</div><div class="cell text-cell"><p></p><h3>Appendix 4</h3>
<p><font color="#323232" style="background-color: rgb(254, 250, 0);">Really just too much detail, but keep it around for refernece.&nbsp;</font></p><p>Some advantages and pitfalls of hybrid NMT Model.</p><p><img src="resources/F683F36BC04BE606B6B53D37C38720DB.png" alt="Screenshot 2020-01-25 at 2.39.40 PM.png" width="400"></p>
<p>The basic word based model was translating &lt;unk&gt; as po=after because when it sees an &lt;unk&gt; it will look at its attention head and get the source word that has the most attention from the &lt;unk&gt;, and either copy it or translate it as a unigram. Unfortunately, the &lt;unk&gt; was showing the most attention on ‘after’ rather than ‘diagnosis’ so what got translated is ‘after after’ and the word diagnosis is just lost. The hybrid model works beautifully.</p>
<p><img src="resources/AD070BDF6A205FCC7C9703E34976C6D7.png" alt="Screenshot 2020-01-25 at 2.40.07 PM.png" width="400"></p>
<p>One disadvantage, is that while the purely character level model is able to use the character level sequence as the conditioning context very effectily, in the hybrid model, although they feel the hidden state of the word level model as the starting hidden state of the character model, it doesn’t have any representation further back than that then what’s in they word model, so it doesnt do as good of a job of capturing context that allows it to do translation with things like names. The embedding for Shani was fed into the character level model, which had some sense that it was a name but not much else context, and unforuntaley generated G instead of S at the first step, resulting in a snowball that results in Graham.</p>
</div></div>
      <script></script>
    </body>
    </html>
  