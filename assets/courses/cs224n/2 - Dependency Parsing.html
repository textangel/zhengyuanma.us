
    <!DOCTYPE html>
    <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <!-- common.css -->
      <style>* {-webkit-tap-highlight-color: rgba(0,0,0,0);}html {-webkit-text-size-adjust: none;}body {font-family: -apple-system, Helvetica, Arial, sans-serif;margin: 0;padding: 20px;color: #333;word-wrap: break-word;}h1, h2, h3, h4, h5, h6 {line-height: 1.1;}img {max-width: 100% !important;height: auto;}blockquote {margin: 0;padding: 0 15px;color: #777;border-left: 4px solid #ddd;}hr {background-color: #ddd;border: 0;height: 1px;margin: 15px 0;}code {font-family: Menlo, Consolas, 'Ubuntu Mono', Monaco, 'source-code-pro', monospace;line-height: 1.4;margin: 0;padding: 0.2em 0;font-size: 90%;background-color: rgba(0,0,0,0.04);border-radius: 3px;}pre > code {margin: 0;padding: 0;font-size: 100%;word-break: normal;background: transparent;border: 0;}ol {list-style-type: decimal;}ol ol, ul ol {list-style-type: lower-latin;}ol ol ol, ul ol ol, ul ul ol, ol ul ol {list-style-type: lower-roman;}table {border-spacing: 0;border-collapse: collapse;margin-top: 0;margin-bottom: 16px;}table th {font-weight: bold;}table th, table td {padding: 6px 13px;border: 1px solid #ddd;}table tr {border-top: 1px solid #ccc;}table tr:nth-child(even) {background-color: #f8f8f8;}input[type="checkbox"] {cursor: default;margin-right: 0.5em;font-size: 13px;}.task-list-item {list-style-type: none;}.task-list-item+.task-list-item {margin-top: 3px;}.task-list-item input {float: left;margin: 0.3em 1em 0.25em -1.6em;vertical-align: middle;}#tag-field {margin: 8px 2px 10px;}#tag-field .tag {display: inline-block;background: #cadff3;border-radius: 4px;padding: 1px 8px;color: black;font-size: 12px;margin-right: 10px;line-height: 1.4;}</style>
      <!-- ace-static.css -->
      <style>.ace_static_highlight {white-space: pre-wrap;}.ace_static_highlight .ace_gutter {width: 2em;text-align: right;padding: 0 3px 0 0;margin-right: 3px;}.ace_static_highlight.ace_show_gutter > .ace_line {padding-left: 2.6em;}.ace_static_highlight .ace_line {position: relative;}.ace_static_highlight .ace_gutter-cell {-moz-user-select: -moz-none;-khtml-user-select: none;-webkit-user-select: none;user-select: none;top: 0;bottom: 0;left: 0;position: absolute;}.ace_static_highlight .ace_gutter-cell:before {content: counter(ace_line, decimal);counter-increment: ace_line;}.ace_static_highlight {counter-reset: ace_line;}</style>
      <style>.ace-chrome .ace_gutter {background: #ebebeb;color: #333;overflow : hidden;}.ace-chrome .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-chrome {background-color: #FFFFFF;color: black;}.ace-chrome .ace_cursor {color: black;}.ace-chrome .ace_invisible {color: rgb(191, 191, 191);}.ace-chrome .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-chrome .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-chrome .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-chrome .ace_invalid {background-color: rgb(153, 0, 0);color: white;}.ace-chrome .ace_fold {}.ace-chrome .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-chrome .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-chrome .ace_support.ace_type,.ace-chrome .ace_support.ace_class.ace-chrome .ace_support.ace_other {color: rgb(109, 121, 222);}.ace-chrome .ace_variable.ace_parameter {font-style:italic;color:#FD971F;}.ace-chrome .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-chrome .ace_comment {color: #236e24;}.ace-chrome .ace_comment.ace_doc {color: #236e24;}.ace-chrome .ace_comment.ace_doc.ace_tag {color: #236e24;}.ace-chrome .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-chrome .ace_variable {color: rgb(49, 132, 149);}.ace-chrome .ace_xml-pe {color: rgb(104, 104, 91);}.ace-chrome .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-chrome .ace_heading {color: rgb(12, 7, 255);}.ace-chrome .ace_list {color:rgb(185, 6, 144);}.ace-chrome .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-chrome .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-chrome .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-chrome .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-chrome .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-chrome .ace_gutter-active-line {background-color : #dcdcdc;}.ace-chrome .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-chrome .ace_storage,.ace-chrome .ace_keyword,.ace-chrome .ace_meta.ace_tag {color: rgb(147, 15, 128);}.ace-chrome .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-chrome .ace_string {color: #1A1AA6;}.ace-chrome .ace_entity.ace_other.ace_attribute-name {color: #994409;}.ace-chrome .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}</style>
      <!-- export.css -->
      <style>
        body{margin:0 auto;max-width:800px;line-height:1.4}
        #nav{margin:5px 0 10px;font-size:15px}
        #titlearea{border-bottom:1px solid #ccc;font-size:17px;padding:10px 0;}
        #contentarea{font-size:15px;margin:16px 0}
        .cell{outline:0;min-height:20px;margin:5px 0;padding:5px 0;}
        .code-cell{font-family:Menlo,Consolas,'Ubuntu Mono',Monaco,'source-code-pro',monospace;font-size:12px;}
        .latex-cell{white-space:pre-wrap;}
      </style>
      <!-- User CSS -->
      <style> .text-cell {font-size: 15px;}.code-cell {font-size: 12px;}.markdown-cell {font-size: 15px;}.latex-cell {font-size: 15px;}</style>
    </head>
    <body>
      <div id="titlearea">
        <h2>CS224n - Lecture 5 - Dependency Parsing</h2>
      </div>
      <div id="contentarea"><div class="cell text-cell"><div style="font-size: 18px;"><b>Two views of linguistic structure</b></div><div><b>- &nbsp;Phrase Structure =&nbsp;</b><b>Context Free Grammars =&nbsp;</b><b>The Lingustics Approach. Not used much in CS.</b></div><div><img src="resources/72E4561820FF4BBAE09A8BCDC0C18018.png" alt="Screenshot 2019-12-27 at 9.19.49 PM.png" width="622" height="334"><br></div><div>- <b>Dependency Structure</b></div><div>&nbsp;- Look at how words are arguemnts of (modify, are dependents of) other words.</div><div><img src="resources/4ABE6751CFD05A20C0235B830E2A9BC5.png" alt="Screenshot 2019-12-28 at 1.57.30 AM.png" width="619" height="455"><br></div><div>Alternative way to represent a dependency parse is with arrows from words to each other.<br></div><div><img src="resources/312A5641F0B60598DA97B039B47D6AA6.png" alt="Screenshot 2019-12-27 at 9.21.37 PM.png" width="581" height="228"><br></div><div><div>Dependency Parses are listed in treebanks. You can use treebanks to build parsers.</div><div>You can use it to break ambiguities and do all sorts of ML NLP things.</div></div><div><br></div><div style="font-size: 20px;"><b>Dependency Parsing</b></div><div>A sentensed is parsed by choosing for each word what other word (including ROOT) it is a dependent of. The parse usually has some constraints, namely, only one word is a dependent of ROOT and the parse as no cycles (it is a Tree).&nbsp;</div><div>The final issue to decide is whether arrows can cross (<b>non-projective dependency</b>) or not (<b>projective dependency</b>). Most of the time, dependencies don’t cross but it does happen rarely.&nbsp;</div><div><br></div><div><img src="resources/859B5D61D4C898441D07A6DFA214628C.png" alt="Screenshot 2019-12-30 at 9.14.10 PM.png" width="623" height="450"><br></div><div><br></div><div style="font-size: 21px;"><b>Methods of Dependency Parsing:</b></div><div>There have been many methods of dependency parsing historically but the most popular one right now is&nbsp;<b>Transition-based or deterministic dependcy parsing</b> popularized by&nbsp;MaltParser&nbsp;(2008).<br></div><div>This works like a <b>shift-reduce parser</b> (that compilers use).</div><div><br></div><div><b style="font-size: 16px;">Transition-based or deterministic dependcy parsing</b><br></div><div style="font-size: 10px;"><span style="font-size: 13px;">There are many transition schemes: An arc-standard transition-based parser works as follows:</span></div><div>I have a stack (what I’ve built) and a buffer (all the words I haven’t dealt with yet).</div><div>In each step, I could either <i>shift</i> the next word onto the stack, or reduce the stack through the<i> left-arc </i>or <i>right-arc</i> actions by adding arcs between objects on the stack. Possible actions at any given point in time are:</div><div>&nbsp; 1. Shift</div><div>&nbsp; 2. Left-arc</div><div>&nbsp; 3. Right-arc</div><div><br></div><div><font size="3"><b>How to make parsing decisions at each&nbsp;step?</b></font></div><div>Use a machine learning classifier to decide the next action: Shift, left-arc, or right-arc. (Note that if we wnat to add relation tags and there are n of them then we build a classifier over 2n+1 actions.)</div><div><br></div><div style="font-size: 12px;"><span style="font-size: 15px;">Finish condition is buffer is empty and only root is on stack.</span></div><div><img src="resources/69F11E4DDE502827F76CA330CEE8EAD7.png" alt="Screenshot 2019-12-30 at 9.27.30 PM.png" width="622" height="451"><b style="font-size: 16px;"><br></b></div><div><img src="resources/23E2C6BF37C3DF27D7972627DB1C0544.png" alt="Screenshot 2019-12-30 at 9.28.24 PM.png" width="623" height="460"><br></div><div><b style="font-size: 16px;"><br></b></div><div style="font-size: 16px;"><span style="font-size: 19px;"><b>Neural Dependency Parsers</b></span></div><div style="font-size: 11px;"><span style="font-size: 14px;">(Chen and Manning 2014)</span></div><div style="font-size: 11px;"><span style="font-size: 14px;">The features to learn parsers before were hand-engineered, and very very time costly to compute.&nbsp;</span></div><div><span style="font-size: 14px;">Represent each word as a d-dimensional dense vector (i.e. word embedding). Similar words are expected to have close vectors</span></div><div><span style="font-size: 14px;">They also represented <b>part of speech tags</b>&nbsp;and<b> dependency labels</b>&nbsp;as d-dimensional vectors. These are smaller discrete sets (only tens of elements) and are embedded into a space where semantical similarities are preserved (like plural noun is close to singular noun etc).&nbsp;</span></div><div><span style="font-size: 14px;"><br></span></div><div><span style="font-size: 14px;">They had the top positions of the stack and the first positions of buffer, and for each of those positions you had a word and a POS, as well as any dependncies that have already been built. So we have a triple for each position, and each is converted into a distributed repreesntaiton which we are learning, and those distributed repreesntaitons were used to learn the parser. A simple FF NN which was a simple classifier was used to learn the next parse decision.</span></div><div><span style="font-size: 14px;">&nbsp;</span></div><div><img src="resources/A62F98DD907EF9FC770A793760F39CD2.png" alt="Screenshot 2019-12-30 at 9.41.08 PM.png" width="625" height="453"><span style="font-size: 14px;"><br></span></div><div><br></div><div><img src="resources/F4AAF371D281A18C8D6C241DB21ED491.png" alt="Screenshot 2019-12-30 at 9.42.37 PM.png" width="527" height="430"><span style="font-size: 14px;"><br></span></div><div><span style="font-size: 14px;"><br></span></div><div><span style="font-size: 14px;">Now all the parsers are Neural Dependency Parsers and UAS is up to 95%.</span></div><div><span style="font-size: 14px;"><br></span></div><div><b>Evaluating Dependency Parsers:</b>&nbsp;Test correctness on treebank test set. Compute the unlabeled attachment score (UAS) = accuracy of # correct arcs / total # of arcs ignoring labels, or the labeled attachment score (LAS) with labels.</div><div><img src="resources/EBCA6363C084B50F2BA02B2300595980.png" alt="Screenshot 2019-12-30 at 9.47.10 PM.png" width="510" height="162"><b style="font-size: 16px;"><br></b></div><div><b style="font-size: 16px;"><br></b></div><div><b style="font-size: 16px;"><br></b></div><div><b style="font-size: 16px;"><br></b></div><div style="font-size: 23px;"><b>Appendix:</b></div><div><b>&nbsp; &nbsp;- There is possible ambiguities in&nbsp;grammar</b></div><div><img src="resources/6D759036B2F5FB6D08FC63198F331593.png" alt="Screenshot 2019-12-27 at 9.24.33 PM.png" width="613" height="444"><br></div><div><br></div><div><img src="resources/3F99608B9FBA7553B8DF60D98EB483A2.png" alt="Screenshot 2019-12-27 at 9.27.29 PM.png" width="614" height="394"><img src="resources/74F6F1A94642F00FA2FBA654E97E29E7.png" alt="Screenshot 2019-12-28 at 1.52.57 AM.png" width="605" height="426"><br></div><div><br></div><div style="font-size: 21px;"></div><div><br></div><div><br></div><div><br></div><div>&nbsp;</div><div><br></div><div>&nbsp;&nbsp;</div></div></div>
    </body>
    </html>
  