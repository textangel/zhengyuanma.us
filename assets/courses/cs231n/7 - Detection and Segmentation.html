
    <!DOCTYPE html>
    <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <!-- common.css -->
      <style>* {-webkit-tap-highlight-color: rgba(0,0,0,0);}html {-webkit-text-size-adjust: none;}body {font-family: -apple-system, Helvetica, Arial, sans-serif;margin: 0;padding: 20px;color: #333;word-wrap: break-word;}h1, h2, h3, h4, h5, h6 {line-height: 1.1;}img {max-width: 100% !important;height: auto;}blockquote {margin: 0;padding: 0 15px;color: #777;border-left: 4px solid #ddd;}hr {background-color: #ddd;border: 0;height: 1px;margin: 15px 0;}code {font-family: Menlo, Consolas, 'Ubuntu Mono', Monaco, 'source-code-pro', monospace;line-height: 1.4;margin: 0;padding: 0.2em 0;font-size: 90%;background-color: rgba(0,0,0,0.04);border-radius: 3px;}pre > code {margin: 0;padding: 0;font-size: 100%;word-break: normal;background: transparent;border: 0;}ol {list-style-type: decimal;}ol ol, ul ol {list-style-type: lower-latin;}ol ol ol, ul ol ol, ul ul ol, ol ul ol {list-style-type: lower-roman;}table {border-spacing: 0;border-collapse: collapse;margin-top: 0;margin-bottom: 16px;}table th {font-weight: bold;}table th, table td {padding: 6px 13px;border: 1px solid #ddd;}table tr {border-top: 1px solid #ccc;}table tr:nth-child(even) {background-color: #f8f8f8;}input[type="checkbox"] {cursor: default;margin-right: 0.5em;font-size: 13px;}.task-list-item {list-style-type: none;}.task-list-item+.task-list-item {margin-top: 3px;}.task-list-item input {float: left;margin: 0.3em 1em 0.25em -1.6em;vertical-align: middle;}#tag-field {margin: 8px 2px 10px;}#tag-field .tag {display: inline-block;background: #cadff3;border-radius: 4px;padding: 1px 8px;color: black;font-size: 12px;margin-right: 10px;line-height: 1.4;}</style>
      <!-- ace-static.css -->
      <style>.ace_static_highlight {white-space: pre-wrap;}.ace_static_highlight .ace_gutter {width: 2em;text-align: right;padding: 0 3px 0 0;margin-right: 3px;}.ace_static_highlight.ace_show_gutter > .ace_line {padding-left: 2.6em;}.ace_static_highlight .ace_line {position: relative;}.ace_static_highlight .ace_gutter-cell {-moz-user-select: -moz-none;-khtml-user-select: none;-webkit-user-select: none;user-select: none;top: 0;bottom: 0;left: 0;position: absolute;}.ace_static_highlight .ace_gutter-cell:before {content: counter(ace_line, decimal);counter-increment: ace_line;}.ace_static_highlight {counter-reset: ace_line;}</style>
      <style>.ace-chrome .ace_gutter {background: #ebebeb;color: #333;overflow : hidden;}.ace-chrome .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-chrome {background-color: #FFFFFF;color: black;}.ace-chrome .ace_cursor {color: black;}.ace-chrome .ace_invisible {color: rgb(191, 191, 191);}.ace-chrome .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-chrome .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-chrome .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-chrome .ace_invalid {background-color: rgb(153, 0, 0);color: white;}.ace-chrome .ace_fold {}.ace-chrome .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-chrome .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-chrome .ace_support.ace_type,.ace-chrome .ace_support.ace_class.ace-chrome .ace_support.ace_other {color: rgb(109, 121, 222);}.ace-chrome .ace_variable.ace_parameter {font-style:italic;color:#FD971F;}.ace-chrome .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-chrome .ace_comment {color: #236e24;}.ace-chrome .ace_comment.ace_doc {color: #236e24;}.ace-chrome .ace_comment.ace_doc.ace_tag {color: #236e24;}.ace-chrome .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-chrome .ace_variable {color: rgb(49, 132, 149);}.ace-chrome .ace_xml-pe {color: rgb(104, 104, 91);}.ace-chrome .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-chrome .ace_heading {color: rgb(12, 7, 255);}.ace-chrome .ace_list {color:rgb(185, 6, 144);}.ace-chrome .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-chrome .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-chrome .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-chrome .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-chrome .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-chrome .ace_gutter-active-line {background-color : #dcdcdc;}.ace-chrome .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-chrome .ace_storage,.ace-chrome .ace_keyword,.ace-chrome .ace_meta.ace_tag {color: rgb(147, 15, 128);}.ace-chrome .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-chrome .ace_string {color: #1A1AA6;}.ace-chrome .ace_entity.ace_other.ace_attribute-name {color: #994409;}.ace-chrome .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}</style>
      <!-- export.css -->
      <style>
        body{margin:0 auto;max-width:800px;line-height:1.4}
        #nav{margin:5px 0 10px;font-size:15px}
        #titlearea{border-bottom:1px solid #ccc;font-size:17px;padding:10px 0;}
        #contentarea{font-size:15px;margin:16px 0}
        .cell{outline:0;min-height:20px;margin:5px 0;padding:5px 0;}
        .code-cell{font-family:Menlo,Consolas,'Ubuntu Mono',Monaco,'source-code-pro',monospace;font-size:12px;}
        .latex-cell{white-space:pre-wrap;}
      </style>
      <!-- User CSS -->
      <style> .text-cell {font-size: 15px;}.code-cell {font-size: 12px;}.markdown-cell {font-size: 15px;}.latex-cell {font-size: 15px;}</style>
    </head>
    <body>
      
      <div id="titlearea">
        <h2>* CS231N - Lecture 11 - Detection and Segmentation</h2>
      </div>
      <div id="contentarea"><div class="cell text-cell"><div><img src="resources/AB69588F0381D70EBB17E9A37D6CF896.png" alt="Screenshot 2020-02-28 at 10.28.33 AM.png" width="831" height="417"><br></div><b><div><b><br></b></div><span style="font-size: 18px;">1. Semantic Segmentaiton</span></b><div>&nbsp; - Rather than assigning the entire image to a category, we assign every pixel in the image to one of a set of (predefined) categories.</div><div><br></div><div>Naive Approach: A naive approach is sliding window (Appendix 1). No one does this, it’s a bad idea.</div><div><br></div><div>Naive Approach 2: (Appendix 2) Vanilla Fully Concolutional layer. Have the network be a stack of convolutional layers which all preserve the spatial size of the image, with no fully connected layers.&nbsp;</div><div>&nbsp; - The final convolutional layer outputs a tensor of size (C x H x W), where C is the number of categories we care about. This tensor gives our classification scores for every tensor in the input image, at every location in the input image. We put a classification loss at every pixel of the output, and take an average over the classification loss over pixels.</div><div>&nbsp; - Issue. Convolutions that preserve spatial size are incredibly computationally expensive and would take a ton of memory. Normal CNNs shrink the image in later layers, making it manageable, but if the image size is preserved at each depth, it’s very memory intensive. Thus, instead of preserving the image size at every depth, often a fully convolutional network with downsampling is used.</div><div><br></div><div>Approach 3:&nbsp;</div><div>&nbsp; - Similar approach to Approach 2, but use downsampling and upsampling in the middle of the network (Appendix 3). Often used in practice as of 2017. This is much more computationally efficient because we can work with the smaller image for many layers.</div><div>&nbsp; - How does one perform upsampling? Some strategies are Nearest Neighbos, Bed of Nails, Max Unpooling, and Upconvolution (Appendix 3).</div><div>&nbsp; - Max unpooling can be used in a symmetric network where there was a max pooling before. We remember which element was max in the maxpooling step and then restore that element when we do max-unpooling (See pic in Appendix 3).</div><div>&nbsp; &nbsp; - Upconvolution (transpose convolution) is a convolutional layer where instead of dot-product, the filter is multiplied by the weight in the input and directly added to the output at the location. This is the same as a convolution in an inverse direction.&nbsp;</div><div><br></div><div style="font-size: 19px;"><b>2. Classification +&nbsp;Localization</b></div><div>Classify the image, and find out where the object is in the image (draw a bounding box).</div><div><br></div><div>Idea 1: Run an ordinary CNN for classification, but also have an additional task of predicting the locations of the bounding box (x,y,w,h). Now there are two loss functions - one classification loss and the bounding box. Often we just use L2 loss for the bounding box coordinates (And treat the bounding box prediction as a regression problem). (Appendix 4)</div><div><br></div><div>Aside: Using Regression to find locations in an image can be used to do human pose estimation also.<br></div><div><br></div><div style="font-size: 19px;"><b>3. Object Detection</b></div><div>(over a fixed set of categories)</div><div><br></div><div><b>Region Proposals Methods for Object Detection</b>-&nbsp;</div><div>&nbsp; Find ‘blobby’ image patches likely to contain objects with a fast algorithm</div><div>&nbsp; Perform object classification on each patch to see if there’s an object we care about in that region. Selective Search is a commonly used one. (Appendix 5)</div><div><br></div><div><b>R-CNN</b> is a 2014 paper that does region proposal.</div><div>Take each of the regions and warp them to a fixed square size, and run a conv net through each of them to mamek classification decisons. (Appendix 5). R-CNN will also use regression to produce a correction to the bounding box proposed at the region proposal stage. However, R-CNN has some problems (it is pretty slow).</div><div><br></div><div><b>Fast-R-CNN</b> puts the crops on the convolutional feature map, not the original image. They then turn these crops into squares in a differentiable way using an “RoI Pooling Layer”. It is 10x faster to train and test because we’re sharing computation between feature maps. (Appendix 5) Now the time bottleneck is the region proposals.</div><div><br></div><div>This was made even faster by <b>Faster R-CNN</b>, where the region proposals is done by the CNN. They insert a Region proposal Network to predict proposals from features, and jointly train on: 1. RPN classify object/not object, 2. RPN regress box coordinates 3. Final Classification Score, 4. Final box coordinates. [The region proposal network was trained based on % overlap with ground truth objects]<br></div><div><br></div><div><b>YOLO/SSD based Methods for Object Detection<br></b></div><div><b>YOLO/SSD (</b><b>Single Shot Detection)</b></div><div>Instead of doing independent processing for each of these separate regions, instaed we want ot treat this like a regression problem and make all these predictions all at once with some kind of big CNN.</div><div><br></div><div>In each input image, divide that input image into some coarse grid, and within that grid cells, imagine a set of base boxes centered at each grid cell. For each of the big grid cells, predict (Appendix 6)</div><div>&nbsp; - For each of the B base bounding boxes, predict offset of the object from the dimensions of the base boudning box with 5 numbers (dx, dy, dh, dw, confidence)</div><div>&nbsp; - Predict classification scores for each of the C classes, including background.</div><div>Thus assuming 7x7 grid, our output is a 3D &nbsp;7 x 7 x (5 * B + C) tensor. We can have the input be an image, the output be a hand-labelled 3D tensor, and train the whole thing with a giant CNN.</div><div><br></div><div>There is a whole family of obkect detection methods. (Appendix 6) Faster RCC is slower but more accurage, SSD is much faster buch not as accuate. “Huang er al Speed/accuracy rtadeoffs for modern convolutional object detectors” 2017.</div><div><br></div><div>Now there is <b>YOLO-v2.</b></div><div><b><br></b></div><div style="font-size: 20px;"><b>4. Instance Segmentation&nbsp;</b></div><div>Mask R-CNN for Instance Segmentation</div><div>Given a full image, predict 1) the locations and identities of objects in that image, and find out which pixels belong to that image instead of just a bounding box.</div><div><br></div><div>Mask R-CNN was used in 2017.</div><div><img src="resources/CDA386651C1D79339D34B1C3CA4ADC99.png" alt="Screenshot 2020-02-28 at 12.39.22 PM.png" width="768" height="376"><b><br></b></div><div>You take the whole image, and the whole image goes into a convolutional network and a learned region proposal network (like Faster R-CNN). Once we have our region proposals, we project htose proposals onto our convolutional feature map, and instead of just making a classification, we want to predict a segmentation mask for each of these region proposals. So we have two branch betworks - one is eaxctly the same as Faster R-CNN, running classification to tell us what is the category of object in that region proposal, and also predict some bounding box coordinates, and the second branch, which looks like a semantic segmantation mini-network that predicts for each pixel whether or now it is an object.&nbsp;</div><div>(Appendix 7)&nbsp;</div><div>&nbsp;</div><div>Mask R-CNN also does pose estimation, if you add joint coordinates as a learning objective to the classification scores in the Faster-R-CNN top branch! (Appendix 7)</div><div><br></div><div style="font-size: 22px;"><b>Appendix 1:</b></div><div>Naive Sliding window approach to semantic segmentaiton - Slide a small window over each patch of the input image and train a classifier for each window. Super super computationally expensive, thus never used.</div><div><img src="resources/D0A3D409D570CEAAA5E75C930CDA4E2B.png" alt="Screenshot 2020-02-28 at 10.30.49 AM.png" width="808" height="379"><br></div><div><br></div><div>Appendix 2:</div><div>Fully Convolutional Network for Semantic Segmentaiton - A better idea. Will work, but is super computationally expensive because we have multiple stacked convolutions over a high resolution image.&nbsp;<br></div><div><img src="resources/C081591A1CF7B8D1FCBAFE5CE402E346.png" alt="Screenshot 2020-02-28 at 10.31.43 AM.png" width="833" height="395"></div><div><br></div><div>Appendix 3:</div><div><img src="resources/8CF3437D53073C4868B6A4991F45CB7B.png" alt="Screenshot 2020-02-28 at 10.38.49 AM.png" width="832" height="401"><br></div><div>Downsample like Maxpool (or stride 2).</div><div><img src="resources/3F0881291B59E4CBEB9B02A6179D08D9.png" alt="Screenshot 2020-02-28 at 10.44.05 AM.png" width="800" height="369"><br></div><div><img src="resources/A5D314EABAA56D40C0DCE1E40C541947.png" alt="Screenshot 2020-02-28 at 10.45.20 AM.png" width="802" height="409"></div><div>Upsample like Nearest Neighbors, Bed of Nails, or Max Unpooling.</div><div><br></div><div><b>Transpose Convolution (Upconvolution)</b></div><div><img src="resources/DB1884B20C50317B884B8A04C356DFD3.png" alt="Screenshot 2020-02-28 at 11.52.43 AM.png" width="774" height="354"><img src="resources/ACA17EA8F850DA00AB3115B7BC0517C3.png" alt="Screenshot 2020-02-28 at 11.53.47 AM.png" width="769" height="387"><img src="resources/A99BF7DF4C5B4291A5C4575C666CE851.png" alt="Screenshot 2020-02-28 at 11.54.52 AM.png" width="764" height="379"></div><div><br></div><div><br></div><div>Appendix 4: Object Classification and Localization</div><div><img src="resources/10C219AC14DDD72592489C8183756D55.png" alt="Screenshot 2020-02-28 at 11.58.50 AM.png" width="773" height="388"><br></div><div><br></div><div>Aside: Using Regression to find locations in an image can be used to do human pose estimation also.</div><div><img src="resources/0B364F9CA044C7E57729055DBC6FE676.png" alt="Screenshot 2020-02-28 at 12.03.15 PM.png" width="765" height="380"><img src="resources/DA131ADC0CC17664C9C3A066F83671C1.png" alt="Screenshot 2020-02-28 at 12.03.58 PM.png" width="593" height="370"><br></div><div><br></div><div><b>Appendix 5:</b> Region Proposal for Object Detection, R-CNN</div><div><img src="resources/38B0D103C849C979CCD14ED19C2FE548.png" alt="Screenshot 2020-02-28 at 12.10.00 PM.png" width="753" height="377"><br></div><div><br></div><div>R-CNN (2014) a region proposal algo, is pretty slow.</div><div><img src="resources/00FF827BD8D83C671302E81912CBDEDC.png" alt="Screenshot 2020-02-28 at 12.12.51 PM.png" width="758" height="389"><br></div><div><img src="resources/81A89859E71563AB9180201AEFBAD385.png" alt="Screenshot 2020-02-28 at 12.15.58 PM.png" width="765" height="386"><br></div><div>It was imrpoved by Fast R-CNN, which crops in the intermedate convolutional layers, so can share features between crops. This makes it a lot faster, and it can execute in less than a second. Fast R-CNN’s speed is bottlenecked by the region proposals.</div><div><img src="resources/AF9F4E181F73BB866FCA54396F51E6DB.png" alt="Screenshot 2020-02-28 at 12.17.58 PM.png" width="770" height="388"><br></div><div><img src="resources/27CAE5F9CEFF7AC036C85DEC805E7F37.png" alt="Screenshot 2020-02-28 at 12.18.06 PM.png" width="758" height="388"><br></div><div><img src="resources/C140EBE76D83720040D536CEBE908E4A.png" alt="Screenshot 2020-02-28 at 12.18.14 PM.png" width="772" height="390"><br></div><div>This was made even faster by Faster R-CNN, where the region proposal is put inside of the network as a Region Proposal Network.</div><div><img src="resources/B598EE40FF9174B788DF4AB20C323B70.png" alt="Screenshot 2020-02-28 at 12.19.14 PM.png" width="766" height="395"><br></div><div><br></div><div>Appendix 6: YOLO/SSD Object Detection</div><div><img src="resources/7CA4472ECFA097248A13ADC4D86F6404.png" alt="Screenshot 2020-02-28 at 12.27.24 PM.png" width="777" height="381"><br></div><div><img src="resources/F4D7AFF61F65E768EE1E3863A644A17F.png" alt="Screenshot 2020-02-28 at 12.35.29 PM.png" width="687" height="390"></div><div><br></div><div>Aside:<img src="resources/ACE51C907DA7717275ED677823C56AE2.png" alt="Screenshot 2020-02-28 at 12.36.56 PM.png" width="774" height="393"><br></div><div><br></div><div>Appendix 7: Mask R-CNN</div><div>For Instance Segmentation</div><div><br></div><div>Mask R-CNN can do object detection, object segmentation, and pose estimation! With very good results!</div><div><br></div><div>A single Feedforward network is determining how many people are in an image, determining where (which pixels) correspond to that image, and what that person’s pose is, even in crowded scenes, and also runs in real time (5 times per second).</div><div><br></div><div><img src="resources/276B8E900D90CA138FB006498E62F039.png" alt="Screenshot 2020-02-28 at 12.42.40 PM.png" width="772" height="391"><br></div><div><img src="resources/448240793BACDEDDFA7377D549569917.png" alt="Screenshot 2020-02-28 at 12.43.59 PM.png" width="777" height="392"></div><div><img src="resources/104E43E9E9F21F9CD3A55C518954BD41.png" alt="Screenshot 2020-02-28 at 12.45.18 PM.png" width="767" height="388"><br></div></div></div>
    </body>
    </html>
  