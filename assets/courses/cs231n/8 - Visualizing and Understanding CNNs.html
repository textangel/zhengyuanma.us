
    <!DOCTYPE html>
    <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <!-- common.css -->
      <style>* {-webkit-tap-highlight-color: rgba(0,0,0,0);}html {-webkit-text-size-adjust: none;}body {font-family: -apple-system, Helvetica, Arial, sans-serif;margin: 0;padding: 20px;color: #333;word-wrap: break-word;}h1, h2, h3, h4, h5, h6 {line-height: 1.1;}img {max-width: 100% !important;height: auto;}blockquote {margin: 0;padding: 0 15px;color: #777;border-left: 4px solid #ddd;}hr {background-color: #ddd;border: 0;height: 1px;margin: 15px 0;}code {font-family: Menlo, Consolas, 'Ubuntu Mono', Monaco, 'source-code-pro', monospace;line-height: 1.4;margin: 0;padding: 0.2em 0;font-size: 90%;background-color: rgba(0,0,0,0.04);border-radius: 3px;}pre > code {margin: 0;padding: 0;font-size: 100%;word-break: normal;background: transparent;border: 0;}ol {list-style-type: decimal;}ol ol, ul ol {list-style-type: lower-latin;}ol ol ol, ul ol ol, ul ul ol, ol ul ol {list-style-type: lower-roman;}table {border-spacing: 0;border-collapse: collapse;margin-top: 0;margin-bottom: 16px;}table th {font-weight: bold;}table th, table td {padding: 6px 13px;border: 1px solid #ddd;}table tr {border-top: 1px solid #ccc;}table tr:nth-child(even) {background-color: #f8f8f8;}input[type="checkbox"] {cursor: default;margin-right: 0.5em;font-size: 13px;}.task-list-item {list-style-type: none;}.task-list-item+.task-list-item {margin-top: 3px;}.task-list-item input {float: left;margin: 0.3em 1em 0.25em -1.6em;vertical-align: middle;}#tag-field {margin: 8px 2px 10px;}#tag-field .tag {display: inline-block;background: #cadff3;border-radius: 4px;padding: 1px 8px;color: black;font-size: 12px;margin-right: 10px;line-height: 1.4;}</style>
      <!-- ace-static.css -->
      <style>.ace_static_highlight {white-space: pre-wrap;}.ace_static_highlight .ace_gutter {width: 2em;text-align: right;padding: 0 3px 0 0;margin-right: 3px;}.ace_static_highlight.ace_show_gutter > .ace_line {padding-left: 2.6em;}.ace_static_highlight .ace_line {position: relative;}.ace_static_highlight .ace_gutter-cell {-moz-user-select: -moz-none;-khtml-user-select: none;-webkit-user-select: none;user-select: none;top: 0;bottom: 0;left: 0;position: absolute;}.ace_static_highlight .ace_gutter-cell:before {content: counter(ace_line, decimal);counter-increment: ace_line;}.ace_static_highlight {counter-reset: ace_line;}</style>
      <style>.ace-chrome .ace_gutter {background: #ebebeb;color: #333;overflow : hidden;}.ace-chrome .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-chrome {background-color: #FFFFFF;color: black;}.ace-chrome .ace_cursor {color: black;}.ace-chrome .ace_invisible {color: rgb(191, 191, 191);}.ace-chrome .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-chrome .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-chrome .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-chrome .ace_invalid {background-color: rgb(153, 0, 0);color: white;}.ace-chrome .ace_fold {}.ace-chrome .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-chrome .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-chrome .ace_support.ace_type,.ace-chrome .ace_support.ace_class.ace-chrome .ace_support.ace_other {color: rgb(109, 121, 222);}.ace-chrome .ace_variable.ace_parameter {font-style:italic;color:#FD971F;}.ace-chrome .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-chrome .ace_comment {color: #236e24;}.ace-chrome .ace_comment.ace_doc {color: #236e24;}.ace-chrome .ace_comment.ace_doc.ace_tag {color: #236e24;}.ace-chrome .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-chrome .ace_variable {color: rgb(49, 132, 149);}.ace-chrome .ace_xml-pe {color: rgb(104, 104, 91);}.ace-chrome .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-chrome .ace_heading {color: rgb(12, 7, 255);}.ace-chrome .ace_list {color:rgb(185, 6, 144);}.ace-chrome .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-chrome .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-chrome .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-chrome .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-chrome .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-chrome .ace_gutter-active-line {background-color : #dcdcdc;}.ace-chrome .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-chrome .ace_storage,.ace-chrome .ace_keyword,.ace-chrome .ace_meta.ace_tag {color: rgb(147, 15, 128);}.ace-chrome .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-chrome .ace_string {color: #1A1AA6;}.ace-chrome .ace_entity.ace_other.ace_attribute-name {color: #994409;}.ace-chrome .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}</style>
      <!-- export.css -->
      <style>
        body{margin:0 auto;max-width:800px;line-height:1.4}
        #nav{margin:5px 0 10px;font-size:15px}
        #titlearea{border-bottom:1px solid #ccc;font-size:17px;padding:10px 0;}
        #contentarea{font-size:15px;margin:16px 0}
        .cell{outline:0;min-height:20px;margin:5px 0;padding:5px 0;}
        .code-cell{font-family:Menlo,Consolas,'Ubuntu Mono',Monaco,'source-code-pro',monospace;font-size:12px;}
        .latex-cell{white-space:pre-wrap;}
      </style>
      <!-- User CSS -->
      <style> .text-cell {font-size: 15px;}.code-cell {font-size: 12px;}.markdown-cell {font-size: 15px;}.latex-cell {font-size: 15px;}</style>
    </head>
    <body>
      
      <div id="titlearea">
        <h2>CS231N - Lecture 12 - Visualizing and Understanding CNNs</h2>
      </div>
      <div id="contentarea"><div class="cell text-cell"><b>First Layer of CNN</b> - you can just directly visualize the weights as images&nbsp;<div><b>Middle Layers</b> - However, the weights don’t match to pixels in middle layers- not interpreble as images. However, we can visualize each feature activation (each channel) and see where they activate in the image. There are often 128 or 256 feature channels in some layer, most are uninterpreable in many layers but some are interpreable.</div><div><b>Last Layer</b> - To visualize the last (fully connected) layer, we can run the network on many images, and collect the feature vectors generated by the last layer. For example, we can compute and visualize the nearest neighbors in the feature space using fature vectors generated by the last layer. (Appendix 1a)</div><div><br></div><div style="font-size: 16px;"><b><u>Visualizing Middle Layers:</u></b></div><div><b>Maximally Activating Patches (For Middle Layers)</b>: Pick a layer and a channel, ie conv5 in AlexNet is 128x13x13, pick channel 17/128.</div><div>Run many images through the network, record values of chosen channel Visualize image patches that correspond to maximal activations. (Appendix 1b)</div><div><b>Occlusion Experiments (For Original Images): </b>Mask part of the image before feeding to CNN, draw heatmap of probability of the actual output image (network score) at each mask location. (Appendix 1b)</div><div><br></div><div><b>Saliency Maps (For Original Images):</b>&nbsp;Compute the gradient of unnormalized class score with respect to raw pixel images. This tells us, if we wiggle the pixels in the raw image, which pixels affect the class score the most (Appendix 1c)&nbsp;</div><div><b>Saliency Maps for Interpediate Features&nbsp;</b><b>(For Middle Layers)</b>: (Using Guided Backprop) Same as saliency maps, you compute the graident of the unnormalized class score with respect to an intermediate feature. However, Images come out nicer if you only backprop positive gradients through each ReLY (guided Backprop) (Appendix 1c)</div><div><br></div><div style="font-size: 16px;"><b><u>Gradient Based Modifications of Images</u></b></div><div><b>Gradient Ascent</b></div><div>Generate a synthetic image that maximally activates a neuron. We keep the network fixed but backprop to change the pixels of the image to maximize the class score. Gradient Ascent requres a pixel-level regularizer to prevent the pixels of our image from overfitting to the peculiarities of our particular network. (Appendix 1d)</div><div>I* = arg max_I f(I) + R(I)</div><div>Simple regulariser: Penalize L2 norm of generated image. See slides below for more complicated regularizer.</div><div><br></div><div>You can perform gradient ascent not on the raw pizels but on the FC6 latent space representation of the image and you get some really good images. (Appendix 1d)</div><div><br></div><div><b>Adversarial Examples</b></div><div>Start from arbitrary image, Pick an arbitrary class. Modify the image to maximize the class. The image will end up looking the same but the particular network will classify it differently. (Appendix 1d)</div><div><br></div><div style="font-size: 16px;"><b><u>Image Style</u></b></div><div><b>DeepDream</b></div><div>(Appendix 1e)</div><div>Choose an image and a layer in a CNN; do</div><div>1. Forward: Compute activations at chosen layer</div><div>2. Set gradient of chosen layer equal to its activation (magnify existing features detected by the network in the image. Whatever features were found in the image at this layer, amplify these features and modify the image).</div><div>Note that this is equivalent to maximizing the L2norm of the features of that layer of the image.</div><div>3. Backward: Compute gradient on image</div><div>4. Update Image</div><div><br></div><div><b>Feature Inversion&nbsp;</b></div><div>(Appendix 1f)</div><div>Given a CNN feature vector for an image, generate a new image that</div><div>&nbsp; 1. &nbsp;Matches the given feature vector</div><div>&nbsp; 2. “looks natural” (image prior regularisation)</div><div>Tells us how much of the image’s information is stored just in the fatures at that particular layer. If you start at a very deep (low) layer in the network, most of the image can be reconstructed perfectly. As you use higher layers, a lot of edges, colors get blurred and you see more of the semantic information that causes the images to be classified to what it is come out.</div><div><br></div><div><b>Neural Texture Synthesis</b></div><div>(Appendix 1g)</div><div>1. Compute the Gram Matrix</div><div>Each layer of CNN gives a C x H x W tensor of features, which is interpreted as an HxW grid of C-dimensional feature vectors.</div><div>The texture of the image is defined as the average of the co-occurence of the feature vectors.&nbsp;</div><div>In particular, the outer product of any two C-dimensional vectors gives C x C matrix measuring co-occurence. This tells us about the co-occurence of feature activations at different spatial positions. If element i,j in the CxC matrix is large, that means that element i in the first feature vector and element j in the second feature vector are both large. This tells us which features of the feature map tend to activate together at different spatial positions (second order statustics).</div><div><br></div><div>The final Gram matrix (texture matrix) is the average of all HW(HW-1)/2 pairs of outer product co-occurence matrices throught the image, although practically we can just sample and keep a running average.</div><div><br></div><div>The Gram Matrix has thrown away all spatial information that was inside the feature volume through the averaging.</div><div><br></div><div><div>2. Run Gradient Ascent on the Gram Matrix. For every layer, compute its Gram Matrix.</div><div><br></div><div><div>3. Initiate a new image to random noise. Take the new image that you want to generate,</div><div>Pass generated image through CNN, compute gram matrix on each layer, and compute loss as the weighted sum of L2 distance between gram matrices. Backprop and update the image.</div></div></div><div><br></div><div><b>Style Transfer</b></div><div>(Appendix 1g)</div><div>Style Transfer take a content image and a style image and generates a new image minimizing the feature reconstruction loss (feature inversion) of the content image and the&nbsp;gram matrix (style) reconstruction loss of the style image.&nbsp;</div><div><br></div><div><b>Fast Style Transfer:</b></div><div>(Appendix 1g)<b><br></b></div><div>Problem:<b> </b>Style transfer requires many forward backward passes through VGG, very slow! May take ~1hr per image.</div><div>Solution: Train another neural network to train style transfer for us.</div><div>1. Train a feedforward network for each style</div><div>2. Use pretrained CNN to compute same losses as before</div><div>3. After training, stylize images using a single forward pass.</div><div><br></div><div>Essentially, replace the syle transfer backward pass gradient ascent procedure with a single feedforward network. This feedforward network is trained to minimize the joint content and style losses for a fixed style. During test time, we just run the target image through the feedforward network once and the stylized image will come out.&nbsp;</div><div>Takes a few hours to train, but once trained, can do style transfer in real time.&nbsp;</div><div><br></div><div>2017 Google Paper has one networks that can apply many styles, and can even blend styles.</div><div><br></div><div><b>Appendix 1a:</b></div><div><img src="resources/5D1D422C4222F46CA67294DAA1CB64AE.png" alt="IMG_0085.PNG" width="2208" height="1242"><img src="resources/DA7F30BCEFC6A6CD8DB4BAD0990F3488.png" alt="IMG_0086.PNG" width="2208" height="1242">&nbsp;<img src="resources/6B4CBADDC7F22E5E35F234854C2985D5.png" alt="IMG_0087.PNG" width="2208" height="1242"></div><div>Check out high-res versions at&nbsp;<a href="https://cs.stanford.edu/people/karpathy/cnnembed/">https://cs.stanford.edu/people/karpathy/cnnembed/</a></div><div><br></div><div>Appendix 1b:</div><div><img src="resources/CCE0DDA89776A80D94EEB3B2C182A410.png" alt="IMG_0089.PNG" width="2208" height="1242"><br></div><div>I think this is the best way to get intertations of individual filters!</div><div><br></div><div><img src="resources/B832F107EB47A78FE24ADB36DE5A2BD1.png" alt="IMG_0090.PNG" width="2208" height="1242"><br></div><div><br></div><div><b>Appendix 1c: Saliency Maps and Guided Backprop</b></div><div><img src="resources/FAE6F80A748AFD13E4C81D733E000096.png" alt="IMG_0091.PNG" width="2208" height="1242"><br></div><div><br></div><div><img src="resources/8088744CDB8934B2759015DB7BA27114.png" alt="IMG_0094.PNG" width="2208" height="1242"><br></div><div><img src="resources/5504A8EF1B08EE714BF2D54F36E2F8BC.png" alt="IMG_0095.PNG" width="2208" height="1242"><br></div><div><br></div><div><b>Appendix 1d: Gradient Ascent</b></div><div><img src="resources/A4CB4F02D4C32B9378D6014CCAE7BFD6.png" alt="IMG_0096.PNG" width="2208" height="1242"><img src="resources/4EE9FE599D2D3A57E5732239BCA20F3E.png" alt="IMG_0097.PNG" width="2208" height="1242"></div><div><br></div><div><img src="resources/07FAC5CA738D969C304A7C8D5FB624D4.png" alt="IMG_0102.PNG" width="2208" height="1242"></div><div><br></div><div>We can also use gradient Ascent to maximize not only final class scores but also the neurons of some intermediate layer.</div><div><img src="resources/A8E2A3207E4F6A446ECA7172B91AD2E5.png" alt="Screenshot 2020-03-21 at 9.04.38 AM.png" width="1268" height="703"><br></div><div><br></div><div><br></div><div><img src="resources/23101BCA30ABC563AF7577DA368F16D3.png" alt="IMG_0103.PNG" width="2208" height="1242"><b><br></b></div><div><br></div><div><img src="resources/A13CA8B1C349BDD560621DF3B57A06E4.png" alt="IMG_0104.PNG" width="2208" height="1242"><br></div><div><br></div><div><b>Appendix 1e: DeepDream</b></div><div><img src="resources/71BAFA4C429598E2E0C9E4202BB491FC.png" alt="Screenshot 2020-03-21 at 9.17.19 AM.png" width="1251" height="700"><br></div><div><img src="resources/DD0CA023A80D86DA712059FE17C2CA76.png" alt="Screenshot 2020-03-21 at 9.19.40 AM.png" width="1262" height="688"><br></div><div>Jittering the image (2 pixel works fine) is a regularizer which encourages spatial smoothness in the image</div><div>L1 normalization on the gradients prevents updates from being too crazy</div><div>Clip pixel values so that they can be visualized. (This is a projected gradient ascent where we project into the space of valid images)</div><div><br></div><div><b>Appendix 1f: Feature Inversion</b></div><div><img src="resources/B18EA11F2A34C3C6C352222F4B62B95C.png" alt="IMG_0110.PNG" width="2208" height="1242"><img src="resources/6101CC453A42CA6BF2C5821A140BF4B3.png" alt="Screenshot 2020-03-21 at 9.35.50 AM.png" width="1123" height="562"><b><br></b></div><div><br></div><div><b>Appendix 1g: Neural Texture Synthesis</b></div><div><div><b>1. Generate the Gram Matrix</b></div><div>Each layer of CNN gives a C x H x W tensor of features, which is interpreted as an HxW grid of C-dimensional feature vectors.</div><div>The texture of the image is defined as the average of the co-occurence of the feature vectors.&nbsp;</div><div>In particular, the outer product of any two C-dimensional vectors gives C x C matrix measuring co-occurence. This tells us about the co-occurence of feature activations at different spatial positions. If element i,j in the CxC matrix is large, that means that element i in the first feature vector and element j in the second feature vector are both large. This tells us which features of the feature map tend to activate together at different spatial positions (second order statustics).</div><div><br></div><div>The final Gram matrix (texture matrix) is the average of all HW(HW-1)/2 pairs of outer product co-occurence matrices throught the image, although practically we can just sample and keep a running average.</div></div><div>Thr Gram Matrix is essentially the covariance matrix but easier to compute.&nbsp;</div><div><br></div><div><div>The Gram Matrix has thrown away all spatial information that was in the feature matrix, because we averaged over all two vectors at every point in the image. Instead, it just captures the second-order co-occurence statistics for features, and this turns out to be a nice proxy for texture.&nbsp;</div><div>The Gram Matrix is very efficient to compute: Reshape features from C X H x W to C x HW, then Compute G = FF^T</div></div><div><img src="resources/57B1DF0A85CF5EC95AB30B30BFE40F86.png" alt="Screenshot 2020-03-21 at 9.43.28 AM.png" width="1132" height="624"><b><br></b></div><div><br></div><div><img src="resources/5C3CB682159E96F47F75E4DE9993D885.png" alt="Screenshot 2020-03-21 at 9.45.11 AM.png" width="1132" height="635"><br></div><div><br></div><div>2. Run Gradient Ascent on the Gram Matrix. For every layer, compute its Gram Matrix.</div><div><br></div><div><img src="resources/1520A81C59ED0689F5C149A9A16DAD40.png" alt="IMG_0113.PNG" width="2208" height="1242"><br></div><div><br></div><div>3. Take a new image that you want to generate,</div><div>Pass generated image through CNN, compute gram matrix on each layer, and compute loss as the weighted sum of L2 distance between gram matrices. Backprop and update the image.</div><div><img src="resources/E8DEBA1B949F12A402A62D66C13F6B66.png" alt="IMG_0114.PNG" width="2208" height="1242"><br></div><div><br></div><div>This process produces pretty good textures!</div><div>You can compute the loss at different layers and mix it. If you use gram matrices at low layers, you get spoltches of colors. If you use higher layers, you tend to reconstruct larger structures from the original image.</div><div><img src="resources/23FCA610C394B5CF63D19556F992013B.png" alt="Screenshot 2020-03-21 at 9.57.10 AM.png" width="1127" height="625"><br></div><div><br></div><div><b>Style Transfer:</b></div><div>Style Transfer is Texture Synthesis by Gram Matrix Matching + Feature Inversion by Feature Matching.</div><div><div><br></div><div>Style Transfers take a content image and a style image and generates a new image minimizing the feature reconstruction loss (feature inversion) of the content image and the&nbsp;gram matrix (style) reconstruction loss of the style image.&nbsp;</div></div><div><br></div><div><img src="resources/52D0213788D48445E32444962C4077DB.png" alt="Screenshot 2020-03-21 at 9.59.37 AM.png" width="1126" height="621"><b><br></b></div><div><img src="resources/F8A95F897DA2069EEF75445BCF5793A8.png" alt="Screenshot 2020-03-21 at 10.02.31 AM.png" width="1113" height="607"><br></div><div><br></div><div>You can trade off the weighting between the content loss and the style loss.</div><div>It is recommended to use a high enough content layer to capture enough detail.</div><div><br></div><div>If you resize the style image before consrticting the gram matrix, you can control what scale of styles you want to use in the new image&nbsp;</div><div>You can do style transfer with multiple style images, by taking a weighted average of different gram matrices.</div><div><br></div><div><img src="resources/0CD245F337092C7B3CEB4DD4D1F79E38.png" alt="IMG_0117.PNG" width="2208" height="1242"><br></div><div><img src="resources/EA68E6550F01311F0E9379CE2560E6C1.png" alt="IMG_0118.PNG" width="2208" height="1242"><br></div><div><br></div><div><b>Fast Style Transfer</b></div><div><img src="resources/BE2EA7510FF8C9AC928E7D163EA37ECD.png" alt="Screenshot 2020-03-21 at 10.09.38 AM.png" width="1131" height="631"></div><div><br></div><div><img src="resources/F06E4650DA9D757A13FFC9774F8663B9.png" alt="Screenshot 2020-03-21 at 10.09.31 AM.png" width="1128" height="623"><b><br></b></div><div><br></div><div>Replacing batch normalization with instance normalization improves results.</div><div>Google came up with a paper in 2017 to use one network to apply many styles (not one style per network).</div><div>Can also do style blending in real time.</div><div><img src="resources/2C9F8D4880C42907C65AECC16CB3C571.png" alt="Screenshot 2020-03-21 at 10.17.51 AM.png" width="1132" height="629"><img src="resources/D85CA8C9C621028451A14BE881CC913F.png" alt="Screenshot 2020-03-21 at 10.18.48 AM.png" width="1124" height="621"><br></div></div></div>
    </body>
    </html>
  