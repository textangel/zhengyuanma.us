
    <!DOCTYPE html>
    <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <!-- common.css -->
      <style>* {-webkit-tap-highlight-color: rgba(0,0,0,0);}html {-webkit-text-size-adjust: none;}body {font-family: -apple-system, Helvetica, Arial, sans-serif;margin: 0;padding: 20px;color: #333;word-wrap: break-word;}h1, h2, h3, h4, h5, h6 {line-height: 1.1;}img {max-width: 100% !important;height: auto;}blockquote {margin: 0;padding: 0 15px;color: #777;border-left: 4px solid #ddd;}hr {background-color: #ddd;border: 0;height: 1px;margin: 15px 0;}code {font-family: Menlo, Consolas, 'Ubuntu Mono', Monaco, 'source-code-pro', monospace;line-height: 1.4;margin: 0;padding: 0.2em 0;font-size: 90%;background-color: rgba(0,0,0,0.04);border-radius: 3px;}pre > code {margin: 0;padding: 0;font-size: 100%;word-break: normal;background: transparent;border: 0;}ol {list-style-type: decimal;}ol ol, ul ol {list-style-type: lower-latin;}ol ol ol, ul ol ol, ul ul ol, ol ul ol {list-style-type: lower-roman;}table {border-spacing: 0;border-collapse: collapse;margin-top: 0;margin-bottom: 16px;}table th {font-weight: bold;}table th, table td {padding: 6px 13px;border: 1px solid #ddd;}table tr {border-top: 1px solid #ccc;}table tr:nth-child(even) {background-color: #f8f8f8;}input[type="checkbox"] {cursor: default;margin-right: 0.5em;font-size: 13px;}.task-list-item {list-style-type: none;}.task-list-item+.task-list-item {margin-top: 3px;}.task-list-item input {float: left;margin: 0.3em 1em 0.25em -1.6em;vertical-align: middle;}#tag-field {margin: 8px 2px 10px;}#tag-field .tag {display: inline-block;background: #cadff3;border-radius: 4px;padding: 1px 8px;color: black;font-size: 12px;margin-right: 10px;line-height: 1.4;}</style>
      <!-- ace-static.css -->
      <style>.ace_static_highlight {white-space: pre-wrap;}.ace_static_highlight .ace_gutter {width: 2em;text-align: right;padding: 0 3px 0 0;margin-right: 3px;}.ace_static_highlight.ace_show_gutter > .ace_line {padding-left: 2.6em;}.ace_static_highlight .ace_line {position: relative;}.ace_static_highlight .ace_gutter-cell {-moz-user-select: -moz-none;-khtml-user-select: none;-webkit-user-select: none;user-select: none;top: 0;bottom: 0;left: 0;position: absolute;}.ace_static_highlight .ace_gutter-cell:before {content: counter(ace_line, decimal);counter-increment: ace_line;}.ace_static_highlight {counter-reset: ace_line;}</style>
      <style>.ace-chrome .ace_gutter {background: #ebebeb;color: #333;overflow : hidden;}.ace-chrome .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-chrome {background-color: #FFFFFF;color: black;}.ace-chrome .ace_cursor {color: black;}.ace-chrome .ace_invisible {color: rgb(191, 191, 191);}.ace-chrome .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-chrome .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-chrome .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-chrome .ace_invalid {background-color: rgb(153, 0, 0);color: white;}.ace-chrome .ace_fold {}.ace-chrome .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-chrome .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-chrome .ace_support.ace_type,.ace-chrome .ace_support.ace_class.ace-chrome .ace_support.ace_other {color: rgb(109, 121, 222);}.ace-chrome .ace_variable.ace_parameter {font-style:italic;color:#FD971F;}.ace-chrome .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-chrome .ace_comment {color: #236e24;}.ace-chrome .ace_comment.ace_doc {color: #236e24;}.ace-chrome .ace_comment.ace_doc.ace_tag {color: #236e24;}.ace-chrome .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-chrome .ace_variable {color: rgb(49, 132, 149);}.ace-chrome .ace_xml-pe {color: rgb(104, 104, 91);}.ace-chrome .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-chrome .ace_heading {color: rgb(12, 7, 255);}.ace-chrome .ace_list {color:rgb(185, 6, 144);}.ace-chrome .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-chrome .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-chrome .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-chrome .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-chrome .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-chrome .ace_gutter-active-line {background-color : #dcdcdc;}.ace-chrome .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-chrome .ace_storage,.ace-chrome .ace_keyword,.ace-chrome .ace_meta.ace_tag {color: rgb(147, 15, 128);}.ace-chrome .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-chrome .ace_string {color: #1A1AA6;}.ace-chrome .ace_entity.ace_other.ace_attribute-name {color: #994409;}.ace-chrome .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}</style>
      <!-- export.css -->
      <style>
        body{margin:0 auto;max-width:800px;line-height:1.4}
        #nav{margin:5px 0 10px;font-size:15px}
        #titlearea{border-bottom:1px solid #ccc;font-size:17px;padding:10px 0;}
        #contentarea{font-size:15px;margin:16px 0}
        .cell{outline:0;min-height:20px;margin:5px 0;padding:5px 0;}
        .code-cell{font-family:Menlo,Consolas,'Ubuntu Mono',Monaco,'source-code-pro',monospace;font-size:12px;}
        .latex-cell{white-space:pre-wrap;}
      </style>
      <!-- User CSS -->
      <style> .text-cell {font-size: 15px;}.code-cell {font-size: 12px;}.markdown-cell {font-size: 15px;}.latex-cell {font-size: 15px;}</style>
    </head>
    <body>
      <div id="titlearea">
        <h2>CS231N (CS224N)- * Lecture 8 - TF and Pytorch Tutorial</h2>
      </div>
      <div id="contentarea"><b>CPU vs GPU</b>&nbsp; (Appendix 3)</div><div><b>Deep Learning Framework</b></div><div>Allows you to easily build big computational graphs, easily compute gradients, and have optimizations done for you. Numpy can’t run on GPU and you have to compute your own gradients.</div><div><br></div><div><b>Tensorflow -&nbsp;Static computational graph, better for deployment</b></div><div>Appendix 1</div><div>You need to learn a new functional graph language to work with tf. However, The language is very optimized.</div><div><br></div><div>First define computational graph, then run the graph many times (Appendix 3) - Allows graph optimization.</div><div>Once graph is built, can serialize it and run it without the code that built the graph. You can train it in Python and deploy it in C++ or something.&nbsp;</div><div><br></div><div>Requires special tensorflow control flow operator to determine control logic in graph before we run it.&nbsp;</div><div>Requires special tensorflow control flow looping constraicts `tf.foldl` determine loop logic in graph before we run it (functional programming)</div><div><br></div><div><div><b>PyTorch-&nbsp;Dynamic computational graph, better for iteration</b></div><div>Appendix 2</div><div>You can use the imperative programming logic that you’re used to.</div><div><br></div><div>Creates new computataional graph each iteration (dynamic computational graph). Always need to keep the original code around if you want to keep using the graph.&nbsp;</div><div><br></div><div>However, code is a lot cleaner for conditionals and loops.&nbsp;</div><div>Recurrent Networks or Recursive Networks use different length graphs. This is harder to do in TF (static graph) than PyTorch (dynamic graph)</div><div><br></div><div>Facebook has Caffe2 for deployment, where you fill out a model template and then can deploy it.</div><div><br></div></div><div><br></div><div><b>High Level Comparison Between the Three</b></div><div><img src="resources/3864195E6052DD85FAE9FA58BEA06C9C.png" alt="Screenshot 2020-02-26 at 1.19.38 PM.png" width="830" height="398"><br></div><div><br></div><div style="font-size: 17px;"><b>Appendices</b></div><div><b>Appendix 1:</b>&nbsp;</div><div>Tensorflow</div><div><b><br></b></div><div><b>Vanilla Tensorflow Code (copies tf.placeholder from CPU to GPU each call, inefficinent )</b></div><div><img src="resources/285FF9138FED213DC5626310A7DA538B.png" alt="Screenshot 2020-02-26 at 1.20.59 PM.png" width="827" height="406"><br></div><div><img src="resources/0F56858CDDC3DB12C5DF1709DD6C3D98.png" alt="Screenshot 2020-02-26 at 1.21.43 PM.png" width="827" height="405"><br></div><div><img src="resources/F99B8FD66C4AAE51B4200305D986FE8D.png" alt="Screenshot 2020-02-26 at 1.22.06 PM.png" width="830" height="414"><br></div><div><img src="resources/34C101A41B486069EA2E0D6D954689EF.png" alt="Screenshot 2020-02-26 at 1.23.02 PM.png" width="827" height="407"><b><br></b></div><div><img src="resources/B5BD532E7452CA3FDC791E5101FDFBE7.png" alt="Screenshot 2020-02-26 at 1.23.20 PM.png" width="829" height="414"><b><br></b></div><div><img src="resources/A65044A5135BDDB66D4298F949AA967B.png" alt="Screenshot 2020-02-26 at 1.23.26 PM.png" width="833" height="421"><b><br></b></div><div><img src="resources/91DDED52D8F8141B72D99BB901E34EE8.png" alt="Screenshot 2020-02-26 at 1.23.39 PM.png" width="833" height="422"><b><br></b></div><div><img src="resources/D421D556E49DE3ACCD9E6F337EC5DC66.png" alt="Screenshot 2020-02-26 at 1.25.03 PM.png" width="820" height="422"><br></div><div><b><br></b></div><div><b>Using tf.Variable to avoid copying from CPU to GPU each call</b></div><div><img src="resources/B4B011DA2F87553FBCC541B3F7104C48.png" alt="Screenshot 2020-02-26 at 1.25.14 PM.png" width="828" height="423"><b><br></b></div><div>You have to use tf.assign to update tf.Variables</div><div><img src="resources/7930F04934BDFA4EED0CFEB990C9B32B.png" alt="Screenshot 2020-02-26 at 1.25.45 PM.png" width="819" height="415"><b><br></b></div><div><img src="resources/D29988118C518DB277255388789DB189.png" alt="Screenshot 2020-02-26 at 1.26.07 PM.png" width="829" height="422"><b><br></b></div><div><img src="resources/E4B45AE6551339DEAEF0A39804B5D966.png" alt="Screenshot 2020-02-26 at 1.26.31 PM.png" width="832" height="418"><b><br></b></div><div><b>Adding Dummies to make sure that intermediate updates are computed</b></div><div>Tf will not compute any values not necessary to computing the outputs. Therefore, you have to make sure that everything that you want to compute is explicitly output (and set as ‘_’ ). This helps the tf optimizer know what to compute.</div><div><img src="resources/857F6ED0E0DFFC65AFA19EA53508F5EA.png" alt="Screenshot 2020-02-26 at 1.27.20 PM.png" width="774" height="423"><b><br></b></div><div><br></div><div><b>Use tf.train (GradientDescentOptimizer) instead of writing the update step yourself</b></div><div><img src="resources/E3E4ED7F90F8B36704D19E9343569D32.png" alt="Screenshot 2020-02-26 at 1.29.32 PM.png" width="823" height="416"><b><br></b></div><div><b>Use tf.loss (mean_squared_error) instaed of computing the loss yourself</b></div><div><img src="resources/819B4838BD581699D589FA5EB23E853F.png" alt="Screenshot 2020-02-26 at 1.30.45 PM.png" width="821" height="424"><b><br></b></div><div><b>Use tf.layers to automatically set up weights (and bias), initializations, nonlinearity instaed of computing those yourself</b></div><div><img src="resources/2A131A1D24A596B071F5ABA513A1CF48.png" alt="Screenshot 2020-02-26 at 1.31.26 PM.png" width="807" height="419"><b><br></b></div><div><b>Other Nice tf libraries</b></div><div><img src="resources/7F9C10538EBC8C93FC85815A1BC1E78F.png" alt="Screenshot 2020-02-26 at 1.32.33 PM.png" width="828" height="399"><br></div><div><img src="resources/72D6D674251B3B3CA13C7B5081883A45.png" alt="Screenshot 2020-02-26 at 1.33.45 PM.png" width="805" height="403"><br></div><div><img src="resources/F7E090851E3E13D449944A633C3124EF.png" alt="Screenshot 2020-02-26 at 1.34.04 PM.png" width="800" height="404"><br></div><div><img src="resources/1C8C695E064F4220777DB2BE2792DC29.png" alt="Screenshot 2020-02-26 at 1.34.09 PM.png" width="815" height="408"><b><br></b></div><div><b><br></b></div><div><b>Appendix 2:</b></div><div><b>Pytorch</b></div><div><img src="resources/AFEFAFF3A0FA80DFF187035B62827948.png" alt="Screenshot 2020-02-26 at 1.34.57 PM.png" width="822" height="410"><br></div><div><br></div><div><b>Pytorch Tensors are basically numpy arrays that live in CPU/GPU (.cuda)</b></div><div><img src="resources/B0F9F636DECB727DFB6F2E2F12771B94.png" alt="Screenshot 2020-02-26 at 1.35.19 PM.png" width="774" height="413"><br></div><div><img src="resources/5C8F3423AE6159BAD6435A66FF49065C.png" alt="Screenshot 2020-02-26 at 1.36.01 PM.png" width="793" height="426"><br></div><div><br></div><div>PyTorch Variabes are Nodes in a computational graph. They include a <b>.data</b> and a <b>.grad</b>&nbsp;attribute.</div><div>Pytorch Variables and Tensors have exactly the same API</div><div><img src="resources/F658D0E15EE7FE71ED29760BCA7DCC7D.png" alt="Screenshot 2020-02-26 at 1.36.20 PM.png" width="826" height="403"><br></div><div><img src="resources/5A27649D9BFF3CDB18D0737D9AD2AF8D.png" alt="Screenshot 2020-02-26 at 1.36.32 PM.png" width="825" height="402"><br></div><div><img src="resources/EF9403E223F1BFF130E97EF2D3EA8967.png" alt="Screenshot 2020-02-26 at 1.36.40 PM.png" width="822" height="407"><br></div><div><img src="resources/2496EE65465AC2B0100C0E6ADF025C95.png" alt="Screenshot 2020-02-26 at 1.36.50 PM.png" width="831" height="394"><br></div><div>Make sure you zero out the gradients before you recompute them!</div><div><img src="resources/D89782931F7BBA400EC50DD08E44761A.png" alt="Screenshot 2020-02-26 at 1.37.00 PM.png" width="821" height="397"><br></div><div>You can make custom autograd functions in pytorch, although usually you won’t need to.</div><div><img src="resources/30EFC749B93A28D571EB6565819D4BD8.png" alt="Screenshot 2020-02-26 at 1.37.23 PM.png" width="813" height="401"><br></div><div><img src="resources/6B86E868D416DFECD2FCB2B46DC023F8.png" alt="Screenshot 2020-02-26 at 1.37.40 PM.png" width="826" height="406"><br></div><div><b>Use pytorch.nn as a high level wrapper for working with neural nets, like Keras</b></div><div><img src="resources/46C25B311D77ADAF1F824830C92F50B1.png" alt="Screenshot 2020-02-26 at 1.37.50 PM.png" width="828" height="414"><br></div><div><img src="resources/EE9B507FC02CC84B51C796D694F23C03.png" alt="Screenshot 2020-02-26 at 1.38.16 PM.png" width="825" height="408"><br></div><div><img src="resources/23833726858257D16C5D96C10FF5F7C2.png" alt="Screenshot 2020-02-26 at 1.38.22 PM.png" width="830" height="421"><br></div><div><img src="resources/5473BB6410DF6D4004515FD28166F02B.png" alt="Screenshot 2020-02-26 at 1.38.36 PM.png" width="824" height="407"><br></div><div><br></div><div><b>Use pytorch.optim to select different optimizers</b></div><div><img src="resources/DE6492153F1C700DFAED1970C61D7422.png" alt="Screenshot 2020-02-26 at 1.38.50 PM.png" width="819" height="414"><img src="resources/E3EE5CDABF308EBB4FA2AA9700CFAC09.png" alt="Screenshot 2020-02-26 at 1.38.57 PM.png" width="809" height="415"><br></div><div><br></div><div><b>Define Modules with torch.nn.module</b></div><div><img src="resources/0B60FB267980EEF34C309788098840E8.png" alt="Screenshot 2020-02-26 at 1.39.05 PM.png" width="824" height="418"><br></div><div><img src="resources/686183EEC378400CAD334BCBB5FBDEA1.png" alt="Screenshot 2020-02-26 at 1.39.16 PM.png" width="822" height="423"><br></div><div><img src="resources/F298030A511B29EA3F530DA75B132AA0.png" alt="Screenshot 2020-02-26 at 1.39.24 PM.png" width="817" height="428"><br></div><div><img src="resources/47B61DBA717A7A0B0561AB336146F37B.png" alt="Screenshot 2020-02-26 at 1.39.28 PM.png" width="828" height="428"><br></div><div><img src="resources/ED8D99392C2A885075515BC846174A33.png" alt="Screenshot 2020-02-26 at 1.39.40 PM.png" width="829" height="428"><br></div><div><br></div><div><b>Use DataLoader to get minibatching, shuffling, multithreading funcitonality</b></div><div><img src="resources/4242B3277D98CA11910BD3E2A015FB1C.png" alt="Screenshot 2020-02-26 at 1.39.58 PM.png" width="827" height="407"><br></div><div><img src="resources/B63F79E0D360B1B77BA19190C38A14A7.png" alt="Screenshot 2020-02-26 at 1.40.11 PM.png" width="826" height="413"><br></div><div><b><br></b></div><div><b>Use Pytorch Pretrained Models (for vision it’s torchvision)</b></div><div><img src="resources/AC8DD74FDAB9AFEF882879733F482D41.png" alt="Screenshot 2020-02-26 at 1.40.20 PM.png" width="827" height="370"><br></div><div><img src="resources/736A077B98FC2F371A770F234DA7E9CD.png" alt="Screenshot 2020-02-26 at 1.40.31 PM.png" width="824" height="412"><br></div><div><br></div><div><b>Tensorflow vs PyTorch: Static Graphs vs Dynamic Graphs. Loops and Conditionals</b></div><div><img src="resources/0AE206FD9AE4B8004B80DBF047B97FAF.png" alt="Screenshot 2020-02-26 at 1.41.33 PM.png" width="830" height="423"><br></div><div><img src="resources/DDA0BFB0F1B75B3342DC17CABB140DD2.png" alt="Screenshot 2020-02-26 at 1.44.50 PM.png" width="830" height="413"><br></div><div><img src="resources/F2011BF676983C3CC152C66215D1F722.png" alt="Screenshot 2020-02-26 at 1.47.16 PM.png" width="831" height="419"><br></div><div><br></div><div><br></div><div><br></div><div><div><div><b>Appendix 3</b></div><div><div>CUDA - Write C-like code that runs directly on the GPU</div><div>&nbsp; &nbsp;- Higher level APIs - cuBLAS, cuFFT, cuDNN, etc</div><div>&nbsp; &nbsp;- Use cuDNN, 3x faster than unoptimized CUDA.</div><div>OpenCL</div><div>&nbsp; - Similar to UCA but runs on anything</div><div><br></div><div>Data Reads can be bottleneck in training. Solutions:</div><div>&nbsp; - Read all data into RAM</div><div>&nbsp; - Use SSD instead of HDD</div><div>&nbsp; - Use multiple CPU threads and prefetch data and buffer it in RAM.</div></div><div><br></div><div>GPU has their own RAM on the chip<b><br></b></div><div><img src="resources/4460549F76F458332ACBC6AF3FB02B04.png" alt="Screenshot 2020-02-26 at 1.07.49 PM.png" width="814" height="416"></div></div><div><img src="resources/3080A49C06E071026ED121A1E2C2E268.png" alt="Screenshot 2020-02-26 at 1.12.33 PM.png" width="814" height="399"><br></div><div>Using cuDNN optimizations gives 3x speed over vanilla CUDA</div><div><br></div><div><br></div></div></div></div>
    </body>
    </html>
  