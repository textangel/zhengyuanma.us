
    <!DOCTYPE html>
    <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <!-- common.css -->
      <style>* {-webkit-tap-highlight-color: rgba(0,0,0,0);}html {-webkit-text-size-adjust: none;}body {font-family: -apple-system, Helvetica, Arial, sans-serif;margin: 0;padding: 20px;color: #333;word-wrap: break-word;}h1, h2, h3, h4, h5, h6 {line-height: 1.1;}img {max-width: 100% !important;height: auto;}blockquote {margin: 0;padding: 0 15px;color: #777;border-left: 4px solid #ddd;}hr {background-color: #ddd;border: 0;height: 1px;margin: 15px 0;}code {font-family: Menlo, Consolas, 'Ubuntu Mono', Monaco, 'source-code-pro', monospace;line-height: 1.4;margin: 0;padding: 0.2em 0;font-size: 90%;background-color: rgba(0,0,0,0.04);border-radius: 3px;}pre > code {margin: 0;padding: 0;font-size: 100%;word-break: normal;background: transparent;border: 0;}ol {list-style-type: decimal;}ol ol, ul ol {list-style-type: lower-latin;}ol ol ol, ul ol ol, ul ul ol, ol ul ol {list-style-type: lower-roman;}table {border-spacing: 0;border-collapse: collapse;margin-top: 0;margin-bottom: 16px;}table th {font-weight: bold;}table th, table td {padding: 6px 13px;border: 1px solid #ddd;}table tr {border-top: 1px solid #ccc;}table tr:nth-child(even) {background-color: #f8f8f8;}input[type="checkbox"] {cursor: default;margin-right: 0.5em;font-size: 13px;}.task-list-item {list-style-type: none;}.task-list-item+.task-list-item {margin-top: 3px;}.task-list-item input {float: left;margin: 0.3em 1em 0.25em -1.6em;vertical-align: middle;}#tag-field {margin: 8px 2px 10px;}#tag-field .tag {display: inline-block;background: #cadff3;border-radius: 4px;padding: 1px 8px;color: black;font-size: 12px;margin-right: 10px;line-height: 1.4;}</style>
      <!-- ace-static.css -->
      <style>.ace_static_highlight {white-space: pre-wrap;}.ace_static_highlight .ace_gutter {width: 2em;text-align: right;padding: 0 3px 0 0;margin-right: 3px;}.ace_static_highlight.ace_show_gutter > .ace_line {padding-left: 2.6em;}.ace_static_highlight .ace_line {position: relative;}.ace_static_highlight .ace_gutter-cell {-moz-user-select: -moz-none;-khtml-user-select: none;-webkit-user-select: none;user-select: none;top: 0;bottom: 0;left: 0;position: absolute;}.ace_static_highlight .ace_gutter-cell:before {content: counter(ace_line, decimal);counter-increment: ace_line;}.ace_static_highlight {counter-reset: ace_line;}</style>
      <style>.ace-chrome .ace_gutter {background: #ebebeb;color: #333;overflow : hidden;}.ace-chrome .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-chrome {background-color: #FFFFFF;color: black;}.ace-chrome .ace_cursor {color: black;}.ace-chrome .ace_invisible {color: rgb(191, 191, 191);}.ace-chrome .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-chrome .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-chrome .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-chrome .ace_invalid {background-color: rgb(153, 0, 0);color: white;}.ace-chrome .ace_fold {}.ace-chrome .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-chrome .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-chrome .ace_support.ace_type,.ace-chrome .ace_support.ace_class.ace-chrome .ace_support.ace_other {color: rgb(109, 121, 222);}.ace-chrome .ace_variable.ace_parameter {font-style:italic;color:#FD971F;}.ace-chrome .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-chrome .ace_comment {color: #236e24;}.ace-chrome .ace_comment.ace_doc {color: #236e24;}.ace-chrome .ace_comment.ace_doc.ace_tag {color: #236e24;}.ace-chrome .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-chrome .ace_variable {color: rgb(49, 132, 149);}.ace-chrome .ace_xml-pe {color: rgb(104, 104, 91);}.ace-chrome .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-chrome .ace_heading {color: rgb(12, 7, 255);}.ace-chrome .ace_list {color:rgb(185, 6, 144);}.ace-chrome .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-chrome .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-chrome .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-chrome .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-chrome .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-chrome .ace_gutter-active-line {background-color : #dcdcdc;}.ace-chrome .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-chrome .ace_storage,.ace-chrome .ace_keyword,.ace-chrome .ace_meta.ace_tag {color: rgb(147, 15, 128);}.ace-chrome .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-chrome .ace_string {color: #1A1AA6;}.ace-chrome .ace_entity.ace_other.ace_attribute-name {color: #994409;}.ace-chrome .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}</style>
      <!-- export.css -->
      <style>
        body{margin:0 auto;max-width:800px;line-height:1.4}
        #nav{margin:5px 0 10px;font-size:15px}
        #titlearea{border-bottom:1px solid #ccc;font-size:17px;padding:10px 0;}
        #contentarea{font-size:15px;margin:16px 0}
        .cell{outline:0;min-height:20px;margin:5px 0;padding:5px 0;}
        .code-cell{font-family:Menlo,Consolas,'Ubuntu Mono',Monaco,'source-code-pro',monospace;font-size:12px;}
        .latex-cell{white-space:pre-wrap;}
      </style>
      <!-- User CSS -->
      <style> .text-cell {font-size: 15px;}.code-cell {font-size: 12px;}.markdown-cell {font-size: 15px;}.latex-cell {font-size: 15px;}</style>
    </head>
    <body>
      <div id="titlearea">
        <h2>* CS231N - Lecture 15 - Appendices</h2>
      </div>
      <div id="contentarea"><div class="cell text-cell"><div><br></div><div><div style="font-size: 16px;"><b>Appendix 1:&nbsp;</b><b>Algorithms for Efficient Inference</b><b>&nbsp;</b></div><div style="font-size: 14px;"><b>Appendix 1.1</b>&nbsp;<b style="font-size: 16px;">&nbsp;</b>Deep Compression&nbsp;(aka, Pruning, Quantization, and Huffman Encoding)</div><div><div><img src="resources/C80D31DB9576727051D8AAA48A03EF3A.png" alt="Screenshot 2020-03-28 at 1.37.35 PM.png" width="677" height="512"><br></div><div><img src="resources/0385D241A19AC26B5C2CDE86253B4B68.png" alt="Screenshot 2020-03-28 at 1.37.45 PM.png" width="674" height="505"></div></div><div><img src="resources/D242C15A4FB6CDEA443DC7A7AD82A02C.png" alt="Screenshot 2020-03-28 at 1.40.40 PM.png" width="676" height="515"><br></div><div><br></div><div>Pruning + Weight Sharing (Trained Quantization)</div><div><img src="resources/4371287C483FCFF7E5B44298A72A6CEE.png" alt="Screenshot 2020-03-28 at 2.05.42 PM.png" width="678" height="501"><br></div><div><img src="resources/BAAE9FE2788E5D92B0F36EBE124CF499.png" alt="Screenshot 2020-03-28 at 2.07.39 PM.png" width="674" height="509"><br></div><div><img src="resources/8708B2A3210FF989844BBA603B183CCE.png" alt="Screenshot 2020-03-28 at 2.07.53 PM.png" width="678" height="516"><br></div><div><br></div><div><img src="resources/11F1FA914A2D49F296CC422F954A5899.png" alt="Screenshot 2020-03-28 at 2.09.52 PM.png" width="673" height="509"><br></div><div><img src="resources/02D844554C8037890257D4E2E2EE36A8.png" alt="Screenshot 2020-03-28 at 2.09.59 PM.png" width="679" height="507"><br></div><div><br></div><div style="font-size: 17px;"><b>Appendix 1.2:</b>&nbsp;Results of Quantization on Accuracy</div><div><img src="resources/7268589BC8621A69D67789245696668A.png" alt="Screenshot 2020-03-28 at 2.24.12 PM.png" width="672" height="510"><br></div><div><br></div><div style="font-size: 17px;"><b>Appendix 1.3:</b>&nbsp;Effects of Low Rank Approximations on Accuracy</div><div><img src="resources/37970C24886F1F9F765A712BAC2BEC80.png" alt="Screenshot 2020-03-28 at 2.29.59 PM.png" width="670" height="504"><br></div><div><br></div><div>For fully connected layers, the below paper proposes tobreak down one fully connected layer into lots of fully connected layers.</div><div><img src="resources/20AEC28FCCE3B683287C8D6E3EFE3342.png" alt="Screenshot 2020-03-28 at 2.32.06 PM.png" width="673" height="504"><br></div><div><br></div><div style="font-size: 16px;"><b>Appendix 1.4:</b>&nbsp;Binary /Ternary Weights at Inference Slides</div><div><img src="resources/2ED6D1AB6C3BCA2F3B276D5FCABCA301.png" alt="Screenshot 2020-03-28 at 2.36.09 PM.png" width="675" height="509"><br></div><div><img src="resources/E6F5FEE5E97A4788761C1FEEA2FACE80.png" alt="Screenshot 2020-03-28 at 2.36.16 PM.png" width="680" height="513"><br></div><div><img src="resources/912626CA48928E81E6EA674B7E299418.png" alt="Screenshot 2020-03-28 at 2.36.25 PM.png" width="677" height="510"><br></div><div><br></div><div style="font-size: 15px;"><b>Appendix 1.5 Speedup from Winograd Convolutions</b></div><div style="font-size: 15px;">More details about the Winograd Convolution:&nbsp;<a href="https://www.youtube.com/watch?v=Xh2hBMUYKAE" style="font-size: 13px;">https://www.youtube.com/watch?v=Xh2hBMUYKAE</a>.</div><div style="font-size: 15px;"><img src="resources/B19FCBF5977A4269C7E0E1DE9BF8A71D.png" alt="Screenshot 2020-03-28 at 2.46.01 PM.png" width="1055" height="591"><b><br></b></div><div><img src="resources/68BD10ED5F0676CE419D3FE595E329BB.png" alt="Screenshot 2020-03-28 at 2.42.47 PM.png" width="673" height="510"><br></div></div></div><div class="cell text-cell"><div><div><div style="font-size: 17px;"><br></div></div></div></div><div class="cell text-cell"><div><div style="font-size: 17px;"><div style="font-size: 13px;"><div><div style="font-size: 17px;"><br></div></div></div></div><div style="font-size: 17px;"><b style="font-size: 18px;">Appendix 2.&nbsp;<span class="highlighted selected">Optimal</span>&nbsp;<span class="highlighted">Hard</span>ware for Efficient Inference</b><b><br></b></div><div style="font-size: 14px;"><span style="font-size: 15px;">2.1 Google TPU</span></div><div style="font-size: 14px;"><img src="resources/29859D8C7BA2404FF33583C3F7BCC02C.png" alt="Screenshot 2020-03-28 at 2.56.55 PM.png" width="674" height="510"><span style="font-size: 15px;"><br></span></div><div style="font-size: 14px;"><div style="font-size: 13px;">This is Google’s NN workload in 2017 - 61% is dedicated to running Multi-layer perceptrons (for ads). LSTM (for Google langauge services) are 29%.</div><div><br></div></div><div style="font-size: 14px;"><img src="resources/E2E9C36B35D5CF62A43078712D057CD9.png" alt="Screenshot 2020-03-28 at 2.57.01 PM.png" width="676" height="513"><span style="font-size: 15px;"><br></span></div><div style="font-size: 17px;"><br></div></div><div style="font-size: 17px;"><div style="font-size: 13px;"><b>Performacne Bottlenecks</b>:</div><div style="font-size: 13px;">In the left (sloped) region, you are bottle-necked by memory intensity.</div><div style="font-size: 13px;">X axis is ratio between FLOPs and memory bandwith overhead (FLOPS/byte of memory, listed as TPU Opt/Weight Byte in above table)</div><div style="font-size: 13px;">Y axis is the actual attainable peak performance.</div><div style="font-size: 13px;">If you’re only doing small operations, you are bottlenecked by memory bandwith, not arithmetic speed. Most NN do not have full utilization!<br></div><div style="font-size: 13px;"><img src="resources/936033707A22F0719D6C41B6B33E803D.png" alt="Screenshot 2020-03-28 at 3.00.50 PM.png" width="671" height="507"><br></div></div><div><img src="resources/0B665E15138650B7401DD9CC5B378022.png" alt="Screenshot 2020-03-28 at 3.00.55 PM.png" width="672" height="507"><br></div><div>CNNs saturate the peak performance of TPU, but MLP and LSTM only use ~10% of potential throughput, because you cannot batch for real-time use cases.</div><div><img src="resources/37DCC0187B2A7C5814453BDAE5795857.png" alt="Screenshot 2020-03-28 at 3.01.01 PM.png" width="680" height="508"><br></div><div><img src="resources/52834A2E7F0D217068B7BF904BE3BA51.png" alt="Screenshot 2020-03-28 at 3.03.37 PM.png" width="679" height="506"><br></div>=&gt; Intro to EIE.<div><br></div></div><div class="cell text-cell"><div><br></div><img src="resources/960A2FB962CE729021C804C2756F33D7.png" alt="Screenshot 2020-03-28 at 3.22.59 PM.png" width="674" height="501"><img src="resources/D169E8155CBB11898A01462AE1B9245B.png" alt="Screenshot 2020-03-28 at 3.23.04 PM.png" width="677" height="504"><div><div><div style="font-size: 17px;"><b><br></b></div><div style="font-size: 17px;"><b>Appendix N: Misc</b></div><div>Where is time and energy consumed by large models?&nbsp;<b>in the memory access</b><br><div>To make deep learning more efficient, we should design hardware that improves memory access efficiency.</div><div><img src="resources/93F18E598158D0EF8D82237BC77F4577.png" alt="Screenshot 2020-03-18 at 4.12.52 PM.png" width="699" height="490"><br></div><div><br></div><div><b>About Hardware:</b></div><div><img src="resources/081296164A6AA37EF1E86563192A88EC.png" alt="Screenshot 2020-03-18 at 4.13.17 PM.png" width="653" height="484"></div><div><img src="resources/099940F88BD38CE08012992310FA67D7.png" alt="Screenshot 2020-03-18 at 4.14.06 PM.png" width="687" height="500"><br></div></div></div><div><img src="resources/61ACB92D05426B03DDE6917C6E3B6505.png" alt="Screenshot 2020-03-28 at 2.18.33 PM.png" width="665" height="462"><br></div><div>Takeaway: use small representations for your numbers!</div></div><div><br></div><div><b>About SqeezeNet:</b></div><div><b>Han</b>&nbsp;et al put out SqueezeNet. They used 1) Squeeze (1x1 conv) layers to reduce feature depth before convolution branches, 2) no fully connected layers. Everything is fully convolutional and the last layer is global average pooling. SqueezeNet is 50x smaller than AlexNet in its basic form and 510x smaller than AlexNet in compressed form.<b>&nbsp;</b>Model inference is ~3x faster on CPU and GPU.</div><div><br></div></div><div class="cell text-cell"></div><div class="cell text-cell"><div><div><div style="font-size: 17px;"><div style="font-size: 13px;"><div><div style="font-size: 17px;"><br></div></div></div></div></div></div></div></div>
    </body>
    </html>
  