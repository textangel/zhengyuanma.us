
    <!DOCTYPE html>
    <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <!-- common.css -->
      <style>* {-webkit-tap-highlight-color: rgba(0,0,0,0);}html {-webkit-text-size-adjust: none;}body {font-family: -apple-system, Helvetica, Arial, sans-serif;margin: 0;padding: 20px;color: #333;word-wrap: break-word;}h1, h2, h3, h4, h5, h6 {line-height: 1.1;}img {max-width: 100% !important;height: auto;}blockquote {margin: 0;padding: 0 15px;color: #777;border-left: 4px solid #ddd;}hr {background-color: #ddd;border: 0;height: 1px;margin: 15px 0;}code {font-family: Menlo, Consolas, 'Ubuntu Mono', Monaco, 'source-code-pro', monospace;line-height: 1.4;margin: 0;padding: 0.2em 0;font-size: 90%;background-color: rgba(0,0,0,0.04);border-radius: 3px;}pre > code {margin: 0;padding: 0;font-size: 100%;word-break: normal;background: transparent;border: 0;}ol {list-style-type: decimal;}ol ol, ul ol {list-style-type: lower-latin;}ol ol ol, ul ol ol, ul ul ol, ol ul ol {list-style-type: lower-roman;}table {border-spacing: 0;border-collapse: collapse;margin-top: 0;margin-bottom: 16px;}table th {font-weight: bold;}table th, table td {padding: 6px 13px;border: 1px solid #ddd;}table tr {border-top: 1px solid #ccc;}table tr:nth-child(even) {background-color: #f8f8f8;}input[type="checkbox"] {cursor: default;margin-right: 0.5em;font-size: 13px;}.task-list-item {list-style-type: none;}.task-list-item+.task-list-item {margin-top: 3px;}.task-list-item input {float: left;margin: 0.3em 1em 0.25em -1.6em;vertical-align: middle;}#tag-field {margin: 8px 2px 10px;}#tag-field .tag {display: inline-block;background: #cadff3;border-radius: 4px;padding: 1px 8px;color: black;font-size: 12px;margin-right: 10px;line-height: 1.4;}</style>
      <!-- ace-static.css -->
      <style>.ace_static_highlight {white-space: pre-wrap;}.ace_static_highlight .ace_gutter {width: 2em;text-align: right;padding: 0 3px 0 0;margin-right: 3px;}.ace_static_highlight.ace_show_gutter > .ace_line {padding-left: 2.6em;}.ace_static_highlight .ace_line {position: relative;}.ace_static_highlight .ace_gutter-cell {-moz-user-select: -moz-none;-khtml-user-select: none;-webkit-user-select: none;user-select: none;top: 0;bottom: 0;left: 0;position: absolute;}.ace_static_highlight .ace_gutter-cell:before {content: counter(ace_line, decimal);counter-increment: ace_line;}.ace_static_highlight {counter-reset: ace_line;}</style>
      <style>.ace-chrome .ace_gutter {background: #ebebeb;color: #333;overflow : hidden;}.ace-chrome .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-chrome {background-color: #FFFFFF;color: black;}.ace-chrome .ace_cursor {color: black;}.ace-chrome .ace_invisible {color: rgb(191, 191, 191);}.ace-chrome .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-chrome .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-chrome .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-chrome .ace_invalid {background-color: rgb(153, 0, 0);color: white;}.ace-chrome .ace_fold {}.ace-chrome .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-chrome .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-chrome .ace_support.ace_type,.ace-chrome .ace_support.ace_class.ace-chrome .ace_support.ace_other {color: rgb(109, 121, 222);}.ace-chrome .ace_variable.ace_parameter {font-style:italic;color:#FD971F;}.ace-chrome .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-chrome .ace_comment {color: #236e24;}.ace-chrome .ace_comment.ace_doc {color: #236e24;}.ace-chrome .ace_comment.ace_doc.ace_tag {color: #236e24;}.ace-chrome .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-chrome .ace_variable {color: rgb(49, 132, 149);}.ace-chrome .ace_xml-pe {color: rgb(104, 104, 91);}.ace-chrome .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-chrome .ace_heading {color: rgb(12, 7, 255);}.ace-chrome .ace_list {color:rgb(185, 6, 144);}.ace-chrome .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-chrome .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-chrome .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-chrome .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-chrome .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-chrome .ace_gutter-active-line {background-color : #dcdcdc;}.ace-chrome .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-chrome .ace_storage,.ace-chrome .ace_keyword,.ace-chrome .ace_meta.ace_tag {color: rgb(147, 15, 128);}.ace-chrome .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-chrome .ace_string {color: #1A1AA6;}.ace-chrome .ace_entity.ace_other.ace_attribute-name {color: #994409;}.ace-chrome .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}</style>
      <!-- export.css -->
      <style>
        body{margin:0 auto;max-width:800px;line-height:1.4}
        #nav{margin:5px 0 10px;font-size:15px}
        #titlearea{border-bottom:1px solid #ccc;font-size:17px;padding:10px 0;}
        #contentarea{font-size:15px;margin:16px 0}
        .cell{outline:0;min-height:20px;margin:5px 0;padding:5px 0;}
        .code-cell{font-family:Menlo,Consolas,'Ubuntu Mono',Monaco,'source-code-pro',monospace;font-size:12px;}
        .latex-cell{white-space:pre-wrap;}
      </style>
      <!-- User CSS -->
      <style> .text-cell {font-size: 15px;}.code-cell {font-size: 12px;}.markdown-cell {font-size: 15px;}.latex-cell {font-size: 15px;}</style>
    </head>
    <body>
      <div id="nav"><div>Next: <a href='CS231N - Lecture 15 - Appendices.html'>* CS231N - Lecture 15 - Appendices</a>, Previous: <a href='CS231N - Lecture 16 - Adversarial Examples - Ian Goodfellow.html'>CS231N - Lecture 16 - Adversarial Examples - Ian Goodfellow</a>, Up: <a href='index.html'>Index</a></div></div>
      <div id="titlearea">
        <h2>* CS231N - Lecture 15 - Efficient Methods and Hardware for Deep Learning</h2>
      </div>
      <div id="contentarea"><div class="cell text-cell"><div><span style="font-size: 15px;">Note that appendices for this lecture are in a separate note becase of space limitations.</span></div><div><span style="font-size: 15px;"><br></span></div><div style="font-size: 11px;"><b style="font-size: 19px;">1. Algorithms for Efficient Inference</b></div><div style="font-size: 15px;"><b>1.1. Deep Compression</b></div><div>Song Han: We can applying truning and retraining, Trained Quantization, and Huffman Encoding to pretrained Neural Networks like ResNet, reducing the networks to ~3% their original size. These together is called&nbsp;<b>Deep Compression.</b><b><br></b></div><div><b><br></b></div><div><b>A. Pruning</b></div><div>(See Appendix 1.1)</div><div>(1) Train connectedly -&gt; (2) Prune Connections Permanently -&gt; (3) Retrain Weights -&gt; Iterate (2)(3) ~10x. You can iteratively prune a big neural network of 60 million parameters down to 10% of original size.&nbsp;</div><div><br></div><div><b>B. Weight Sharing (Trained Quantization)</b><br></div><div>Group similar weights according to k-means, and keep just the centroids</div><div><div><img src="resources/6D04BE5A1BC5B079CA7C597A673800CA.png" alt="Screenshot 2020-03-28 at 1.59.14 PM.png" width="678" height="510"><br></div><div><img src="resources/32B46781F59A1A7AD0CE202CF03688D3.png" alt="Screenshot 2020-03-28 at 1.59.28 PM.png" width="675" height="510"><br></div><div>This procedure is done per layer. We only need 4 bits per weight for each conv layer and 2 bits &nbsp;per weight for each fully connected layer! (Q: Are centroids recomputed after n steps or is the cluster index always fixed? Yes, seems that they are always fixed.)</div></div><div><br></div><div><b>Pruning + Trained Quantization</b> together can compress the model to 3% of its original size with negligeable loss of accuracy! (Appendix 1.1)</div><div><br></div><div>C. <b>Huffman Encoding</b></div><div>Finally, we can use huffman encoding to use more bits to represent infrequent numbers (quanta/centroids from the above step) and less bits to represent frequent numbers. (Appendix 1.1)</div><div><br></div><div>Pruning + Trained Quantization + Huffman encoding together is called&nbsp;<b>Deep Compression.&nbsp;</b></div><div><b><br></b></div><div style="font-size: 16px;"><b>1.2.&nbsp;</b><b>Quantization</b></div><div>Train the NN with normal floating point numbers, and then gather statistics for each layer- what is the maximum number, minimum number, how much bits is enough this dynamic range. Then convert the trained nueural network to a compressed quantized version: use that number of bits for the integer part of the weights for that layer, and the rest of the 8 or 7 bits remaining for the rest of the 8 bit representation.</div><div><br></div><div><img src="resources/FC8539101CBB5279CA58674EE0C45366.png" alt="Screenshot 2020-03-28 at 2.20.23 PM.png" width="674" height="507"><br></div><div>If you stick to 8 bits, the accuracy drop will be almost negligeable (Appendix 1.2).<br></div><div><br></div><div style="font-size: 16px;"><b>1.3. Low Rank Approximation</b></div><div>Break up k x k x d convolutions into k x k x c and 1 x 1 x d where c &lt; d. (MobileNet).</div><div><div>We can use deep compression to achieve a speedup of ~2x with almost no loss of accuracy. (Appendix 1.3)<br></div><div>We can also apply low rank approximation to fully connected layers, by using SVD to factorize the fully connected layers into two lower-dimensionality sublayers. (Appendix 1.3)</div></div><div><img src="resources/82FD533E4E421B1CD42F6D74699FE5EE.png" alt="Screenshot 2020-03-28 at 2.25.42 PM.png" width="646" height="494"><br></div><div><br></div><div style="font-size: 15px;"><b>1.4. Binary/Ternary Weight</b></div><div>Use the full precision weights during training time, but swap out all weights with (1, -1, 0) at infernece time. Basically can get the same accuracy as AlexNet. (Appendix 1.4)</div><div><br></div><div style="font-size: 16px;"><b>1.5. Winograd Convolution</b></div><div>An equivalent (lossless) but faster convolutional method to training neural networks.&nbsp;</div><div><a href="https://blog.usejournal.com/understanding-winograd-fast-convolution-a75458744ff">https://blog.usejournal.com/understanding-winograd-fast-convolution-a75458744ff</a>&nbsp;is a good summary. First of all note that convolutions are not implemented by sliding in real GPUs, they are implemented by a single multiply, making each convolutional kernel a column vector (this requires some replicating data and some additional memory overhead).</div><div>Winograd convolutions is an alternate formulation of the multiply that uses less matmuls and more adds. (It produces 2.25x speedup)</div><div><br></div><div><img src="resources/7495561802BA0FCF18429739C4D99AA8.png" alt="Screenshot 2020-03-28 at 2.40.14 PM.png" width="678" height="509"><br></div><div><img src="resources/D661BD9172ADC7B71CD612DBE0221917.png" alt="Screenshot 2020-03-28 at 2.40.21 PM.png" width="676" height="507"><br></div><div>Winograd convolutions transforms the convnet layer into a batch of direct matrix multiplications. (Appendix 1.5) Winograd convolutions bring about 2x speedup. (Appendix 1.5)&nbsp;</div><div></div></div><div class="cell text-cell"><b>2. Algorithms for Efficient Training</b><div style="font-size: 14px;"><b>2.1. Parallelization</b></div><div style="font-size: 14px;">Moore’s law is stopping because of the power constraint. To make up for this, the number of cores in each machine is increasing. So what we really need is parallelization.<br></div><div style="font-size: 14px;"><b>&nbsp; &nbsp; - Data Parallelization -&nbsp;</b>Training more data at once with the model, with more cores. N x more cores effectively increases the effective batch size by n, but it requires coordinated weight update across cores. Google Uses a parameter server which syncs all the executors that are making the weight updates.</div><div style="font-size: 14px;"><b>&nbsp; &nbsp; - Model Parallelization</b><b><br></b></div><div style="font-size: 14px;">Idea 1: For example, chop the image into 4 2x2 blocks such that each processor only handles 1/4 of the image.</div><div style="font-size: 14px;">Idea 2: Parallelize by feature maps.</div><div style="font-size: 14px;"><b>&nbsp; &nbsp; - Hyperparameter Parallelization</b></div><div style="font-size: 14px;">Easy to get 16-64 GPUs training one model in parallel.</div><div style="font-size: 14px;"><br></div><div style="font-size: 14px;"><b>2.2. Mixed Precision Training with FP16 and FP32</b></div><div style="font-size: 14px;">We can do the multiplucation using FP16 (gaining a lot of speedup), but we still have to do the summation in 32bit. The result is 32bit float.&nbsp;</div><div style="font-size: 14px;">More specifically, we can do the entire feedforward part of training using FP16, and all intermediate steps to compute activation gradients in FP16. When we get the gradient of a weight in FP16, we convert it back to FP32 and add it to our original weight. The result is an updated weight in FP32.</div><div style="font-size: 14px;"><img src="resources/E98D38A87853AD71AE949FC7A72C5ADE.png" alt="Screenshot 2020-03-28 at 4.27.24 PM.png" width="666" height="504"><br></div><div style="font-size: 14px;">Basically the results are the same as full precision weight.</div><div style="font-size: 14px;"><br></div><div style="font-size: 14px;"><b>2.3. Model Distillation (Teacher-Student)</b></div><div style="font-size: 14px;">Take multiple large, powerful “teacher” NNs and use them to train a much smaller “student” NN.</div><div style="font-size: 14px;">Take a geometric ensemble of the prediction softmax outputs, and soften them by dividing the softmax score by a “temperature" (make values very close to 1 not so close to 1, very close to 0 not so close to 0, but keep relative ranking).</div><div style="font-size: 14px;"><img src="resources/BCC9FA99A815F0D0A419B1AEC6AC8A94.png" alt="Screenshot 2020-03-28 at 4.35.09 PM.png" width="675" height="509"><br></div><div style="font-size: 14px;"><img src="resources/6A09C3F59990F3AA1DB06969121B7C61.png" alt="Screenshot 2020-03-28 at 4.35.13 PM.png" width="671" height="503"><br></div><div style="font-size: 14px;"><img src="resources/6C666D4ECC9F6B56CE7EEE0CA49B9223.png" alt="Screenshot 2020-03-28 at 4.35.17 PM.png" width="669" height="506"><br></div><div style="font-size: 14px;"><br></div><div style="font-size: 14px;"><b>4. Dense-Sparse-Dense Training.</b></div><div style="font-size: 14px;">A better regularization.</div><div style="font-size: 14px;">Train NN first with all connections, then drop out a certain amount of connections, retrain to convergence, then add back in the original connections, and train them again to convergence.</div><div style="font-size: 14px;">DSD training arrives at better minima.</div><div style="font-size: 14px;">Leads to about 1% absolute (3% relative) accuracy improvement in almost all models.</div><div style="font-size: 14px;"><img src="resources/4EAD4C88D6E79F451FC3EFCA02D48C60.png" alt="Screenshot 2020-03-28 at 4.38.17 PM.png" width="671" height="504"><br></div></div><div class="cell text-cell"><div style="font-size: 13px;"><div style="font-size: 17px;"><br></div></div><div><div style="font-size: 13px;"><b style="font-size: 18px;">3. Optimal Hardware for Efficient Inference</b><br></div><div style="font-size: 0.10000000149011612px;"><b>Asafs</b></div><div style="font-size: 13px;">All ASICs for NN training have a common goal, which is to&nbsp;<b>minimize memory access during training and inference.</b></div><div style="font-size: 13px;">Eyeriss (MIT) - Use “RS Dataflow” to minimize offchip access</div><div style="font-size: 13px;">Dadiannao (Chinese CAS) buffer all weights on on-chip DRAM so that they wont have to talk to extenal memory</div><div style="font-size: 13px;">Google TPU - uses 8 bit integer (for dense NN infernece)</div><div style="font-size: 13px;">Stanford EIE - for sparse NN inference</div><div style="font-size: 13px;"><br></div><div style="font-size: 13px;"><b>3.1 Google TPU -&nbsp;</b></div><div style="font-size: 13px;">TPUs can be slotted into the disk drive, up to 4 cards/server.</div><div style="font-size: 13px;">At the center is a HUGE matrix multiply unit - 256x256 = 65536 8 bit units, at 700MHz clock rate, can perform 65536 * 84 * 700M = 92 Tera-ops.</div><div style="font-size: 13px;">Has a really large unified on-chip buffer of 24MB (3x as much on chip memory as GPU).</div><div style="font-size: 13px;">Powered by 2 DDR3 DRAM channels.</div><div style="font-size: 13px;"><br></div><div style="font-size: 13px;">Area: Half the size of GPU/TPU, power consumtion is 1/2 of CPU/GPU, Peak TOPS/s is ~2x CPU/GPU. (Appendix 2.1)</div><div style="font-size: 13px;"><br></div><div style="font-size: 13px;"><img src="resources/FEBB3A967A119952E383F889A23E8F6B.png" alt="Screenshot 2020-03-28 at 2.51.03 PM.png" width="673" height="508"></div><div style="font-size: 13px;"><br></div><br style="font-size: 13px;"><div style="font-size: 13px;">The performance (Tera-Ops/sec) on GPU/CPU/TPU of different models are shown below against theoretical maximum. Why are they so far below the rooflines? It is because in MLPs and LSTMs, there is a requirement for fast, real-time, low-latency computation. This prevents us from batching more at inference -&gt; low ops/byte.</div><div style="font-size: 13px;"><img src="resources/2556F2F93C2D62545484D864A601709A.png" alt="Screenshot 2020-03-28 at 3.02.21 PM.png" width="676" height="509"><br></div><div style="font-size: 13px;">The solution to this problem is to have a less memory footprint so that you can reduce the memory footprint requirement. One solution is to compress the model. Can we build hardware that can do inference directly on a compressed model? -&gt; EIE.</div><div style="font-size: 13px;"><br></div><div style="font-size: 13px;"><b>3.2 Stanford EIE (Efficient Inference Engine)</b></div><div style="font-size: 13px;"><b>Challenge: Hardware that can infer on compressed model.</b></div><div style="font-size: 13px;">1. Anything times 0 is 0 &nbsp;- Don’t store it, don’t compute on it (Appendix 2.2)</div><div style="font-size: 13px;">2. Weight sharing - Change all weights to similar discrete values.</div><div style="font-size: 13px;"><img src="resources/2488D81C0FC6212F3F46F40241C44EE6.png" alt="Screenshot 2020-03-28 at 3.10.49 PM.png" width="680" height="510"></div><div style="font-size: 13px;">Note:</div><div style="font-size: 13px;"></div><div style="font-size: 13px;">- “Sparse Weight” refers to keeping the weights in a sparse matrix. The 2x difference between the computational and memory savings is due to overhead of keeping track of indices.<br></div><div style="font-size: 13px;">- “Sparse Activation” means, after ReLU, if activation is 0, then ignore it.&nbsp;</div><div style="font-size: 13px;">- Weight sharing to use 4 bits, not 32 bit weights.&nbsp;</div><div style="font-size: 13px;"><br></div><div style="font-size: 13px;">This is how the weights are stored logically and physically:</div><div style="font-size: 13px;"><img src="resources/538B1CF0B97B2F791C90E01D3AAD90A2.png" alt="Screenshot 2020-03-28 at 3.14.52 PM.png" width="674" height="510"><br></div><div style="font-size: 13px;">The lecture has an animation for how the multiplication is computed, but basically it goes down the array a (top row) and skips all entryes which are 0, goes down the relevant column to find all entries that are 0, performs the multiplucation, and then adds the results to the running sum array.</div><div style="font-size: 13px;"><br></div><div style="font-size: 13px;">There is then a lookup table used to get the 16-bit real weight form the 4 bit virtual weight used to do the computation<img src="resources/709BE901A53BDE30F24DA88B18066FCE.png" alt="Screenshot 2020-03-28 at 3.18.33 PM.png" width="673" height="507"></div><div style="font-size: 13px;"><br></div><div style="font-size: 13px;">Hardware Architecture of EIE at high level.</div><div style="font-size: 16px;"><img src="resources/6F24528921030BB7E46ED0062C8A974F.png" alt="Screenshot 2020-03-28 at 3.19.29 PM.png" width="677" height="513"><b><br></b></div><div style="font-size: 16px;"><br></div><div style="font-size: 16px;">Immense Speedups! 189x faster than CPU, 24,000x more energy efficient than CPU.&nbsp;<span style="font-size: 15px;">EIEs have higher throughput and are more energy efficient than other ASICs also by an order of magnitude.</span></div><div style="font-size: 24px;"></div></div></div><div class="cell text-cell"><div style="font-size: 17px;"><b style="font-size: 18px;">4. Optimal Hardware for Efficient Training</b><br></div><div style="font-size: 24px;"><span style="font-size: 14px;">(A bit dated as of 2017)</span><span style="font-size: 14px;"><br></span></div><div style="font-size: 24px;"><span style="font-size: 14px;">Computation and memory bandwidth, and communication (NV link) determines your overall performance.</span></div><div style="font-size: 24px;"><span style="font-size: 14px;">1. GPUs -&nbsp;</span><span style="font-size: 14px;">NVIDIA PASCAL GP100</span></div><div><span style="font-size: 14px;">750 GB/s bandwidth - this is very high bandwidth. 300W, 16nm process, 160 GB/s NV link</span></div><div><span style="font-size: 14px;"><br></span></div><div><span style="font-size: 14px;">2. Nvidia Volta GV100</span></div><div><span style="font-size: 14px;">120 Tensor TFLOPS specifically designed for deep learning. Plus 15 FP32 TFLOPS with Tensor Core.</span></div><div><span style="font-size: 14px;">16GB HBM2 @ 900 GB/s bandwidth&nbsp;</span></div><div><span style="font-size: 14px;">12 nm process</span></div><div><span style="font-size: 14px;">die size 815 mm2 - really large chip</span></div><div><span style="font-size: 14px;">suported by 300GB/s NVlink.</span></div><div><span style="font-size: 14px;"><br></span></div><div><span style="font-size: 14px;">The Tensor Core of Volta is a new instruction that performs 4x4x4 fused-multiply-add mixed precision operations in one single clock cycle. 12x increase in throughput for Volta V100 compared to Pascal P100.</span></div><div><span style="font-size: 14px;"><br></span></div><div><span style="font-size: 14px;">Training is 2.4x faster in Tensor Core than FP32, Inference is 3.7x faster.</span></div><div><span style="font-size: 14px;"><br></span></div><div><span style="font-size: 14px;">The below figure is a single SM (Stream Multiprocessor) in the Volta CV100. The SM is partitioned into 4 processing blocks, in each block the cores are listed below.</span></div><div><img src="resources/7CE028FB14A348D18E28EA29781988B8.png" alt="Screenshot 2020-03-28 at 4.51.26 PM.png" width="675" height="507"></div><div><br></div><div><img src="resources/F1ED49A35B4744AF5865044036A9422B.png" alt="Screenshot 2020-03-28 at 4.51.59 PM.png" width="676" height="511"></div><div><img src="resources/7C72CC66F357E089D221AA52714A042A.png" alt="Screenshot 2020-03-28 at 4.55.40 PM.png" width="671" height="502"><br></div><div>TPU has a really large on-chip memory and is a lot more efficient.&nbsp;</div><div><br></div><div><br></div></div></div>
    </body>
    </html>
  