
    <!DOCTYPE html>
    <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <!-- common.css -->
      <style>* {-webkit-tap-highlight-color: rgba(0,0,0,0);}html {-webkit-text-size-adjust: none;}body {font-family: -apple-system, Helvetica, Arial, sans-serif;margin: 0;padding: 20px;color: #333;word-wrap: break-word;}h1, h2, h3, h4, h5, h6 {line-height: 1.1;}img {max-width: 100% !important;height: auto;}blockquote {margin: 0;padding: 0 15px;color: #777;border-left: 4px solid #ddd;}hr {background-color: #ddd;border: 0;height: 1px;margin: 15px 0;}code {font-family: Menlo, Consolas, 'Ubuntu Mono', Monaco, 'source-code-pro', monospace;line-height: 1.4;margin: 0;padding: 0.2em 0;font-size: 90%;background-color: rgba(0,0,0,0.04);border-radius: 3px;}pre > code {margin: 0;padding: 0;font-size: 100%;word-break: normal;background: transparent;border: 0;}ol {list-style-type: decimal;}ol ol, ul ol {list-style-type: lower-latin;}ol ol ol, ul ol ol, ul ul ol, ol ul ol {list-style-type: lower-roman;}table {border-spacing: 0;border-collapse: collapse;margin-top: 0;margin-bottom: 16px;}table th {font-weight: bold;}table th, table td {padding: 6px 13px;border: 1px solid #ddd;}table tr {border-top: 1px solid #ccc;}table tr:nth-child(even) {background-color: #f8f8f8;}input[type="checkbox"] {cursor: default;margin-right: 0.5em;font-size: 13px;}.task-list-item {list-style-type: none;}.task-list-item+.task-list-item {margin-top: 3px;}.task-list-item input {float: left;margin: 0.3em 1em 0.25em -1.6em;vertical-align: middle;}#tag-field {margin: 8px 2px 10px;}#tag-field .tag {display: inline-block;background: #cadff3;border-radius: 4px;padding: 1px 8px;color: black;font-size: 12px;margin-right: 10px;line-height: 1.4;}</style>
      <!-- ace-static.css -->
      <style>.ace_static_highlight {white-space: pre-wrap;}.ace_static_highlight .ace_gutter {width: 2em;text-align: right;padding: 0 3px 0 0;margin-right: 3px;}.ace_static_highlight.ace_show_gutter > .ace_line {padding-left: 2.6em;}.ace_static_highlight .ace_line {position: relative;}.ace_static_highlight .ace_gutter-cell {-moz-user-select: -moz-none;-khtml-user-select: none;-webkit-user-select: none;user-select: none;top: 0;bottom: 0;left: 0;position: absolute;}.ace_static_highlight .ace_gutter-cell:before {content: counter(ace_line, decimal);counter-increment: ace_line;}.ace_static_highlight {counter-reset: ace_line;}</style>
      <style>.ace-chrome .ace_gutter {background: #ebebeb;color: #333;overflow : hidden;}.ace-chrome .ace_print-margin {width: 1px;background: #e8e8e8;}.ace-chrome {background-color: #FFFFFF;color: black;}.ace-chrome .ace_cursor {color: black;}.ace-chrome .ace_invisible {color: rgb(191, 191, 191);}.ace-chrome .ace_constant.ace_buildin {color: rgb(88, 72, 246);}.ace-chrome .ace_constant.ace_language {color: rgb(88, 92, 246);}.ace-chrome .ace_constant.ace_library {color: rgb(6, 150, 14);}.ace-chrome .ace_invalid {background-color: rgb(153, 0, 0);color: white;}.ace-chrome .ace_fold {}.ace-chrome .ace_support.ace_function {color: rgb(60, 76, 114);}.ace-chrome .ace_support.ace_constant {color: rgb(6, 150, 14);}.ace-chrome .ace_support.ace_type,.ace-chrome .ace_support.ace_class.ace-chrome .ace_support.ace_other {color: rgb(109, 121, 222);}.ace-chrome .ace_variable.ace_parameter {font-style:italic;color:#FD971F;}.ace-chrome .ace_keyword.ace_operator {color: rgb(104, 118, 135);}.ace-chrome .ace_comment {color: #236e24;}.ace-chrome .ace_comment.ace_doc {color: #236e24;}.ace-chrome .ace_comment.ace_doc.ace_tag {color: #236e24;}.ace-chrome .ace_constant.ace_numeric {color: rgb(0, 0, 205);}.ace-chrome .ace_variable {color: rgb(49, 132, 149);}.ace-chrome .ace_xml-pe {color: rgb(104, 104, 91);}.ace-chrome .ace_entity.ace_name.ace_function {color: #0000A2;}.ace-chrome .ace_heading {color: rgb(12, 7, 255);}.ace-chrome .ace_list {color:rgb(185, 6, 144);}.ace-chrome .ace_marker-layer .ace_selection {background: rgb(181, 213, 255);}.ace-chrome .ace_marker-layer .ace_step {background: rgb(252, 255, 0);}.ace-chrome .ace_marker-layer .ace_stack {background: rgb(164, 229, 101);}.ace-chrome .ace_marker-layer .ace_bracket {margin: -1px 0 0 -1px;border: 1px solid rgb(192, 192, 192);}.ace-chrome .ace_marker-layer .ace_active-line {background: rgba(0, 0, 0, 0.07);}.ace-chrome .ace_gutter-active-line {background-color : #dcdcdc;}.ace-chrome .ace_marker-layer .ace_selected-word {background: rgb(250, 250, 255);border: 1px solid rgb(200, 200, 250);}.ace-chrome .ace_storage,.ace-chrome .ace_keyword,.ace-chrome .ace_meta.ace_tag {color: rgb(147, 15, 128);}.ace-chrome .ace_string.ace_regex {color: rgb(255, 0, 0)}.ace-chrome .ace_string {color: #1A1AA6;}.ace-chrome .ace_entity.ace_other.ace_attribute-name {color: #994409;}.ace-chrome .ace_indent-guide {background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAACCAYAAACZgbYnAAAAE0lEQVQImWP4////f4bLly//BwAmVgd1/w11/gAAAABJRU5ErkJggg==") right repeat-y;}</style>
      <!-- export.css -->
      <style>
        body{margin:0 auto;max-width:800px;line-height:1.4}
        #nav{margin:5px 0 10px;font-size:15px}
        #titlearea{border-bottom:1px solid #ccc;font-size:17px;padding:10px 0;}
        #contentarea{font-size:15px;margin:16px 0}
        .cell{outline:0;min-height:20px;margin:5px 0;padding:5px 0;}
        .code-cell{font-family:Menlo,Consolas,'Ubuntu Mono',Monaco,'source-code-pro',monospace;font-size:12px;}
        .latex-cell{white-space:pre-wrap;}
      </style>
      <!-- User CSS -->
      <style> .text-cell {font-size: 15px;}.code-cell {font-size: 12px;}.markdown-cell {font-size: 15px;}.latex-cell {font-size: 15px;}</style>
    </head>
    <body>
      <div id="titlearea">
        <h2>* CS231N - Lecture 13 - Generative Models (PixelCNN, VAE, GAN)</h2>
      </div>
      <div id="contentarea"><div class="cell text-cell">Unsupervised learning allows us to use a lot more data, since unlabelled data is very abundant.<div><b><br></b></div><div><b>Generative Models</b><br><div>Generative Models do densitiy estimation of our underlying data distribution.&nbsp;</div><div>&nbsp; - We can do explicit density estimation (explicitly define and solve for p(x))</div><div>&nbsp; - Or we can do implicit density estimation - model that can sample from p(x) without explicitly defining it. (Appendix 1a)</div><div><br></div><div><b>PixelRNN, PixelCNN</b></div><div>(Appendix 1b)</div><div>Pros: Decently good quality images</div><div>Cons: Quite slow to generate pixel by pixel.&nbsp;</div><div><br></div><div>These are an explicit density estimation generative model (called a fully visible belief network).</div><div>This model uses the chain rule to decompose likelihood of image x into product of the probabilities of each pixel given the previous pixels.<br></div><div><b>PixelRNN</b>&nbsp;generates image pixels starting from corner, dependency on previous pixels modeled using an LSTM. (This is slow becauses it uses sequential generation!)</div><div><b>PixelCNN</b> was introduced to speed things up a but. We still generate image pixels starting from the corner, but deprenency on previous pixels now modeled using a CNN over context region. Maximise the likelihood of the training image. (Training time is faster, but test time still must proceed sequentially -&gt; still slow)</div><div><br></div><div><b>Variational Autoencoders</b></div><div>(Intuition in Appendix 1c.1., Details in Appendix 1.c.2)</div><div>Pros: Allows inference of q(z|x), where z is e.g. the amount of smile in an image, which can be a useful feature representaiton for other tasks.&nbsp;</div><div>Cons: Samples blurrier and lower qulaity compared to state of the art (GANs).</div><div><br></div><div>Variational Autoencoders is an approximate density generative model. We optimise an intractible density fucntion using a tractable lower bound as a proxy.</div><div><b>Autoencoders</b> are models (generally CNN for images) that compress the representations of inputs by being trained to compress a representaiton and then regenerate itself. The architecture is a encoder-decoder, the encoder is 4-layer convolutional network, decoder is 4-layer <b>upconvolutional</b> network. If the original image is x, we call the compressed feature representaiton z.<br></div><div>The loss function is L2 loss of the reconstructed image w.r.t. the original image&nbsp;</div><div>|| x - hat(x) || ^2<br></div><div><br></div><div><div>In autoencoders, the compressed feature representaion z we start with is discrete, and we can generate a different image for each z.</div><div><b>In variational autoencoders</b>, we take a distribition over the values of z. For this reason, we do not produce z directly from x in the encoder, but produce the mean and diagonal covariance of z given x.</div><div>Then, we don’t produce x directly from z in the decoder, but produce the mean and diagonal covariance of x given z. (See Appendix 1c)</div></div><div><br></div><div><b>Generative Adversarial Network (GAN)</b></div><div>(Appendix 1.d)</div><div>Direct implicit density (no explicit density function) sampling.<br></div><div><br></div><div><br></div><div><b>Appendix 1a: Intro to Generative Models</b><br><div><img src="resources/1E40222EC9441190C58FE8E500CEF30A.png" alt="IMG_0128.PNG" width="2208" height="1242"><br></div></div></div><div><br></div><div><img src="resources/AF7976FA724274756EFED89C7C925F8F.png" alt="Screenshot 2020-03-21 at 10.30.35 AM.png" width="942" height="524"><br></div><div><br></div><div><b>Appendix 1b:&nbsp;</b></div><div>PixelRNN, PixelCNN</div><div>This model uses the chain rule to decompose likelihood of image x into product of the probabilities of each pixel given the previous pixels.<br></div><div><br></div><div><img src="resources/9A15C742FC477957EEE2B3F0EB78A1BC.png" alt="Screenshot 2020-03-21 at 10.34.38 AM.png" width="939" height="534"><br></div><div><img src="resources/5AFBA744AC98A5233A9B80DE0C6321C1.png" alt="IMG_0134.PNG" width="2208" height="1242"></div><div><br></div><div><img src="resources/A5868552185A288680EFECA092ABD461.png" alt="Screenshot 2020-03-21 at 10.38.19 AM.png" width="938" height="532"><br></div><div><br></div><div><img src="resources/EC4A6E508E8CB2C910EB6FB6A1DD8F39.png" alt="IMG_0136.PNG" width="2208" height="1242"><br></div><div><img src="resources/54ECDF6014B4001F9DCC8F5ADBF72075.png" alt="IMG_0138.PNG" width="2208" height="1242"><br></div><div><br></div><div><b>Appendix 1c.1:&nbsp;</b><b>Variational Autoencoders (VAE) Intuition</b></div><div><a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a><br></div><div>At this point, a natural question that comes in mind is “what is the link between autoencoders and content generation?”. Indeed, once the autoencoder has been trained, we have both an encoder and a decoder but still no real way to produce any new content. At first sight, we could be tempted to think that, if the latent space is regular enough (well “organized” by the encoder during the training process), we could take a point randomly from that latent space and decode it to get a new content. The decoder would then act more or less like the generator of a Generative Adversarial Network.<b><br></b></div><div><br></div><div><div>To illustrate this point, let’s consider the example we gave previously in which we described an encoder and a decoder powerful enough to put any N initial training data onto the real axis (each data point being encoded as a real value) and decode them without any reconstruction loss. In such case, the high degree of freedom of the autoencoder that makes possible to encode and decode with no information loss (despite the low dimensionality of the latent space) <b>leads to a severe overfitting</b> implying that some points of the latent space will give meaningless content once decoded. If this one dimensional example has been voluntarily chosen to be quite extreme, we can notice that the problem of the autoencoders latent space regularity is much more general than that and deserve a special attention.</div></div><div><br></div><div><img src="resources/FA7C99A2084EB58DF93C7D60FCBFD1EE.png" alt="Screenshot 2020-03-21 at 12.02.58 PM.png" width="965" height="348"><br></div><div>When thinking about it for a minute, this lack of structure among the encoded data into the latent space is pretty normal. Indeed, nothing in the task the autoencoder is trained for enforce to get such organisation:<b> the autoencoder is solely trained to encode and decode with as few loss as possible, no matter how the latent space is organised.</b><br></div><div><br></div><div>One possible solution to obtain such regularity is to introduce explicit regularisation during the training process. <b>Thus, as we briefly mentioned in the introduction of this post, a variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process. </b>In particular, the latent space representation of the image is forced to be very close to a standard normal distribution.<br></div><div><br></div><div>…</div><div><br></div><div><b>The Reparameterization Sampling trick</b></div><div>The overall architecture is then obtained by concatenating the encoder and the decoder parts. However we still need to be very careful about the way we sample from the distribution returned by the encoder during the training. The sampling process has to be expressed in a way that allows the error to be backpropagated through the network. A simple trick, called <b>reparametrisation trick</b>, is used to make the gradient descent possible despite the random sampling that occurs halfway of the architecture and consists in using the fact that if z is a random variable following a Gaussian distribution with mean g(x) and with covariance h(x) then it can be expressed as<br></div><div><img src="resources/FC2E7C98EC8200BDC732F47D082EB295.png" alt="Screenshot 2020-03-21 at 12.09.25 PM.png" width="908" height="473"><br></div><div><b><br></b></div><div><b>Appendix 1c.2:&nbsp;</b><b>Variational</b><b>&nbsp;Autoencoders (VAE) Details</b><b><br></b></div><div>VAEs are approximate density models - which is, we do not optimize the density directly, instead we optimize an approximate density which is a lower bound on the density.</div><div><img src="resources/721ACEF02B194B611B3B517FA32BA9B5.png" alt="IMG_0139.PNG" width="2208" height="1242"><br></div><div><br></div><div><b>Intro: Autoencoders:</b></div><div>Autoencoders are models that compress the representations of inputs by being trained to compress a representaiton and then regenerate itself. Encoder-decoder architecture, decoder is 4-layer upconvolutional network.</div><div>The loss function is L2 loss of the reconstructed image w.r.t. the original image&nbsp;</div><div>|| x - hat(x) || ^2</div><div><br></div><div><img src="resources/98150BBE4E33163191CB67DF00CEFB2C.png" alt="IMG_0142.PNG" width="2208" height="1242"><br></div><div><br></div><div>After training, we throw away the decoder. The encoder now can generate the low-dimensionality feature representation of the image. We can use the encoder outputs as the inputs to some supervised model (make sure to do some fine-tuning on the new objective).</div><div><img src="resources/11FC2562FDF296507ADA4AA076160C42.png" alt="Screenshot 2020-03-21 at 10.52.36 AM.png" width="939" height="530"><img src="resources/0BF1B7A32027117F602BD64C8B77B1EE.png" alt="IMG_0143.PNG" width="2208" height="1242"><br></div><div><br></div><div><b>Variational Autoencoders:</b></div><div>We assume training data is generated from underlying unobserved representaiton z (where z is latent factors used to generate).</div><div>We first sample from a prior (like a gaussian) that determines our expected distribution of the latent factor. Then, given the sampled latent factor, we same from the space image space conditional on z. (z is like how much pose, how much smile).</div><div>Sample from p(z) using Gaussian.</div><div>Sample from p(x|z) using neural network.</div><div><img src="resources/2CD62C8954C5E57B8A431170B2C087EF.png" alt="IMG_0145.PNG" width="2208" height="1242"><b><br></b></div><div><img src="resources/FC8FBF2FA6E918ABBBB622E328F23222.png" alt="Screenshot 2020-03-21 at 10.59.29 AM.png" width="945" height="531"><br></div><div><img src="resources/099A1173C2735C8C96CFDE274C1B4B7F.png" alt="Screenshot 2020-03-21 at 10.59.34 AM.png" width="935" height="526"><br></div><div><img src="resources/4D07627EB3248C8A0D607EB37C5296BF.png" alt="Screenshot 2020-03-21 at 11.01.32 AM.png" width="938" height="525"><br></div><div>Problem: The integral is intractible (the computation cannot be computed in reasonable time).</div><div>Solution. Instead of bothering with p(x) (See below slide) which we use to compute the posterior density p(z|s), what we really care about is the posterior density itself. So instaed we define an additional encoder network q(z|x) that approximates p(z|x) directly.&nbsp;</div><div><img src="resources/11C08FAC992E7C5C0A4494EA00507E72.png" alt="IMG_0150.PNG" width="2208" height="1242"><br></div><div><br></div><div>In autoencoders, the compressed feature representaion z we start with is discrete, and we can generate a different image for each z.</div><div>In variational autoencoders, we take a distribition over the values of z. For this reason, we do not produce z directly from x in the encoder, but produce the mean and diagonal covariance of z given x.</div><div>Then, we don’t produce x directly from z in the decoder, but produce the mean and diagonal covariance of x given z.</div><div><img src="resources/9830AD5C3A85EB5C99069005D69C4946.png" alt="Screenshot 2020-03-21 at 11.35.39 AM.png" width="946" height="523"><br></div><div><br></div><div><img src="resources/92BEEE14702ED168B63FA15098ACD197.png" alt="Screenshot 2020-03-21 at 11.38.27 AM.png" width="942" height="530"><br></div><div><br></div><div><img src="resources/33756B08C60CB46F50F35E4652AE7762.png" alt="Screenshot 2020-03-21 at 11.38.40 AM.png" width="942" height="525"><br></div><div><img src="resources/C517CC7B451420DED371A2898E4AB337.png" alt="Screenshot 2020-03-21 at 11.39.57 AM.png" width="939" height="515"><br></div><div><br></div><div><img src="resources/5225B50257A1339139759704B8513819.png" alt="Screenshot 2020-03-21 at 11.41.11 AM.png" width="953" height="535"><br></div><div><br></div><div><b>Training VAEs</b></div><div>1. Take a minibatch of input data</div><div><img src="resources/2775B04C1DAB7561288B5373D6EF0C07.png" alt="Screenshot 2020-03-21 at 11.44.33 AM.png" width="946" height="518"><b><br></b></div><div>2. Pass it through encoder network q(z|x) to get mu_{z|x}, sigma_{z|x}. [One value for each minibatch?] Use q(z|x) to compute the second (KL) term of the loss.&nbsp;</div><div><img src="resources/A593065E496CD0D4A841D569EF3179B7.png" alt="Screenshot 2020-03-21 at 11.44.51 AM.png" width="947" height="529"><br></div><div><br></div><div>3. Sample z from z|x~N(mu_{z|x}, Sigma_{z|x}). Pass z through decoder network p(x|z) to get mu_{x|z}, Sigma_{x|z}. Note that we make the sampling step differentiable through the reparameterization trick (see Kingma’s original paper)</div><div><img src="resources/526792F01624668A0F3BFC5B78DAE7C0.png" alt="Screenshot 2020-03-21 at 11.46.07 AM.png" width="945" height="528"><br></div><div>4. Sample x from x | z ~ N(mu_{x|z}, Sigma_{x|z})<img src="resources/AC73B833994DB5551C2D9B75A4C19BD5.png" alt="IMG_0160.PNG" width="2208" height="1242"></div><div><br></div><div><b>Generating Data from VAEs</b><br></div><div>Use just the decoder network. We sample z from some prior, like a normal gaussian, get the mean and variance of x |z, and sample x from that distribution.</div><div><img src="resources/03C1663489043D84C37E2D04853AF898.png" alt="IMG_0161.PNG" width="2208" height="1242"><br></div><div><br></div><div>Different dimensions of z encoder interprable factors of variation [how to we pick a z such that this occurs?]</div><div><img src="resources/72497BAC6C636DAA25999499BC1EF04F.png" alt="Screenshot 2020-03-21 at 11.54.41 AM.png" width="944" height="528"><img src="resources/C556A8B4630C926713432E6EA9CE8D22.png" alt="IMG_0163.PNG" width="2208" height="1242"><br></div><div><br></div><div><img src="resources/48A4247754416C97576757D9055B9A47.png" alt="IMG_0164.PNG" width="2208" height="1242"><br></div><div><br></div><div><b>Appendix 1.d-&nbsp;</b><b>Generative Adversarial Networks</b></div><div><img src="resources/DBDE2630B7CF63237F8767C90659C6DC.png" alt="IMG_0169.PNG" width="2208" height="1242"><br></div><div><img src="resources/834298E584BA792C7DC3F24362D92E1A.png" alt="IMG_0172.PNG" width="2208" height="1242"><br></div><div><img src="resources/92D2384F72B825BAD37A365A3B3845FE.png" alt="IMG_0174.PNG" width="2208" height="1242"><br></div><div><img src="resources/6DA02A29B8915A301DFCEF816CB50723.png" alt="IMG_0176.PNG" width="2208" height="1242"><br></div><div><br></div><div><img src="resources/FFB41D8A77E266B34603A9FB7B414518.png" alt="IMG_0178.PNG" width="2208" height="1242"><br></div><div>Wasserstein GAN- more stable GAN for training (read)</div><div><br></div><div>Convolutional Architectures for GANs</div><div><img src="resources/AF647589DBACB3404C6346DC79692896.png" alt="Screenshot 2020-03-21 at 12.24.33 PM.png" width="943" height="509"><br></div><div><img src="resources/39AB6BB335644C66BBA4331D32A77F72.png" alt="Screenshot 2020-03-21 at 1.35.37 PM.png" width="946" height="529"></div><div><br></div><div>Since like the variational autoencoder the GAN a low-dimensional feature space representation of the image (the prior, the input to the generator), we can use that representation to interpolate between image latent spaces.</div><div><img src="resources/CF99E2334EA27BF6D02B5063CDDCF73D.png" alt="IMG_0180.PNG" width="2208" height="1242"><br></div><div><img src="resources/6688A62F5150F38FE93324A46B740840.png" alt="Screenshot 2020-03-21 at 1.37.41 PM.png" width="954" height="526"><br></div><div><br></div><div><img src="resources/C903B4A245977B91F96CE44DD5EDEA11.png" alt="IMG_0182.PNG" width="2208" height="1242"></div><div><br></div><div><img src="resources/0C01D3B1A5944EC094D46CD286AB58A9.png" alt="IMG_0184.PNG" width="2208" height="1242"></div><div><br></div><div><img src="resources/C44BACDBB7D6EFB56EAF4374A3FF8BAE.png" alt="Screenshot 2020-03-21 at 1.44.27 PM.png" width="943" height="530"><br></div></div></div>
    </body>
    </html>
  