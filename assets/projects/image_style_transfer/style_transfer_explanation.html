<h2>Neural Image Style Transfer Explained</h2> 
<form action="#style_transfer">
  <input type="submit" value="Back" style="display:inline; text-align:center" />
</form>
<div id="contentarea"><div class="cell text-cell"></div><div class="cell markdown-cell"><p>The problem of how  to transfer image style is an old problem in computer graphics, where it was originally called <strong>texture transfer</strong>.</p>
<p><strong>Image Quilting</strong><br>
Texture transfer involvs applying a <strong>style texture</strong> to a <strong>content image</strong>. The first influential method do this was called <strong>Image Quilting</strong> and hailed from 2001. Here, the style texture was a small image patch that is repeated over and over in a quilt-like fashion. [1, <a href="https://people.eecs.berkeley.edu/~efros/research/quilting/quilting.pdf">link</a>]  Quilting was rather successfully used in computer graphics to generate repeated patches, but was rather less commonly used as a style transfer algorithm, due to the generated images being rather patchy, as can be seen in the examples in Appendix 1.</p>
<p>Image Quilting defined the texture transfer problem as a minimization problem with jointly  with two objectives, roughly,</p>
<ol>
<li>(<strong>Style Error</strong>) The patches must be quilted together in a way that minimizes error or disjointment</li>
<li>(<strong>Content Error</strong>) Some features of the generated image must match those of the original image at every patch. The paper did not go into details, but mentioned image intensity, blur intensity, and local orientation angles as possibilities.</li>
</ol>
<p><strong>Neural Style Transfer</strong><br>
Neural Networks layers allow us to effectively and precicely represent image features at all levels of granularity for any image processing task, including the texture transfer task. (Appendix 2) In a 2015 Paper, Gatys et al. [2, <a href="https://arxiv.org/pdf/1505.07376.pdf">link</a>] found a way to represent the style of an entire image by taking the correlations of the image's features at any particular level of the neural network. In other words, a style image is generated when the features which tend to appear together in the original image also tend to appear together in the generated image, without regard for where in the generated image this occurs. This insight allowed Gatys et al. to make the above two error terms from Image Quilting precise, in an algorithm they termed <strong>Neural Style Transfer</strong>. [3, <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf">link</a>]</p>
<ol>
<li>(<strong>Style Error</strong>) Pick some lower-level neural network layers. The style of the stylistic image is computed as the Gram Matrix (<em>unnormalized</em> correlation matrix between features) between all two feature maps at that layer- the Gram matrix captures image style at a low level of granularity, without worrying too much about content. The style error is then the total difference between the Gram Matrix computed using the style image and the Gram Matrix computed using the generated image for all the low layers. ("The generated image must look like the content image in low-level style.")</li>
<li>(<strong>Content Error</strong>) Pick a high-level neural network layer. From the activations on this layer, the overall content of the original image can be reproduced while at the same time not constraining the exact pixel values of the reproduction too much. The content error is thus computed as the total difference between the neural network activations of the content image and the neural network activations of the generated image for this high-level layer. ("The generated image must look like the content image in high-level content")</li>
</ol>
<p>If you pick lower-level neural network layers for style, style matching is done at a higher degree of granularity with lower attention to macro-detail, the reverse is true using higher-level layers. An optimal algorithm uses a good weighting of higher-level and lower-level layers for style and also has some optimal interpolation between the style and the content error.</p>
<p><strong>Implementation</strong><br>
To generate the image, Gatys et al. used a large pre-trained 19-layer VGG neural network for image representations, initialized the generated image as random noise, and ran several iterations of L-BGFS, a second-order optimiazation algorithm, updating the image in the direction of smaller combined error.</p>
<p>In our implementation, we use a pre-trained 7-layer SqueezeNet (highly compact) image model for image representations, and shrank the input images before passing them as Tensorflow tensors into the network. Similar to Gatys et al, we use multiple layers in the style representation, weighing lower layers much more than macro (higher-level) layers, and only the highest layer in the content representation. In addition to the content loss and style loss, we add a regularization term (total-variation loss) which encourages local similarity to help improve the smoothness of the generated image.</p>
<h2>References</h2>
<p>[1] A. Efros, and W. Freeman. Image Quilting for Texture Synthesis and Transfer. In <em>SIGGRAPH 2001</em>.<br>
[2] L. Gatys, A. Ecker, M. Bethge. Texture Synthesis Using Convolutional Neural Networks. In <em>NIPS 2015</em>.<br>
[3] L. Gatys, A. Ecker, M. Bethge. Image Style Transfer Using Convolutional Neural Networks. In <em>CVPR 2016</em>.<br>
[4] A. Krizhevsky, I. Sutskever, G. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In <em>NIPS 2012</em>.</p>
<h2>Appendices</h2>
<h3>Appendix 1</h3>
</div><div class="cell markdown-cell"><p><strong>Texture Synthesis Using Image Quilts</strong><br>
<img src="./assets/projects/image_style_transfer/resources/341CB3E9436068CA2E3EEEEF06D03839.png"  width="300"> <img src="./assets/projects/image_style_transfer/resources/E83FD9FE340CAE9303A5C4C809EF20B7.png"  width="300"></p>
<p>Both images are from the original quilting paper. [1] Some other examples where one can see quilt-based texture transfer are <a href="http://cs.brown.edu/courses/cs129/results/proj4/chaoqian/">here</a> and <a href="https://sandipanweb.wordpress.com/2017/10/24/some-computational-photography-image-quilting-texture-synthesis-with-dynamic-programming-and-texture-transfer-in-python/">here</a> (external).</p>
<h3>Appendix 2</h3>
<p>A Convolutional Neural Network [4] comprises of multiple layers, which may be thought of as "filters". The lower level layers match lower level features such as edges, the higher level layers match high level features such as entire faces or parts of objects. The "matching" of an image at all these different layers is what gives Convolutional Neural Networks so much power in all forms of image processing.</p>
</div><div class="cell markdown-cell"><p><img src="./assets/projects/image_style_transfer/resources/10CBA1FABB8C6C1B39D87FA96FE862D9.png"  width="300"></p>
<h3>Appendix 3</h3>
<p>Below, you can think of <code>conv1_1</code> as corresponding to using the correlations of the "low-level features" above, <code>pool1</code>-<code>pool3</code> are corresponding to using the correlations of the "mid-level features" above, and <code>pool4</code> as using the high-level features above. A style image is generated when the feature correlations match the feature correlations of the original image, that is, when the features which tend to appear together in the original image also tend to appear together in the generated image, without regard for where in the generated image this occurs.</p>
</div><div class="cell markdown-cell"><p><img src="./assets/projects/image_style_transfer/resources/B0AF4498E58D5D032D6BA96216C4467F.png"  width="300"></p>
</div></div>