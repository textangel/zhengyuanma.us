
<div id="titlearea">
  <h2>Training NMT Models</h2>
</div>
<form action="#nmt">
  <input type="submit" value="Back" style="display:inline; text-align:center" />
</form>
<div id="contentarea"><div class="cell markdown-cell"><h3>Training the Transformer Model</h3>
<p>The Transformer Model follows the design of the original Transformer paper, <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a>.</p>
<p>To train the transformer model, we first tokenized text into words and punctuation and performed unidecoding to ensure vocabulary uniformlity. The resultant text was processed into subword tokens using <a href="https://github.com/rsennrich/subword-nmt">Senrich's Byte-Pair Encoding</a>. Byte-Pair encoding decreases the vocabulary size by breaking rare words into subword chunks. The English and Spanish training data was used jointly to generate a joint codes file. Since, encoding was trained on both English and Spanish. we want to avoid applying an encoding an English text with subword tokens that only appeared in the Spanish. Thus, we use the <code>--vocabulary-threshold</code> with an argument of <code>50</code>, which causes us to only apply the encoding if that particular subword token appeared over 50 times in the English text.</p>
<p>We used a reduced-size Transformer model with <code>N=2</code> transformer layers, <code>h=4</code> multi-head attention heads, <code>128</code> dimensional mdoel embeddings and <code>1024</code> dimensional feed-forward embeddings.</p>
<p>During training we used a batch_size of <code>512</code> and applied dropout of <code>0.3</code>. We trained with the NoamOpt optimizer from the original Transformer paper with initial learning rate of <code>0.001</code> - the learning rate goes up during a warmup-phase until a threshold, then tapers off slowly thereafter.</p>
<p>During decoding, we applied beam search with a beam size of 8 and detokenized and merged BPE outputs. The model was trained on ~250,000 parallel sentences from the TED 2016 dataset, and was trained for 8 hours on an NVIDIA Tesla K80 GPU, although the model seems to converge by the third hour. Test BELU on English-Spanish and Spanish-English was both 24.5.</p>
</div><div class="cell markdown-cell" style="text-align: center;"><p>
  <img src="./assets/projects/nmt/resources/11A7668679BF9527CFD4C4C2E618C570.png" width="400">
</p>
</div><div class="cell markdown-cell"><h3>Training the LSTM Model</h3>
<p>The LSTM model follows the design of Bahdabau et al's <a href="https://arxiv.org/pdf/1409.0473.pdf">original paper on Attention</a>, however, using dot-product attention instead of Bahdanau's additive attention. We implement this model, including attention, from PyTorch primitives.</p>
<p>Because LSTMs are length-constrained, we limited our input and output to 70 tokens. We use a bi-directional LSTM model, with word embedding and model embedding size of <code>256</code> (Since the model is bidirectional, the full concatenated embedding size is <code>512</code>). Instead of the attention proposed in Bahdanau's paper, we use dot-product attention, implemented from scratch, from <a href="https://arxiv.org/pdf/1508.04025.pdf">Luong's 2015 Paper</a> since it is more efficient to implement with similar results.</p>
<p>During training, we used a batch size of <code>32</code> and applied dropout of <code>0.3</code>. We trained using an Adam Optimizer with initial learning rate of <code>0.001</code>, a learning rate decay of <code>0.5</code> for every <code>5</code> epochs without improvement, and clipped gradients whose norms were greater than <code>5</code>.</p>
<p>During decoding, we used beam search with a beam size of <code>5</code>. The model was trained on ~250,000 parallel sentences English to Spanish from the TED 2016 dataset, and was trained for 8 hours on an NVIDIA Tesla K80 GPU, although the model converged by the 4th hour. Test BELU on English-Spanish and Spanish-English was 21.3 and 21.7.</p>
<h3>Training the Char-LSTM Model</h3>
<p>The Char-LSTM model follows the design of <a href="https://arxiv.org/pdf/1604.00788.pdf">Luong and Manning's 2016 Paper</a>, with the exception that we only implement the character-level decoder, not the character-level encoder. It is exactly the same as the LSTM model above, except a character-level LSTM model with character embeddings is trained in conjunction with the word-level LSTM model. If an unknown word is generated, the character level LSTM is run to generate the unknown word one character at a time.</p>
<p>We train the LSTMs using the same hyperparameters as above, using a character decoder with the same model size as the word-level LSTM. During training, the Character level LSTM is trained on the characters of every output word, not just unknown tokens. During testing, we only use it when an unknown token is generated.</p>
<p>As with the LSTM model, training on ~250,000 parallel English to Spanish sentences from the TED 2016 dataset on a NVIDIA Tesla K80 converged in around 4th hours. Luong and Manning's paper found that this model trained on English-Czech and Czech-English saw BLEU improvements of about 2 points. We see similar results on English-Spanish and Spanish-English, obtaining 23.0 BLEU compared to the pure LSTM model.</p>
<h3>Future Work</h3>
<p>The most important improvement to these models is the addition of more training data, especially from multiple domains. Becuase of the limited scope and domain of the training data, the NMT application will get many out-of-domain translations wrong. Parallel, but noisy, English to Spanish corpora exist in <a href="http://opus.nlpl.eu/">the 10s of GB</a>. Much work has been done already on aligning these <a href="http://opus.nlpl.eu/OpenSubtitles-v2018.php">corpora</a>. Adding such data to our training loop will significantly improve the quality of out-of-domain translations.</p>